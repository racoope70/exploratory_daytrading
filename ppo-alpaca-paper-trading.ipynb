{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONtvMPpPoulL0LBRxS81si",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/exploratory_daytrading/blob/main/ppo-alpaca-paper-trading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-DEy5gEqqEi",
        "outputId": "ce36d714-4bd7-404b-de92-f614b5c2d3f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alpaca-trade-api\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.32.4)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.8.0)\n",
            "Collecting websockets<11,>=9.0 (from alpaca-trade-api)\n",
            "  Downloading websockets-10.4.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting msgpack==1.0.3 (from alpaca-trade-api)\n",
            "  Downloading msgpack-1.0.3.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (3.12.15)\n",
            "Collecting PyYAML==6.0.1 (from alpaca-trade-api)\n",
            "  Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting deprecation==2.1.0 (from alpaca-trade-api)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.20.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp<4,>=3.8.3->alpaca-trade-api) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.18.1->alpaca-trade-api) (1.17.0)\n",
            "Downloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (724 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: msgpack, websockets\n",
            "  Building wheel for msgpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-1.0.3-cp312-cp312-linux_x86_64.whl size=15688 sha256=ce813bd88a31df787c05a2c7d29be4a91a454a74e55a8334e07aa6914e4a2937\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/bd/3f/f043e8f634db9c90ae128d631f43ae9990eef01274a63291f9\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-linux_x86_64.whl size=107320 sha256=c093d9a829dc499e658f3c9e544dd621ec13b56702b772b303682ef7d7c13ea8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/cf/6d/5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
            "Successfully built msgpack websockets\n",
            "Installing collected packages: msgpack, websockets, urllib3, PyYAML, deprecation, alpaca-trade-api\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.1\n",
            "    Uninstalling msgpack-1.1.1:\n",
            "      Successfully uninstalled msgpack-1.1.1\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.13.0 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\n",
            "google-adk 1.13.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 10.4 which is incompatible.\n",
            "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "google-genai 1.33.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.1 alpaca-trade-api-3.2.0 deprecation-2.1.0 msgpack-1.0.3 urllib3-1.26.20 websockets-10.4\n",
            "Requirement already satisfied: alpaca-trade-api in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.32.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.8.0)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (10.4)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.0.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (3.12.15)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.20.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp<4,>=3.8.3->alpaca-trade-api) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.18.1->alpaca-trade-api) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=c6eafa970a92367dc3e6ff4944402679337804b1a32a8287e66e2c266b9c99e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install alpaca-trade-api\n",
        "!pip install alpaca-trade-api python-dotenv ta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean any partials\n",
        "!pip uninstall -y stable-baselines3 shimmy gymnasium gym autorom AutoROM.accept-rom-license ale-py\n",
        "\n",
        "# Install the compatible trio (no [extra] to avoid Atari deps)\n",
        "!pip install \"gymnasium==0.29.1\" \"shimmy==1.3.0\" \"stable-baselines3==2.3.0\"\n",
        "\n",
        "# Your other libs (safe to keep separate)\n",
        "!pip install alpaca-trade-api ta python-dotenv gym-anytrading\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HgUjRXJs8ez",
        "outputId": "2a67c3a2-cfa0-4710-c0a1-d1f067fae3b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping stable-baselines3 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping shimmy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: gymnasium 1.2.0\n",
            "Uninstalling gymnasium-1.2.0:\n",
            "  Successfully uninstalled gymnasium-1.2.0\n",
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "\u001b[33mWARNING: Skipping autorom as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping AutoROM.accept-rom-license as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: ale-py 0.11.2\n",
            "Uninstalling ale-py-0.11.2:\n",
            "  Successfully uninstalled ale-py-0.11.2\n",
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting shimmy==1.3.0\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting stable-baselines3==2.3.0\n",
            "  Downloading stable_baselines3-2.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.3.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->stable-baselines3==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13->stable-baselines3==2.3.0) (3.0.2)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'stable-baselines3' candidate (version 2.3.0 at https://files.pythonhosted.org/packages/51/0b/6539076ed58343f1404dea0462167b079b5264508b8e5bbed01cea9f66b8/stable_baselines3-2.3.0-py3-none-any.whl (from https://pypi.org/simple/stable-baselines3/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Loading broken with PyTorch 1.13\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium, shimmy, stable-baselines3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 shimmy-1.3.0 stable-baselines3-2.3.0\n",
            "Requirement already satisfied: alpaca-trade-api in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: ta in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Collecting gym-anytrading\n",
            "  Downloading gym_anytrading-2.0.0-py3-none-any.whl.metadata (292 bytes)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.32.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.8.0)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (10.4)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.0.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (3.12.15)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (25.0)\n",
            "Requirement already satisfied: gymnasium>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from gym-anytrading) (0.29.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from gym-anytrading) (3.10.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.20.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.1->gym-anytrading) (1.17.0)\n",
            "Downloading gym_anytrading-2.0.0-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gym-anytrading\n",
            "Successfully installed gym-anytrading-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install if not already\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8LJ--nts-e9",
        "outputId": "7ecf2b86-e702-4551-cd28-8e6897e7bacd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, math, time, typing, numpy as np\n",
        "import warnings\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import ta\n",
        "\n",
        "from alpaca_trade_api.rest import REST, TimeFrame, APIError\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.vec_env.vec_normalize import VecNormalize\n",
        "\n",
        "# Colab Drive (optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Use a PPO-specific output dir (rename from LightGBM)\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/Alpaca_Results/PPO\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS4-sGMItAIM",
        "outputId": "facdb3a5-92d0-4cc2-b2d9-360c9dd259f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Prompts you to upload the .env file for the key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "hWMP7LcYtCOg",
        "outputId": "b9ac9b4c-db47-42d2-d840-40fa35fcf996"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0cfa7d05-c428-451f-9d4c-db8785a73182\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0cfa7d05-c428-451f-9d4c-db8785a73182\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Alpaca_keys.env.txt to Alpaca_keys.env.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Rename to .env so dotenv can recognize it\n",
        "os.rename(\"Alpaca_keys.env.txt\", \".env\")"
      ],
      "metadata": {
        "id": "emNRU9dEtIHo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload: model_ABT.txt, features_ABT.txt, scaler_ABT.pkl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "R8oUOgIYtJxk",
        "outputId": "66636b17-4f5c-42e2-9495-2bb39fdaaf1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-88b0afec-0429-44ba-87ed-8ea4ea3dea10\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-88b0afec-0429-44ba-87ed-8ea4ea3dea10\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ppo_UNH_window3_features.json to ppo_UNH_window3_features.json\n",
            "Saving ppo_UNH_window3_model_info.json to ppo_UNH_window3_model_info.json\n",
            "Saving ppo_UNH_window3_model.zip to ppo_UNH_window3_model.zip\n",
            "Saving ppo_UNH_window3_probability_config.json to ppo_UNH_window3_probability_config.json\n",
            "Saving ppo_UNH_window3_vecnorm.pkl to ppo_UNH_window3_vecnorm.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- One-time imports & logging ---\n",
        "import os, logging, re, json, time, math, pickle, warnings\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from typing import Dict, Optional, Tuple, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "import alpaca_trade_api as tradeapi  # pip install alpaca-trade-api\n",
        "\n",
        "# Load .env FIRST so LOG_LEVEL and keys from .env are available\n",
        "load_dotenv(override=True)\n",
        "\n",
        "LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\n",
        "logging.basicConfig(\n",
        "    level=getattr(logging, LOG_LEVEL, logging.INFO),\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
        ")\n",
        "\n",
        "# Accept both APCA_* and ALPACA_* names\n",
        "API_KEY    = os.getenv(\"APCA_API_KEY_ID\")     or os.getenv(\"ALPACA_API_KEY_ID\")     or \"\"\n",
        "API_SECRET = os.getenv(\"APCA_API_SECRET_KEY\") or os.getenv(\"ALPACA_API_SECRET_KEY\") or \"\"\n",
        "BASE_URL   = os.getenv(\"APCA_API_BASE_URL\")   or os.getenv(\"ALPACA_API_BASE_URL\")   or \"https://paper-api.alpaca.markets\"\n",
        "\n",
        "def init_alpaca() -> \"tradeapi.REST\":\n",
        "    if not API_KEY or not API_SECRET:\n",
        "        raise RuntimeError(\"Missing API keys. Set APCA_API_KEY_ID/APCA_API_SECRET_KEY (or ALPACA_* equivalents).\")\n",
        "    api = tradeapi.REST(API_KEY, API_SECRET, base_url=BASE_URL)\n",
        "    _ = api.get_account()  # sanity check\n",
        "    return api\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Optional quick sanity check (kept out of import-time side effects)\n",
        "    logging.info(\"Keys loaded: %s %s | BASE_URL=%s\", bool(API_KEY), bool(API_SECRET), BASE_URL)\n",
        "    api = init_alpaca()\n",
        "    logging.info(\"Account status: %s\", api.get_account().status)\n",
        "\n",
        "    # run_live(TICKERS)  # uncomment when ready\n",
        "\n",
        "def sanity_check():\n",
        "    print(\"Keys loaded (bools):\", bool(API_KEY), bool(API_SECRET), \"| BASE_URL:\", BASE_URL)\n",
        "    api = init_alpaca()\n",
        "    acct = api.get_account()\n",
        "    print(\"Account status:\", acct.status)\n",
        "\n",
        "sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJmb00cRaHyH",
        "outputId": "dc2bb284-bd33-4f84-d08a-7dfd80510fbb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys loaded (bools): True True | BASE_URL: https://paper-api.alpaca.markets\n",
            "Account status: ACTIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "for root in [\"/content\", \"/content/drive/MyDrive\"]:\n",
        "    hits = list(Path(root).rglob(\"ppo_*_model.zip\"))\n",
        "    if hits:\n",
        "        print(\"Found in:\", root)\n",
        "        for h in hits: print(\"  \", h)\n"
      ],
      "metadata": {
        "id": "qkfPWqz7yxrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UNH artifacts + paper-trading runtime config (run BEFORE sanity_check/run_live) ---\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "from alpaca_trade_api.rest import TimeFrame, APIError\n",
        "\n",
        "import os\n",
        "os.environ[\"ARTIFACTS_DIR\"]     = \"/content\"\n",
        "os.environ[\"TICKERS\"]           = \"UNH\"\n",
        "os.environ[\"BEST_WINDOW\"]       = \"3\"\n",
        "os.environ[\"INF_DETERMINISTIC\"] = \"1\"\n",
        "\n",
        "# paper-trading defaults\n",
        "#DRY_RUN=1 → logs only (no orders)\n",
        "#DRY_RUN=0 → actually sends orders to Alpaca\n",
        "\n",
        "os.environ[\"DRY_RUN\"]   = \"1\"    # no orders placed; set \"0\" to place PAPER orders\n",
        "os.environ[\"BARS_FEED\"] = \"iex\"  # paper feed\n",
        "\n",
        "# Paper/live execution knobs\n",
        "DRY_RUN   = os.getenv(\"DRY_RUN\", \"1\").lower() in (\"1\",\"true\",\"yes\")  # 1=don’t place orders\n",
        "BARS_FEED = os.getenv(\"BARS_FEED\", \"iex\").strip()                    # paper: \"iex\"; live: \"sip\" (if entitled)\n",
        "\n",
        "\n",
        "# Paper trading / data feed knobs\n",
        "os.environ[\"DRY_RUN\"]   = \"1\"    # \"1\"=log only (no orders); \"0\"=place PAPER orders\n",
        "os.environ[\"BARS_FEED\"] = \"iex\"  # paper feed is \"iex\"; live (if entitled) would be \"sip\"\n",
        "\n",
        "# ===== Ensure the artifact trio sits together in ARTIFACTS_DIR =====\n",
        "ART_DIR = Path(os.environ[\"ARTIFACTS_DIR\"])\n",
        "# Try the common Colab Drive root; adjust if you stored files elsewhere\n",
        "CANDIDATE_DRIVE_ROOTS = [Path(\"/content/drive/MyDrive\"), Path(\"/content/drive\")]\n",
        "\n",
        "need = [\n",
        "    \"ppo_UNH_window3_model.zip\",\n",
        "    \"ppo_UNH_window3_vecnorm.pkl\",\n",
        "    \"ppo_UNH_window3_features.json\",\n",
        "]\n",
        "\n",
        "# Copy any missing files from Drive into ARTIFACTS_DIR\n",
        "for name in need:\n",
        "    dst = ART_DIR / name\n",
        "    if dst.exists():\n",
        "        continue\n",
        "\n",
        "    src = None\n",
        "    for root in CANDIDATE_DRIVE_ROOTS:\n",
        "        try:\n",
        "            hit = next(root.rglob(name))\n",
        "            src = hit\n",
        "            break\n",
        "        except StopIteration:\n",
        "            continue\n",
        "\n",
        "    if src:\n",
        "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "# ===== Quick verification =====\n",
        "print(\"ARTIFACTS_DIR =\", ART_DIR)\n",
        "print(\"Exists:\", ART_DIR.exists())\n",
        "print(\"Artifacts found:\", sorted(p.name for p in ART_DIR.glob(\"ppo_UNH_window3_*\")))\n",
        "print(\"DRY_RUN   =\", os.getenv(\"DRY_RUN\"))\n",
        "print(\"BARS_FEED =\", os.getenv(\"BARS_FEED\"))\n",
        "\n",
        "# Hard fail early if anything is missing\n",
        "missing = [n for n in need if not (ART_DIR / n).exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing artifacts in {ART_DIR}:\\n  \" + \"\\n  \".join(missing) +\n",
        "        \"\\nIf they are on Drive, mount it and ensure paths are correct.\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66_yCEpJxNcI",
        "outputId": "c6269bc5-27e9-4b5d-efd7-58b2cf25b89e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARTIFACTS_DIR = /content\n",
            "Exists: True\n",
            "Artifacts found: ['ppo_UNH_window3_features.json', 'ppo_UNH_window3_model.zip', 'ppo_UNH_window3_model_info.json', 'ppo_UNH_window3_probability_config.json', 'ppo_UNH_window3_vecnorm.pkl']\n",
            "DRY_RUN   = 1\n",
            "BARS_FEED = iex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Tuple, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---- Column normalization ---------------------------------------------------------------\n",
        "def normalize_ohlcv_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Case-insensitive rename to: Open, High, Low, Close, Adj Close, Volume.\n",
        "    If Adj Close missing, copy Close.\n",
        "    \"\"\"\n",
        "    cols_ci = {c.lower(): c for c in df.columns}\n",
        "    want = {\n",
        "        \"Open\":      [\"open\"],\n",
        "        \"High\":      [\"high\"],\n",
        "        \"Low\":       [\"low\"],\n",
        "        \"Close\":     [\"close\", \"close*\", \"last\"],\n",
        "        \"Adj Close\": [\"adj close\", \"adj_close\", \"adjclose\", \"adjusted close\"],\n",
        "        \"Volume\":    [\"volume\", \"vol\"],\n",
        "    }\n",
        "    rename = {}\n",
        "    for final, alts in want.items():\n",
        "        for a in [final.lower()] + alts:\n",
        "            if a in cols_ci:\n",
        "                rename[cols_ci[a]] = final\n",
        "                break\n",
        "    out = df.rename(columns=rename).copy()\n",
        "    if \"Adj Close\" not in out.columns and \"Close\" in out.columns:\n",
        "        out[\"Adj Close\"] = out[\"Close\"]\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---- Light wavelet denoiser (safe fallbacks) -------------------------------------------\n",
        "def denoise_wavelet(series: pd.Series, wavelet: str = \"db1\", level: int = 2) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Low-pass via wavelets; if pywt missing or params invalid, fallback to a short EMA.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import pywt\n",
        "    except Exception:\n",
        "        return pd.Series(series).astype(float).ewm(span=5, adjust=False).mean()\n",
        "\n",
        "    s = pd.Series(series).astype(float).ffill().bfill()\n",
        "    arr = s.to_numpy()\n",
        "    try:\n",
        "        w = pywt.Wavelet(wavelet)\n",
        "        maxlvl = pywt.dwt_max_level(len(arr), w.dec_len)\n",
        "        lvl = int(max(0, min(level, maxlvl)))\n",
        "        if lvl < 1:\n",
        "            return s\n",
        "        coeffs = pywt.wavedec(arr, w, mode=\"symmetric\", level=lvl)\n",
        "        for i in range(1, len(coeffs)):  # zero details, keep approximation\n",
        "            coeffs[i] = np.zeros_like(coeffs[i])\n",
        "        rec = pywt.waverec(coeffs, w, mode=\"symmetric\")\n",
        "        return pd.Series(rec[:len(arr)], index=s.index)\n",
        "    except Exception:\n",
        "        return s.ewm(span=5, adjust=False).mean()\n",
        "\n",
        "\n",
        "# ---- Simple regime labels ---------------------------------------------------------------\n",
        "def add_regime(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"Vol20\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Ret20\"] = df[\"Close\"].pct_change(20)\n",
        "    vol_hi   = (df[\"Vol20\"] > df[\"Vol20\"].median()).astype(int)\n",
        "    trend_hi = (df[\"Ret20\"].abs() > df[\"Ret20\"].abs().median()).astype(int)\n",
        "    df[\"Regime4\"] = vol_hi * 2 + trend_hi  # values in {0,1,2,3}\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---- Indicators used at live time (match training set) ---------------------------------\n",
        "def add_features_live(\n",
        "    df: pd.DataFrame,\n",
        "    use_sentiment: bool = False,\n",
        "    rsi_wilder: bool = True,\n",
        "    atr_wilder: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds a compact set of indicators (Bollinger, Stoch, ROC, OBV, CCI, EMAs, MACD, RSI, ATR, Volatility).\n",
        "    Returns a numeric DataFrame with NaN where warmups are needed (to be dropped later).\n",
        "    \"\"\"\n",
        "    df = normalize_ohlcv_cols(df).copy().sort_index()\n",
        "\n",
        "    # Bollinger\n",
        "    df[\"SMA_20\"] = df[\"Close\"].rolling(20).mean()\n",
        "    df[\"STD_20\"] = df[\"Close\"].rolling(20).std()\n",
        "    df[\"Upper_Band\"] = df[\"SMA_20\"] + 2 * df[\"STD_20\"]\n",
        "    df[\"Lower_Band\"] = df[\"SMA_20\"] - 2 * df[\"STD_20\"]\n",
        "\n",
        "    # Stochastic %K (unsmoothed)\n",
        "    df[\"Lowest_Low\"]   = df[\"Low\"].rolling(14).min()\n",
        "    df[\"Highest_High\"] = df[\"High\"].rolling(14).max()\n",
        "    denom = (df[\"Highest_High\"] - df[\"Lowest_Low\"]).replace(0, np.nan)\n",
        "    df[\"Stoch\"] = ((df[\"Close\"] - df[\"Lowest_Low\"]) / denom) * 100\n",
        "\n",
        "    # Momentum / volume / CCI\n",
        "    df[\"ROC\"] = df[\"Close\"].pct_change(10)\n",
        "    sign = np.sign(df[\"Close\"].diff().fillna(0))\n",
        "    df[\"OBV\"] = (sign * df[\"Volume\"].fillna(0)).cumsum()\n",
        "\n",
        "    tp = (df[\"High\"] + df[\"Low\"] + df[\"Close\"]) / 3.0\n",
        "    sma_tp = tp.rolling(20).mean()\n",
        "    md = (tp - sma_tp).abs().rolling(20).mean().replace(0, np.nan)\n",
        "    df[\"CCI\"] = (tp - sma_tp) / (0.015 * md)\n",
        "\n",
        "    # EMAs + MACD\n",
        "    df[\"EMA_10\"] = df[\"Close\"].ewm(span=10, adjust=False).mean()\n",
        "    df[\"EMA_50\"] = df[\"Close\"].ewm(span=50, adjust=False).mean()\n",
        "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "    df[\"MACD_Line\"]   = ema12 - ema26\n",
        "    df[\"MACD_Signal\"] = df[\"MACD_Line\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    # RSI (Wilder default)\n",
        "    d = df[\"Close\"].diff()\n",
        "    gain = d.clip(lower=0)\n",
        "    loss = (-d.clip(upper=0))\n",
        "    if rsi_wilder:\n",
        "        avg_gain = gain.ewm(alpha=1/14, adjust=False).mean()\n",
        "        avg_loss = loss.ewm(alpha=1/14, adjust=False).mean()\n",
        "    else:\n",
        "        avg_gain = gain.rolling(14).mean()\n",
        "        avg_loss = loss.rolling(14).mean()\n",
        "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
        "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # ATR (Wilder default)\n",
        "    tr = pd.concat([\n",
        "        (df[\"High\"] - df[\"Low\"]),\n",
        "        (df[\"High\"] - df[\"Close\"].shift()).abs(),\n",
        "        (df[\"Low\"]  - df[\"Close\"].shift()).abs(),\n",
        "    ], axis=1).max(axis=1)\n",
        "    df[\"ATR\"] = tr.ewm(alpha=1/14, adjust=False).mean() if atr_wilder else tr.rolling(14).mean()\n",
        "\n",
        "    # Volatility + denoised close\n",
        "    df[\"Volatility\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Denoised_Close\"] = denoise_wavelet(df[\"Close\"])\n",
        "\n",
        "    # Regime + optional sentiment + simple “Greeks” proxies\n",
        "    df = add_regime(df)\n",
        "    df[\"SentimentScore\"] = (df.get(\"SentimentScore\", 0.0) if use_sentiment else 0.0)\n",
        "    df[\"Delta\"] = df[\"Close\"].pct_change(1).fillna(0.0)\n",
        "    df[\"Gamma\"] = df[\"Delta\"].diff().fillna(0.0)\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---- Artifact feature order (for 1D vector models) -------------------------------------\n",
        "def compute_art_feat_order(features_hint: Any, df: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"\n",
        "    Resolve the exact feature order from your artifact, keeping only numeric columns present.\n",
        "    Accepts either a list or a dict with key 'features'.\n",
        "    \"\"\"\n",
        "    if features_hint is None:\n",
        "        return [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "    feats = features_hint.get(\"features\", features_hint) if isinstance(features_hint, dict) else list(features_hint)\n",
        "    drop = {\"datetime\", \"symbol\", \"target\", \"return\"}\n",
        "    return [c for c in feats if c not in drop and (c in df.columns) and pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "\n",
        "def build_obs_from_row(row: pd.Series, order: List[str]) -> np.ndarray:\n",
        "    \"\"\"Row -> 1D float32 vector in the artifact's column order (NaNs -> 0).\"\"\"\n",
        "    vals = []\n",
        "    for c in order:\n",
        "        v = row.get(c, np.nan)\n",
        "        vals.append(0.0 if (pd.isna(v) or v is None or v is False) else float(v))\n",
        "    return np.array(vals, dtype=np.float32)\n",
        "\n",
        "\n",
        "# ---- Shape discovery + live columns picking (for 2D models) -----------------------------\n",
        "def expected_obs_shape(model, vecnorm) -> Optional[tuple]:\n",
        "    \"\"\"\n",
        "    Returns the tuple shape the model expects for a *single* observation,\n",
        "    e.g. (10, 2). Falls back to None if not available.\n",
        "    \"\"\"\n",
        "    for src in (vecnorm, model):\n",
        "        try:\n",
        "            shp = tuple(src.observation_space.shape)  # SB3 exposes this\n",
        "            if shp:\n",
        "                return shp\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None  # unknown; caller will use 1D fallback\n",
        "\n",
        "\n",
        "def _pick_columns_for_channels(features_hint: Any, df: pd.DataFrame, channels: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Pick which DataFrame columns to use as 'channels' when building a 2D observation.\n",
        "    Priority: artifact feature list (if present) → common OHLCV columns → other numeric.\n",
        "    \"\"\"\n",
        "    numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    cols: List[str] = []\n",
        "\n",
        "    if isinstance(features_hint, dict) and \"features\" in features_hint:\n",
        "        cand = [c for c in features_hint[\"features\"] if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
        "        if len(cand) >= channels:\n",
        "            cols = cand[:channels]\n",
        "\n",
        "    if not cols:\n",
        "        pref = [\"Close\", \"Volume\", \"Adj Close\", \"Open\", \"High\", \"Low\"]\n",
        "        cols = [c for c in pref if c in numeric]\n",
        "        cols += [c for c in numeric if c not in cols]\n",
        "        cols = cols[:channels]\n",
        "\n",
        "    if len(cols) < channels and cols:\n",
        "        while len(cols) < channels:\n",
        "            cols.append(cols[-1])  # duplicate last to fill\n",
        "    return cols[:channels]\n",
        "\n",
        "\n",
        "# ---- Observation builder (2D-aware with 1D fallback) ------------------------------------\n",
        "def prepare_observation_from_bars(\n",
        "    bars_df: pd.DataFrame,\n",
        "    features_hint: Any = None,\n",
        "    min_required_rows: int = 60,\n",
        "    expected_shape: Optional[tuple] = None,\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Convert recent OHLCV bars to a single observation.\n",
        "    If the model expects 2D (lookback, channels), we build a window using raw columns.\n",
        "    Otherwise we build a 1D feature vector in the artifact's column order.\n",
        "\n",
        "    Returns: (obs: np.ndarray, obs_timestamp_utc: int)\n",
        "    \"\"\"\n",
        "    feats_df = add_features_live(bars_df).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Timestamp for staleness guard\n",
        "    ts = pd.Timestamp.utcnow()\n",
        "    try:\n",
        "        idx_ts = pd.Timestamp(feats_df.index[-1])\n",
        "        ts = idx_ts.tz_convert(\"UTC\") if idx_ts.tzinfo else idx_ts.tz_localize(\"UTC\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # ---- 2D path (e.g., expected_shape == (10, 2)) --------------------------------------\n",
        "    if expected_shape is not None and len(expected_shape) == 2:\n",
        "        lookback, channels = int(expected_shape[0]), int(expected_shape[1])\n",
        "        cols = _pick_columns_for_channels(features_hint, feats_df, channels)\n",
        "\n",
        "        # Use the raw (or close-to-raw) columns; take tail(lookback) and pad if short\n",
        "        window_df = feats_df[cols].tail(lookback)\n",
        "        arr = window_df.to_numpy(dtype=np.float32)\n",
        "\n",
        "        if arr.shape[0] < lookback:\n",
        "            pad_rows = lookback - arr.shape[0]\n",
        "            arr = np.vstack([np.zeros((pad_rows, channels), dtype=np.float32), arr])\n",
        "\n",
        "        # Guard final shape exactly\n",
        "        arr = arr[-lookback:, :channels]\n",
        "        return arr.reshape(lookback, channels), int(ts.timestamp())\n",
        "\n",
        "    # ---- 1D fallback (artifact feature vector) ------------------------------------------\n",
        "    order = compute_art_feat_order(features_hint, feats_df)\n",
        "    if not order:\n",
        "        raise ValueError(\"No usable features after resolving artifact order.\")\n",
        "    feats_df = feats_df.dropna(subset=order)\n",
        "\n",
        "    if len(feats_df) < max(20, min_required_rows):\n",
        "        raise ValueError(f\"Not enough bars to compute features robustly (have {len(feats_df)}).\")\n",
        "\n",
        "    last = feats_df.iloc[-1]\n",
        "    obs = build_obs_from_row(last, order)\n",
        "    return obs.astype(np.float32), int(ts.timestamp())\n"
      ],
      "metadata": {
        "id": "88meE9-BgkON"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _extract_window_idx(path: Path) -> Optional[int]:\n",
        "    m = re.search(r\"_window(\\d+)_\", path.stem, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    try:\n",
        "        return int(m.group(1))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def pick_artifacts_for_ticker(ticker: str, artifacts_dir: str, best_window: Optional[str] = BEST_WINDOW_ENV) -> Dict[str, Optional[Path]]:\n",
        "    p = Path(artifacts_dir)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Artifacts directory not found: {p.resolve()}\")\n",
        "\n",
        "    models = sorted(p.glob(f\"ppo_{ticker}_window*_model.zip\"))\n",
        "    if not models:\n",
        "        models = sorted(p.glob(f\"ppo_{ticker}_model.zip\")) or sorted(p.glob(f\"*{ticker}*model.zip\"))\n",
        "        if not models:\n",
        "            found = \"\\n\".join(f\" - {x}\" for x in p.rglob(\"*model.zip\"))\n",
        "            raise FileNotFoundError(f\"No PPO model for {ticker} in {p.resolve()}.\\nSeen:\\n{found or ' (none)'}\")\n",
        "\n",
        "    chosen: Optional[Path] = None\n",
        "    if best_window:\n",
        "        for m in models:\n",
        "            if f\"_window{best_window}_\" in m.stem:\n",
        "                chosen = m\n",
        "                break\n",
        "        if not chosen:\n",
        "            logging.warning(\"BEST_WINDOW=%s not found; using last sort model.\", best_window)\n",
        "    chosen = chosen or models[-1]\n",
        "\n",
        "    base   = chosen.stem.replace(\"_model\", \"\")\n",
        "    vecnorm= next(iter(sorted(p.glob(base + \"_vecnorm.pkl\"))), None)\n",
        "    feats  = next(iter(sorted(p.glob(base + \"_features.json\"))), None)\n",
        "\n",
        "    logging.info(f\"[{ticker}] model={chosen.name} | vecnorm={bool(vecnorm)} | features={bool(feats)}\")\n",
        "    return {\"model\": chosen, \"vecnorm\": vecnorm, \"features\": feats}\n",
        "\n",
        "def load_vecnormalize(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def load_features(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_ppo_model(model_path: Path):\n",
        "    return PPO.load(str(model_path))\n",
        "\n",
        "# ===== Alpaca helpers ====================================================================\n",
        "\n",
        "def ensure_market_open(api) -> bool:\n",
        "    try:\n",
        "        clock = api.get_clock()\n",
        "        return bool(getattr(clock, \"is_open\", False))\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"ensure_market_open() failed: {e}\")\n",
        "        return True  # allow loop; broker will reject if actually closed\n",
        "\n",
        "def get_recent_bars(api, symbol: str, limit: int = 200, timeframe=TimeFrame.Minute) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch recent bars and return a DataFrame with index=timestamp and\n",
        "    columns: Open, High, Low, Close, Volume (normalized for feature builder).\n",
        "    Honors BARS_FEED if provided (e.g., 'iex' for paper, 'sip' for live if entitled).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if BARS_FEED:\n",
        "            bars = api.get_bars(symbol, timeframe, limit=limit, feed=BARS_FEED)\n",
        "        else:\n",
        "            bars = api.get_bars(symbol, timeframe, limit=limit)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars failed: {e}\")\n",
        "        return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "\n",
        "    # Path 1: accessor returns a dataframe\n",
        "    if hasattr(bars, \"df\"):\n",
        "        df = bars.df.copy()\n",
        "        if not df.empty:\n",
        "            if isinstance(df.index, pd.MultiIndex):\n",
        "                df = df.xs(symbol, level=0)\n",
        "            df.index = pd.to_datetime(df.index, utc=True)\n",
        "            df = df.rename(columns={\"open\":\"Open\",\"high\":\"High\",\"low\":\"Low\",\"close\":\"Close\",\"volume\":\"Volume\"})\n",
        "            return df[[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]].sort_index()\n",
        "        return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "\n",
        "    # Path 2: iterable list of bar objects\n",
        "    rows = []\n",
        "    for b in bars:\n",
        "        ts_raw = getattr(b, \"t\", None)\n",
        "        ts = _to_utc_timestamp(ts_raw)\n",
        "        rows.append({\n",
        "            \"timestamp\": ts,\n",
        "            \"Open\":   float(getattr(b, \"o\", getattr(b, \"open\",  np.nan))),\n",
        "            \"High\":   float(getattr(b, \"h\", getattr(b, \"high\",  np.nan))),\n",
        "            \"Low\":    float(getattr(b, \"l\", getattr(b, \"low\",   np.nan))),\n",
        "            \"Close\":  float(getattr(b, \"c\", getattr(b, \"close\", np.nan))),\n",
        "            \"Volume\": float(getattr(b, \"v\", getattr(b, \"volume\",np.nan))),\n",
        "        })\n",
        "    return pd.DataFrame(rows).set_index(\"timestamp\").sort_index()\n",
        "\n",
        "def get_account_equity(api) -> float:\n",
        "    return float(api.get_account().equity)\n",
        "\n",
        "def get_position(api, symbol: str):\n",
        "    try:\n",
        "        return api.get_position(symbol)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_position_qty(api, symbol: str) -> int:\n",
        "    pos = get_position(api, symbol)\n",
        "    if not pos:\n",
        "        return 0\n",
        "    try:\n",
        "        return int(float(pos.qty))\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def get_last_price(api, symbol: str) -> float:\n",
        "    try:\n",
        "        bars = api.get_bars(symbol, TimeFrame.Minute, limit=1)\n",
        "        if len(bars) > 0:\n",
        "            b = bars[0]\n",
        "            close = getattr(b, \"c\", getattr(b, \"close\", None))\n",
        "            if close is not None:\n",
        "                return float(close)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_last_price via bars failed: {e}\")\n",
        "    pos = get_position(api, symbol)\n",
        "    if pos:\n",
        "        try:\n",
        "            return float(pos.avg_entry_price)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(\"nan\")\n",
        "\n",
        "def cancel_open_symbol_orders(api, symbol: str):\n",
        "    try:\n",
        "        for o in api.list_orders(status=\"open\"):\n",
        "            if o.symbol == symbol:\n",
        "                api.cancel_order(o.id)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] cancel orders failed: {e}\")\n",
        "\n",
        "def market_order_to_qty(api, symbol: str, side: str, qty: int):\n",
        "    if qty <= 0:\n",
        "        return None\n",
        "    if DRY_RUN:\n",
        "        logging.info(f\"[DRY_RUN] Would submit {side} {qty} {symbol} (market, day)\")\n",
        "        return None\n",
        "    try:\n",
        "        o = api.submit_order(\n",
        "            symbol=symbol,\n",
        "            side=side,\n",
        "            type=\"market\",\n",
        "            qty=qty,\n",
        "            time_in_force=\"day\"\n",
        "        )\n",
        "        logging.info(f\"[{symbol}] Submitted {side} {qty} (market) id={o.id}\")\n",
        "        return o\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[{symbol}] submit_order failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ===== Sizing / Risk =====================================================================\n",
        "def action_to_weight(action) -> Tuple[float, float, float]:\n",
        "    a = float(np.array(action).squeeze())\n",
        "    if a <= 0:\n",
        "        return 0.0, abs(np.tanh(a)), a\n",
        "    conf = abs(np.tanh(a))\n",
        "    if SIZING_MODE == \"linear\":\n",
        "        w = WEIGHT_CAP * conf\n",
        "    else:\n",
        "        w = 0.0 if conf < CONF_FLOOR else WEIGHT_CAP * (conf - CONF_FLOOR) / (1.0 - CONF_FLOOR)\n",
        "    return max(0.0, min(WEIGHT_CAP, w)), conf, a\n",
        "\n",
        "def compute_target_qty(equity: float, price: float, target_weight: float) -> int:\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        return 0\n",
        "    notional = equity * target_weight\n",
        "    return max(0, int(notional // price))  # floor\n",
        "\n",
        "def flatten_symbol(api, symbol: str):\n",
        "    qty = get_position_qty(api, symbol)\n",
        "    if qty == 0:\n",
        "        return\n",
        "    cancel_open_symbol_orders(api, symbol)\n",
        "    side = \"sell\" if qty > 0 else \"buy\"\n",
        "    market_order_to_qty(api, symbol, side, abs(qty))\n",
        "\n",
        "def rebalance_to_weight(api, symbol: str, equity: float, target_weight: float):\n",
        "    if target_weight <= 0:\n",
        "        flatten_symbol(api, symbol)\n",
        "        return\n",
        "    price    = get_last_price(api, symbol)\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        logging.warning(f\"[{symbol}] Price unavailable; skipping rebalance this cycle.\")\n",
        "        return\n",
        "    have_qty = get_position_qty(api, symbol)\n",
        "    want_qty = compute_target_qty(equity, price, target_weight)\n",
        "    delta    = want_qty - have_qty\n",
        "    if delta == 0:\n",
        "        return\n",
        "    side = \"buy\" if delta > 0 else \"sell\"\n",
        "    market_order_to_qty(api, symbol, side, abs(delta))\n",
        "\n",
        "# ===== Simple TP/SL (optional) ============================================================\n",
        "def check_tp_sl_and_maybe_flatten(api, symbol: str) -> bool:\n",
        "    if TAKE_PROFIT_PCT <= 0 and STOP_LOSS_PCT <= 0:\n",
        "        return False\n",
        "    pos = get_position(api, symbol)\n",
        "    if not pos:\n",
        "        return False\n",
        "    try:\n",
        "        plpc = float(pos.unrealized_plpc)  # +0.031 = +3.1%\n",
        "    except Exception:\n",
        "        return False\n",
        "    if TAKE_PROFIT_PCT > 0 and plpc >= TAKE_PROFIT_PCT:\n",
        "        logging.info(f\"[{symbol}] TP hit ({plpc:.4f} >= {TAKE_PROFIT_PCT:.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    if STOP_LOSS_PCT > 0 and plpc <= -abs(STOP_LOSS_PCT):\n",
        "        logging.info(f\"[{symbol}] SL hit ({plpc:.4f} <= {-abs(STOP_LOSS_PCT):.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# ===== Inference ==========================================================================\n",
        "def infer_target_weight(model: PPO, vecnorm: Optional[VecNormalize], obs: np.ndarray) -> Tuple[float, float, float]:\n",
        "    x = obs\n",
        "    if vecnorm is not None and hasattr(vecnorm, \"normalize_obs\"):\n",
        "        try:\n",
        "            x = vecnorm.normalize_obs(x)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"VecNormalize.normalize_obs failed; using raw obs. Err: {e}\")\n",
        "    action, _ = model.predict(x, deterministic=INF_DETERMINISTIC)\n",
        "    return action_to_weight(action)\n",
        "\n",
        "# ===== Single-symbol step ================================================================\n",
        "def run_live_once_for_symbol(api, symbol: str, model: PPO, vecnorm: Optional[VecNormalize], features_hint: Optional[dict] = None):\n",
        "    # Discover the expected shape (e.g., (lookback, channels))\n",
        "    shape = expected_obs_shape(model, vecnorm)\n",
        "\n",
        "    # 1) Get fresh bars → features → obs\n",
        "    bars_df = get_recent_bars(api, symbol, limit=200, timeframe=TimeFrame.Minute)\n",
        "    if bars_df.empty:\n",
        "        logging.warning(f\"[{symbol}] No recent bars; skipping.\")\n",
        "        return\n",
        "\n",
        "    obs, obs_ts = prepare_observation_from_bars(\n",
        "        bars_df,\n",
        "        features_hint=features_hint,\n",
        "        min_required_rows=60,\n",
        "        expected_shape=shape,   # <-- IMPORTANT\n",
        "    )\n",
        "\n",
        "    # 2) Staleness guard\n",
        "    if utc_ts(now_utc()) - obs_ts > STALE_MAX_SEC:\n",
        "        logging.warning(f\"[{symbol}] Observation stale (> {STALE_MAX_SEC}s). Skipping.\")\n",
        "        return\n",
        "\n",
        "    # 3) Optional TP/SL pre-check\n",
        "    if check_tp_sl_and_maybe_flatten(api, symbol):\n",
        "        return\n",
        "\n",
        "    # 4) Inference → target weight\n",
        "    target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "    eq = get_account_equity(api)\n",
        "    logging.info(f\"[{symbol}] action={raw:.4f} conf={conf:.3f} → target_w={target_w:.3f} | equity=${eq:,.2f}\")\n",
        "\n",
        "    # 5) Rebalance\n",
        "    rebalance_to_weight(api, symbol, eq, target_w)\n",
        "\n",
        "\n",
        "# ===== Live loop =========================================================================\n",
        "def run_live(tickers: List[str]):\n",
        "    api = init_alpaca()\n",
        "\n",
        "    # Load artifacts per ticker\n",
        "    per_ticker: Dict[str, Tuple[PPO, Optional[VecNormalize], Optional[dict]]] = {}\n",
        "    for t in tickers:\n",
        "        picks      = pick_artifacts_for_ticker(t, ARTIFACTS_DIR, best_window=BEST_WINDOW_ENV)\n",
        "        model      = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm    = load_vecnormalize(picks[\"vecnorm\"]) if picks[\"vecnorm\"] else None\n",
        "        feat_order = load_features(picks[\"features\"])\n",
        "        per_ticker[t] = (model, vecnorm, feat_order)\n",
        "\n",
        "    logging.info(f\"Starting live execution for: {tickers}\")\n",
        "    last_exec_at = now_utc() - timedelta(minutes=COOLDOWN_MIN)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            if not ensure_market_open(api):\n",
        "                logging.info(\"Market closed. Sleeping 30s.\")\n",
        "                time.sleep(30)\n",
        "                continue\n",
        "\n",
        "            if (now_utc() - last_exec_at) < timedelta(minutes=COOLDOWN_MIN):\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "\n",
        "            for t in tickers:\n",
        "                model, vecnorm, feat_hint = per_ticker[t]\n",
        "                run_live_once_for_symbol(api, t, model, vecnorm, features_hint=feat_hint)\n",
        "\n",
        "            last_exec_at = now_utc()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logging.info(\"KeyboardInterrupt: stopping live loop.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            logging.exception(f\"Live loop exception: {e}\")\n",
        "            time.sleep(5)\n",
        "\n",
        "# ===== UNH Diagnostic (friendly prints, works even if market is closed) ===================\n",
        "def unh_diagnostic(\n",
        "    dry_run: bool = True,\n",
        "    timeframe: TimeFrame = TimeFrame.Minute,\n",
        "    limit: int = 300\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs a one-off UNH evaluation, printing messages like your example:\n",
        "      Running strategy for UNH...\n",
        "      Model artifacts loaded for UNH\n",
        "      Prediction for UNH: 0 (1 = Buy, 0 = Sell)\n",
        "      Market is closed.\n",
        "    \"\"\"\n",
        "    ticker = \"UNH\"\n",
        "    print(f\"\\nRunning strategy for {ticker}...\")\n",
        "\n",
        "    # Stats for the summary\n",
        "    processed = 1\n",
        "    loaded    = 0\n",
        "    predicted = 0\n",
        "    closed    = 0\n",
        "    data_err  = 0\n",
        "    model_err = 0\n",
        "    orders    = 0\n",
        "\n",
        "    # Connect & snapshot counts\n",
        "    try:\n",
        "        api = init_alpaca()\n",
        "        positions_start = len(api.list_positions())\n",
        "        orders_start    = len(api.list_orders(status=\"open\"))\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Alpaca: {e}\")\n",
        "        return\n",
        "\n",
        "    # Load artifacts\n",
        "    try:\n",
        "        picks   = pick_artifacts_for_ticker(ticker, ARTIFACTS_DIR, best_window=BEST_WINDOW_ENV)\n",
        "        model   = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm = load_vecnormalize(picks[\"vecnorm\"]) if picks[\"vecnorm\"] else None\n",
        "        feats   = load_features(picks[\"features\"])\n",
        "        print(f\"Model artifacts loaded for {ticker}\")\n",
        "        loaded += 1\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load model for {ticker}: {e}\")\n",
        "        model_err += 1\n",
        "        # Summary\n",
        "        positions_end = len(api.list_positions())\n",
        "        orders_end    = len(api.list_orders(status=\"open\"))\n",
        "        print(\"\\n========== SUMMARY ==========\")\n",
        "        print(f\"Processed:         {processed}\")\n",
        "        print(f\"Models loaded:     {loaded}\")\n",
        "        print(f\"Predictions made:  {predicted}\")\n",
        "        print(f\"Market closed:     {closed}\")\n",
        "        print(f\"Data errors:       {data_err}\")\n",
        "        print(f\"Model errors:      {model_err}\")\n",
        "        print(f\"Orders submitted:  {orders} (dry_run={dry_run})\")\n",
        "        print(f\"Existing positions (start -> end): {positions_start} -> {positions_end}\")\n",
        "        print(f\"Open orders        (start -> end): {orders_start} -> {orders_end}\")\n",
        "        print(\"=============================\")\n",
        "        return\n",
        "\n",
        "    # Fetch bars (recent)\n",
        "    try:\n",
        "        bars_df = get_recent_bars(api, ticker, limit=limit, timeframe=timeframe)\n",
        "        if len(bars_df) < 100:\n",
        "            print(f\"Not enough data for {ticker}: {len(bars_df)} rows\")\n",
        "            data_err += 1\n",
        "            raise ValueError(\"insufficient bars\")\n",
        "        # Features -> obs\n",
        "        obs, obs_ts = prepare_observation_from_bars(bars_df, features_hint=feats, min_required_rows=60)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching/processing bars for {ticker}: {e}\")\n",
        "        data_err += 1\n",
        "        bars_df = None\n",
        "        obs = None\n",
        "\n",
        "    # Predict (binary-like, derived from PPO weight)\n",
        "    signal = None\n",
        "    if obs is not None:\n",
        "        try:\n",
        "            target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "            signal = int(target_w > 0.0)  # 1 = Buy, 0 = Sell/Hold\n",
        "            print(f\"Prediction for {ticker}: {signal} (1 = Buy, 0 = Sell)\")\n",
        "            predicted += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction error for {ticker}: {e}\")\n",
        "            model_err += 1\n",
        "\n",
        "    # Trade execution (dry-run friendly) + market-open info\n",
        "    try:\n",
        "        clock = api.get_clock()\n",
        "        if not clock.is_open:\n",
        "            print(\"Market is closed.\")\n",
        "            closed += 1\n",
        "        else:\n",
        "            # Only attempt order submission if not dry_run and we had a valid signal\n",
        "            if not dry_run and signal is not None:\n",
        "                has_position = False\n",
        "                try:\n",
        "                    pos = api.get_position(ticker)\n",
        "                    has_position = float(pos.qty) > 0\n",
        "                except APIError:\n",
        "                    has_position = False\n",
        "\n",
        "                if signal == 1 and not has_position:\n",
        "                    api.submit_order(symbol=ticker, qty=1, side='buy', type='market', time_in_force='day')\n",
        "                    orders += 1\n",
        "                    print(f\"BUY order submitted for {ticker}\")\n",
        "                elif signal == 0 and has_position:\n",
        "                    api.submit_order(symbol=ticker, qty=1, side='sell', type='market', time_in_force='day')\n",
        "                    orders += 1\n",
        "                    print(f\"SELL order submitted for {ticker}\")\n",
        "                else:\n",
        "                    print(f\"No action taken for {ticker}\")\n",
        "            else:\n",
        "                if signal is not None:\n",
        "                    print(f\"(dry-run) No order submitted for {ticker} — signal={signal}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Trade/clock error for {ticker}: {e}\")\n",
        "\n",
        "    # Final snapshot + summary\n",
        "    positions_end = len(api.list_positions())\n",
        "    orders_end    = len(api.list_orders(status=\"open\"))\n",
        "    print(\"\\n========== SUMMARY ==========\")\n",
        "    print(f\"Processed:         {processed}\")\n",
        "    print(f\"Models loaded:     {loaded}\")\n",
        "    print(f\"Predictions made:  {predicted}\")\n",
        "    print(f\"Market closed:     {closed}\")\n",
        "    print(f\"Data errors:       {data_err}\")\n",
        "    print(f\"Model errors:      {model_err}\")\n",
        "    print(f\"Orders submitted:  {orders} (dry_run={dry_run})\")\n",
        "    print(f\"Existing positions (start -> end): {positions_start} -> {positions_end}\")\n",
        "    print(f\"Open orders        (start -> end): {orders_start} -> {orders_end}\")\n",
        "    print(\"=============================\")\n",
        "\n",
        "# ===== Utilities: quick sanity ============================================================\n",
        "def sanity_check(run_single_step: bool = True):\n",
        "    \"\"\"Connectivity + artifacts + optional one live step for first ticker.\"\"\"\n",
        "    logging.info(\"Sanity: keys=%s/%s base=%s\", bool(API_KEY), bool(API_SECRET), BASE_URL)\n",
        "    api = init_alpaca()\n",
        "    acct = api.get_account()\n",
        "    logging.info(\"Account status: %s | equity=%s | cash=%s\", acct.status, acct.equity, acct.cash)\n",
        "\n",
        "    if not TICKERS:\n",
        "        raise RuntimeError(\"No TICKERS configured.\")\n",
        "    t0 = TICKERS[0]\n",
        "    picks = pick_artifacts_for_ticker(t0, ARTIFACTS_DIR, best_window=BEST_WINDOW_ENV)\n",
        "    logging.info(\"Artifacts for %s -> %s\", t0, {k: (str(v) if v else None) for k,v in picks.items()})\n",
        "\n",
        "    if run_single_step:\n",
        "        model   = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm = load_vecnormalize(picks[\"vecnorm\"]) if picks[\"vecnorm\"] else None\n",
        "        feat    = load_features(picks[\"features\"])\n",
        "        run_live_once_for_symbol(api, t0, model, vecnorm, features_hint=feat)\n",
        "        print(\"One-step OK for\", t0)\n",
        "\n",
        "# ===== Entry (script & notebook friendly) ================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Default to the friendly UNH diagnostic so you see prints even after-hours.\n",
        "    # To switch to the live loop, set AUTO_RUN_LIVE=1 in env before running.\n",
        "    if os.getenv(\"AUTO_RUN_LIVE\", \"0\") == \"1\":\n",
        "        if not TICKERS:\n",
        "            raise SystemExit(\"No TICKERS provided (env var TICKERS).\")\n",
        "        run_live(TICKERS)\n",
        "    else:\n",
        "        unh_diagnostic(dry_run=True, timeframe=TimeFrame.Minute, limit=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bACO23vYtPcR",
        "outputId": "2c665fd0-93db-4f66-add3-cb5028df54a4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running strategy for UNH...\n",
            "Model artifacts loaded for UNH\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:VecNormalize.normalize_obs failed; using raw obs. Err: operands could not be broadcast together with shapes (30,) (10,2) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction error for UNH: Error: Unexpected observation shape (30,) for Box environment, please use (10, 2) or (n_env, 10, 2) for the observation shape.\n",
            "Market is closed.\n",
            "\n",
            "========== SUMMARY ==========\n",
            "Processed:         1\n",
            "Models loaded:     1\n",
            "Predictions made:  0\n",
            "Market closed:     1\n",
            "Data errors:       0\n",
            "Model errors:      1\n",
            "Orders submitted:  0 (dry_run=True)\n",
            "Existing positions (start -> end): 6 -> 6\n",
            "Open orders        (start -> end): 0 -> 0\n",
            "=============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Live-trade-only plotting add-on\n",
        "#   • plots only when fills exist\n",
        "#   • quiet (no chart) if no fills\n",
        "# ================================\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta, timezone\n",
        "\n",
        "# mark when your session started (so we only look for *new* fills)\n",
        "SESSION_START = datetime.now(timezone.utc)\n",
        "\n",
        "def _rfc3339(dt_utc: datetime) -> str:\n",
        "    \"\"\"RFC3339 string for Alpaca 'after' param (must be tz-aware UTC).\"\"\"\n",
        "    if dt_utc.tzinfo is None:\n",
        "        dt_utc = dt_utc.replace(tzinfo=timezone.utc)\n",
        "    else:\n",
        "        dt_utc = dt_utc.astimezone(timezone.utc)\n",
        "    # Alpaca accepts ISO8601/RFC3339 with 'Z'\n",
        "    return dt_utc.isoformat().replace(\"+00:00\", \"Z\")\n",
        "\n",
        "def get_fills_since(api, symbol: str, since_utc: datetime):\n",
        "    \"\"\"\n",
        "    Return a list of fills for `symbol` since `since_utc`.\n",
        "    Each item: dict(symbol, side, qty, price, ts[UTC]).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        acts = api.get_activities(\n",
        "            activity_types=\"FILL\",\n",
        "            after=_rfc3339(since_utc)\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.warning(\"get_activities(FILL) failed: %s\", e)\n",
        "        return []\n",
        "\n",
        "    fills = []\n",
        "    for a in acts or []:\n",
        "        sym = getattr(a, \"symbol\", None) or getattr(a, \"order_symbol\", None)\n",
        "        if str(sym or \"\") != symbol:\n",
        "            continue\n",
        "        t_raw = (\n",
        "            getattr(a, \"transaction_time\", None)\n",
        "            or getattr(a, \"filled_at\", None)\n",
        "            or getattr(a, \"activity_time\", None)\n",
        "        )\n",
        "        try:\n",
        "            ts = pd.to_datetime(t_raw, utc=True)\n",
        "        except Exception:\n",
        "            continue\n",
        "        fills.append({\n",
        "            \"symbol\": symbol,\n",
        "            \"side\":   (getattr(a, \"side\", \"\") or \"\").lower(),  # 'buy'/'sell'\n",
        "            \"qty\":    float(getattr(a, \"qty\", 0) or 0),\n",
        "            \"price\":  float(getattr(a, \"price\", np.nan)) if hasattr(a, \"price\") else np.nan,\n",
        "            \"ts\":     ts\n",
        "        })\n",
        "\n",
        "    fills.sort(key=lambda x: x[\"ts\"])\n",
        "    return fills\n",
        "\n",
        "def _bars_between(api, symbol: str, start_utc: pd.Timestamp, end_utc: pd.Timestamp) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch minute bars in [start, end] and return a normalized OHLCV DF.\n",
        "    Uses your helper get_recent_bars if available; else falls back to direct API.\n",
        "    \"\"\"\n",
        "    # Try your helper first (if present)\n",
        "    try:\n",
        "        # Heuristic: fetch enough minutes to cover the window\n",
        "        mins = max(30, int((end_utc - start_utc).total_seconds() // 60) + 5)\n",
        "        df = get_recent_bars(api, symbol, limit=mins, timeframe=TimeFrame.Minute)\n",
        "        # Filter to the exact window just in case\n",
        "        df = df.loc[(df.index >= start_utc) & (df.index <= end_utc)]\n",
        "        if not df.empty:\n",
        "            return df\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Fallback: direct call with start/end\n",
        "    bars = api.get_bars(\n",
        "        symbol,\n",
        "        TimeFrame.Minute,\n",
        "        start=start_utc.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "        end=end_utc.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "    )\n",
        "    if hasattr(bars, \"df\"):\n",
        "        df = bars.df.copy()\n",
        "        if isinstance(df.index, pd.MultiIndex):\n",
        "            df = df.xs(symbol, level=0)\n",
        "        df.index = pd.to_datetime(df.index, utc=True)\n",
        "        df = df.rename(columns={\"open\":\"Open\",\"high\":\"High\",\"low\":\"Low\",\"close\":\"Close\",\"volume\":\"Volume\"})\n",
        "        return df[[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]].sort_index()\n",
        "    else:\n",
        "        rows = []\n",
        "        for b in bars:\n",
        "            rows.append({\n",
        "                \"timestamp\": pd.to_datetime(getattr(b, \"t\", None), utc=True),\n",
        "                \"Open\":  float(getattr(b, \"o\", getattr(b, \"open\", np.nan))),\n",
        "                \"High\":  float(getattr(b, \"h\", getattr(b, \"high\", np.nan))),\n",
        "                \"Low\":   float(getattr(b, \"l\", getattr(b, \"low\", np.nan))),\n",
        "                \"Close\": float(getattr(b, \"c\", getattr(b, \"close\", np.nan))),\n",
        "                \"Volume\":float(getattr(b, \"v\", getattr(b, \"volume\", np.nan))),\n",
        "            })\n",
        "        return pd.DataFrame(rows).set_index(\"timestamp\").sort_index()\n",
        "\n",
        "def plot_live_trades(api, symbol: str, since_utc: datetime, window_before=\"30min\", window_after=\"30min\"):\n",
        "    \"\"\"\n",
        "    Plot minute price around *new* fills only.\n",
        "    If no fills -> prints a note and returns (no plot).\n",
        "    \"\"\"\n",
        "    fills = get_fills_since(api, symbol, since_utc)\n",
        "    if not fills:\n",
        "        print(f\"No new fills for {symbol}; not plotting.\")\n",
        "        return\n",
        "\n",
        "    # Build a single price series covering all fills + padding windows\n",
        "    first_ts = min(f[\"ts\"] for f in fills)\n",
        "    last_ts  = max(f[\"ts\"] for f in fills)\n",
        "    start = first_ts - pd.Timedelta(window_before)\n",
        "    end   = last_ts  + pd.Timedelta(window_after)\n",
        "\n",
        "    df = _bars_between(api, symbol, start, end)\n",
        "    if df.empty:\n",
        "        print(f\"No bars available around fills for {symbol}; not plotting.\")\n",
        "        return\n",
        "\n",
        "    # Plot\n",
        "    tz = \"America/New_York\"\n",
        "    idx_ny = df.index.tz_convert(tz)\n",
        "    close = df[\"Close\"]\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(idx_ny, close.values, lw=2, label=f\"{symbol} Close\")\n",
        "\n",
        "    # annotate each fill\n",
        "    for f in fills:\n",
        "        ts_ny = f[\"ts\"].tz_convert(tz)\n",
        "        # nearest bar price for marker\n",
        "        try:\n",
        "            near_idx = close.index.get_indexer([f[\"ts\"]], method=\"nearest\")[0]\n",
        "            y = close.iloc[near_idx]\n",
        "        except Exception:\n",
        "            y = np.nan\n",
        "\n",
        "        if f[\"side\"] == \"buy\":\n",
        "            plt.scatter([ts_ny], [y], marker=\"^\", s=80, label=\"BUY fill\", zorder=5)\n",
        "        elif f[\"side\"] == \"sell\":\n",
        "            plt.scatter([ts_ny], [y], marker=\"v\", s=80, label=\"SELL fill\", zorder=5)\n",
        "        else:\n",
        "            plt.scatter([ts_ny], [y], s=70, label=\"FILL\", zorder=5)\n",
        "\n",
        "        # optional dashed vline\n",
        "        plt.axvline(ts_ny, linestyle=\"--\", alpha=0.2)\n",
        "\n",
        "    plt.title(f\"{symbol} — Live fills\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DibQP-mRtcJE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Trade Summary (aligned with PPO logs: signal/raw_action/weight/confidence) ===\n",
        "print(\"\\nTrade Summary:\")\n",
        "for ticker in tickers:\n",
        "    log_file = os.path.join(RESULTS_DIR, f\"trade_log_{ticker}.csv\")\n",
        "    if not os.path.exists(log_file):\n",
        "        print(f\"{ticker}: no trades logged yet.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(log_file, on_bad_lines=\"skip\", parse_dates=[\"log_time\",\"bar_time\"], infer_datetime_format=True)\n",
        "        # Count by 'signal' (BUY/SELL/HOLD). If your older logs used 'action', fall back.\n",
        "        key = \"signal\" if \"signal\" in df.columns else (\"action\" if \"action\" in df.columns else None)\n",
        "        if key:\n",
        "            counts = df[key].value_counts().to_dict()\n",
        "            print(f\"{ticker}: {counts}\")\n",
        "        else:\n",
        "            print(f\"{ticker}: log present but missing 'signal'/'action' columns.\")\n",
        "\n",
        "        # Confidence histogram (matplotlib only)\n",
        "        if \"confidence\" in df.columns and df[\"confidence\"].notna().any():\n",
        "            plt.figure(figsize=(8, 3.5))\n",
        "            df[\"confidence\"].dropna().plot(kind=\"hist\", bins=10, edgecolor=\"black\")\n",
        "            plt.title(f\"{ticker} - Confidence Distribution\")\n",
        "            plt.xlabel(\"confidence\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # Optional: summary stats for PPO-specific fields\n",
        "        for col in [\"weight\", \"raw_action\"]:\n",
        "            if col in df.columns and df[col].notna().any():\n",
        "                s = df[col].dropna()\n",
        "                print(f\"{ticker} {col}: mean={s.mean():.3f}, std={s.std():.3f}, min={s.min():.3f}, max={s.max():.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{ticker}: could not summarize trades: {e}\")\n",
        "\n",
        "# === Summary of Positions ===\n",
        "try:\n",
        "    positions = api.list_positions()\n",
        "    total_market_value = 0.0\n",
        "    print(\"\\nPosition Summary:\")\n",
        "    for p in positions:\n",
        "        mv = float(p.market_value)\n",
        "        total_market_value += mv\n",
        "        print(f\"  {p.symbol}: {p.qty} shares @ ${float(p.current_price):.2f} | Value: ${mv:,.2f}\")\n",
        "    print(f\"\\nTotal Market Value: ${total_market_value:,.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not summarize positions: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVDk8hn5wJqY",
        "outputId": "5b339c4e-8898-48a0-eeb4-68b5a07a7764"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trade Summary:\n",
            "ABT: no trades logged yet.\n",
            "BRK.B: no trades logged yet.\n",
            "MDT: no trades logged yet.\n",
            "PG: no trades logged yet.\n",
            "PM: no trades logged yet.\n",
            "\n",
            "Position Summary:\n",
            "  AAPL: 2 shares @ $237.86 | Value: $475.72\n",
            "  ABT: 1 shares @ $132.20 | Value: $132.20\n",
            "  BRK.B: 1 shares @ $493.49 | Value: $493.49\n",
            "  MDT: 1 shares @ $92.96 | Value: $92.96\n",
            "  PG: 1 shares @ $159.18 | Value: $159.18\n",
            "  PM: 1 shares @ $162.72 | Value: $162.72\n",
            "\n",
            "Total Market Value: $1,516.27\n"
          ]
        }
      ]
    }
  ]
}