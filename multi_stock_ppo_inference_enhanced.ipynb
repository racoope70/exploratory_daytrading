{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/daytrading-with-ml/blob/main/multi_stock_ppo_inference_enhanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install yfinance pywavelets transformers --upgrade"
      ],
      "metadata": {
        "id": "PJPENN3xz_Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get remove --purge -y cuda* libcuda* nvidia* || echo \"No conflicting CUDA packages\"\n",
        "!apt-get autoremove -y\n",
        "!apt-get clean"
      ],
      "metadata": {
        "id": "0xMJutMCD_uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!apt-get update -qq && apt-get install -y     libcusover11 libcusparse11 libcurand10 libcufft10 libnppig10 ibnppc10 libnppial10\n",
        "   cuda-toolkit-12-4\n"
      ],
      "metadata": {
        "id": "eANWApmOE_ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==3.20.3\n"
      ],
      "metadata": {
        "id": "DyLxSfGAFFcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --extra-index-url=https://pypi.nvidia.com\n",
        "    cuml-cu12==25.2.0 cudf-cu12==25.2.0 cupy-cuda12x\n",
        "    dask-cuda==25.2.0 dask-cudf-cu12==25.2.0\n"
      ],
      "metadata": {
        "id": "IxNjaKmaFI0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba==0.60.0\n"
      ],
      "metadata": {
        "id": "NnmelMY0FMO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra] gymnasium gym-anytrading yfinance xgboost joblib\n",
        "!pip install matplotlib scikit-learn pandas\n"
      ],
      "metadata": {
        "id": "YZaPFzTYFQ3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.18.0\n"
      ],
      "metadata": {
        "id": "v8lDHud8FS72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n"
      ],
      "metadata": {
        "id": "K-iHY6_pFVEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"TensorFlow GPU memory config failed: {e}\")\n"
      ],
      "metadata": {
        "id": "1KUGG5IaFWl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-12.4'\n",
        "os.environ['PATH'] += ':/usr/local/cuda-12.4/bin'\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/cuda-12.4/lib64'\n"
      ],
      "metadata": {
        "id": "RdkCCYU1FApi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz6JMzAHdEJZ"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxTo_GOSemtU"
      },
      "outputs": [],
      "source": [
        "#Step 7: authenticate with hugging face hub (optional)\n",
        "#This allows for better access and avoids rate limits when downloading public models/datasets\n",
        "\n",
        "# Authenticate with Hugging Face Hub\n",
        "#notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9AoPa7K4FEK"
      },
      "outputs": [],
      "source": [
        "# === Download + Feature Engineering (standalone) ===\n",
        "# --- Imports ---\n",
        "import os, gc, time, json, pywt, logging\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from google.colab import drive\n",
        "\n",
        "# ---------------- Logging ----------------\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# ---------------- Google Drive ----------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "USE_REGIME = True  # toggle\n",
        "\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "ticker_list = [\n",
        "    'AAPL','TSLA','MSFT','GOOGL','AMZN','NVDA','META','BRK-B','JPM','JNJ',\n",
        "    'XOM','V','PG','UNH','MA','HD','LLY','MRK','PEP','KO',\n",
        "    'BAC','ABBV','AVGO','PFE','COST','CSCO','TMO','ABT','ACN','WMT',\n",
        "    'MCD','ADBE','DHR','CRM','NKE','INTC','QCOM','NEE','AMD','TXN',\n",
        "    'AMGN','UPS','LIN','PM','UNP','BMY','LOW','RTX','CVX','IBM',\n",
        "    'GE','SBUX','ORCL'\n",
        "]\n",
        "strategy_name = \"ppo_walkforward_v1\"\n",
        "\n",
        "TEST_MODE = True  # set False to process full list\n",
        "SYMBOLS = ['AAPL', 'NVDA', 'MSFT'] if TEST_MODE else ticker_list\n",
        "\n",
        "# Data horizon\n",
        "end_date = datetime.today()\n",
        "start_date = end_date - timedelta(days=730)\n",
        "INTERVAL = \"1h\"\n",
        "\n",
        "# Output paths\n",
        "LOCAL_OUT = \"multi_stock_feature_engineered_dataset.csv\"\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/trading_data/\"\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------- Sentiment Toggle ----------------\n",
        "USE_SENTIMENT = False  # set True to enable FinBERT\n",
        "\n",
        "if USE_SENTIMENT:\n",
        "    from transformers import pipeline\n",
        "    try:\n",
        "        import torch\n",
        "        device_id = 0 if torch.cuda.is_available() else -1\n",
        "    except Exception:\n",
        "        device_id = -1\n",
        "    sentiment_pipeline = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"ProsusAI/finbert\",\n",
        "        device=device_id\n",
        "    )\n",
        "else:\n",
        "    sentiment_pipeline = None  # ensures code path stays simple when off\n",
        "\n",
        "# ---------------- Helpers ----------------\n",
        "def download_stock_data(ticker, start_date=None, end_date=None, interval=\"1h\",\n",
        "                        max_retries=5, window_days=730):\n",
        "    \"\"\"Download OHLCV from yfinance with retries.\"\"\"\n",
        "    if start_date is None or end_date is None:\n",
        "        end_date = datetime.today()\n",
        "        start_date = end_date - timedelta(days=window_days)\n",
        "    start_str = start_date.strftime('%Y-%m-%d')\n",
        "    end_str = end_date.strftime('%Y-%m-%d')\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            logging.info(f\"ðŸ“¥ Attempt {attempt}: {ticker} {interval} {start_str}â†’{end_str}\")\n",
        "            df = yf.download(\n",
        "                ticker,\n",
        "                start=start_str,\n",
        "                end=end_str,\n",
        "                interval=interval,\n",
        "                progress=False,\n",
        "                auto_adjust=False\n",
        "            )\n",
        "            if not df.empty:\n",
        "                df = df.copy()\n",
        "                df.reset_index(inplace=True)\n",
        "                df['Symbol'] = ticker\n",
        "                # yfinance uses 'Datetime' for intraday, 'Date' for daily\n",
        "                dt_col = 'Datetime' if 'Datetime' in df.columns else 'Date'\n",
        "                df['Datetime'] = pd.to_datetime(df[dt_col])\n",
        "\n",
        "                # ensure unique, sorted timestamps\n",
        "                df = (\n",
        "                    df.drop_duplicates(subset=['Datetime'])\n",
        "                      .sort_values('Datetime')\n",
        "                      .reset_index(drop=True)\n",
        "                )\n",
        "                return df\n",
        "            else:\n",
        "                raise ValueError(\"Empty data\")\n",
        "        except Exception as e:\n",
        "            wait = attempt * 5\n",
        "            logging.warning(f\"âš ï¸ {ticker} download error: {e} | retrying in {wait}s\")\n",
        "            time.sleep(wait)\n",
        "\n",
        "    logging.error(f\"âŒ Failed to download {ticker}\")\n",
        "    return None\n",
        "\n",
        "def denoise_wavelet(series, wavelet='db1', level=2):\n",
        "    s = pd.Series(series).astype(float).ffill().bfill().to_numpy()\n",
        "    try:\n",
        "        coeffs = pywt.wavedec(s, wavelet, mode='symmetric', level=level)\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = np.zeros_like(coeffs[i])\n",
        "        rec = pywt.waverec(coeffs, wavelet, mode='symmetric')\n",
        "        return pd.Series(rec[:len(s)], index=series.index)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Wavelet denoising failed ({e}); using raw Close.\")\n",
        "        return pd.Series(s, index=series.index)\n",
        "\n",
        "def score_sentiment(texts):\n",
        "    \"\"\"Return sentiment scores aligned to texts; neutral when disabled.\"\"\"\n",
        "    if not USE_SENTIMENT or sentiment_pipeline is None:\n",
        "        return [0] * len(texts)\n",
        "    try:\n",
        "        outputs = sentiment_pipeline(texts, truncation=True, max_length=256, batch_size=64)\n",
        "        scores = []\n",
        "        for r in outputs:\n",
        "            label = r['label'].lower()\n",
        "            if label == 'positive':\n",
        "                scores.append(+float(r['score']))\n",
        "            elif label == 'negative':\n",
        "                scores.append(-float(r['score']))\n",
        "            else:  # neutral\n",
        "                scores.append(0.0)\n",
        "        return scores\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Sentiment scoring error: {e}\")\n",
        "        return [0] * len(texts)\n",
        "def add_regime(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # 2 fast, robust signals: rolling vol & 20â€‘bar return\n",
        "    df['Vol20'] = df['Close'].pct_change().rolling(20).std()\n",
        "    df['Ret20'] = df['Close'].pct_change(20)\n",
        "    # 4-bin regime: (high/low vol) x (trending/ranging)\n",
        "    vol_hi   = (df['Vol20'] > df['Vol20'].median()).astype(int)\n",
        "    trend_hi = (df['Ret20'].abs() > df['Ret20'].abs().median()).astype(int)\n",
        "    df['Regime4'] = vol_hi * 2 + trend_hi  # 0..3\n",
        "    return df\n",
        "\n",
        "def compute_enhanced_features(df):\n",
        "    df = df.copy()\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = df.columns.get_level_values(0)\n",
        "    df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "    # --- Technicals ---\n",
        "    df['SMA_20'] = df['Close'].rolling(20).mean()\n",
        "    df['STD_20'] = df['Close'].rolling(20).std()\n",
        "    df['Upper_Band'] = df['SMA_20'] + 2 * df['STD_20']\n",
        "    df['Lower_Band'] = df['SMA_20'] - 2 * df['STD_20']\n",
        "\n",
        "    df['Lowest_Low'] = df['Low'].rolling(14).min()\n",
        "    df['Highest_High'] = df['High'].rolling(14).max()\n",
        "    denom = (df['Highest_High'] - df['Lowest_Low']).replace(0, np.nan)\n",
        "    df['Stoch'] = ((df['Close'] - df['Lowest_Low']) / denom) * 100\n",
        "\n",
        "    df['ROC'] = df['Close'].pct_change(10)\n",
        "    df['OBV'] = (np.sign(df['Close'].diff()).fillna(0) * df['Volume'].fillna(0)).cumsum()\n",
        "\n",
        "    tp = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "    sma_tp = tp.rolling(20).mean()\n",
        "    md = (tp - sma_tp).abs().rolling(20).mean()\n",
        "    df['CCI'] = (tp - sma_tp) / (0.015 * md)\n",
        "\n",
        "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
        "    ema12 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "    df['MACD_Line'] = ema12 - ema26\n",
        "    df['MACD_Signal'] = df['MACD_Line'].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    delta = df['Close'].diff()\n",
        "    gain = delta.clip(lower=0).rolling(14).mean()\n",
        "    loss = (-delta.clip(upper=0)).rolling(14).mean()\n",
        "    rs = gain / (loss.replace(0, np.nan))\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    tr = pd.concat([\n",
        "        (df['High'] - df['Low']),\n",
        "        (df['High'] - df['Close'].shift()).abs(),\n",
        "        (df['Low'] - df['Close'].shift()).abs()\n",
        "    ], axis=1).max(axis=1)\n",
        "    df['ATR'] = tr.rolling(14).mean()\n",
        "\n",
        "    df['Volatility'] = df['Close'].pct_change().rolling(20).std()\n",
        "\n",
        "    df['Return'] = (df['Close'].shift(-10) - df['Close']) / df['Close']\n",
        "    df['Target'] = np.select([df['Return'] > 0.02, df['Return'] < -0.02], [1, -1], default=0)\n",
        "\n",
        "    # --- Denoised Close ---\n",
        "    df['Denoised_Close'] = denoise_wavelet(df['Close'].ffill())\n",
        "    # --- Optional market regime feature ---\n",
        "    if USE_REGIME:\n",
        "        df = add_regime(df)\n",
        "\n",
        "    # --- Sentiment (toggle) ---\n",
        "    headline = f\"{df['Symbol'].iloc[0]} is expected to perform well in the market.\" if len(df) else \"\"\n",
        "    df['Mock_Headline'] = headline\n",
        "    try:\n",
        "        single_score = score_sentiment([headline])[0] if (USE_SENTIMENT and len(df)) else 0.0\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Sentiment scoring failed for {df['Symbol'].iloc[0] if 'Symbol' in df else '?'}: {e}\")\n",
        "        single_score = 0.0\n",
        "    df['SentimentScore'] = float(single_score)\n",
        "    df.drop(columns=['Mock_Headline'], errors='ignore', inplace=True)\n",
        "\n",
        "# inside compute_enhanced_features(...)\n",
        "if USE_REGIME:\n",
        "    df = add_regime(df)\n",
        "\n",
        "    # --- Greeks-ish (simple proxies) ---\n",
        "    df['Delta'] = df['Close'].pct_change(1).fillna(0)\n",
        "    df['Gamma'] = df['Delta'].diff().fillna(0)\n",
        "\n",
        "    # Final cleanup\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    cols = [c for c in df.columns if c not in ['Target', 'Return', 'Symbol']] + ['Target', 'Return', 'Symbol']\n",
        "    return df[cols]\n",
        "\n",
        "# ---------------- Main Loop ----------------\n",
        "all_dfs = []\n",
        "for i, ticker in enumerate(SYMBOLS, 1):\n",
        "    logging.info(f\"[{i}/{len(SYMBOLS)}] Processing {ticker}\")\n",
        "    raw = download_stock_data(ticker, start_date, end_date, interval=INTERVAL)\n",
        "    if raw is None or raw.empty:\n",
        "        logging.warning(f\"No data for {ticker}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        features = compute_enhanced_features(raw)\n",
        "        if features is not None and not features.empty:\n",
        "            all_dfs.append(features)\n",
        "        else:\n",
        "            logging.warning(f\"Feature set empty for {ticker}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Feature engineering failed for {ticker}: {e}\")\n",
        "    finally:\n",
        "        del raw\n",
        "        try:\n",
        "            del features\n",
        "        except NameError:\n",
        "            pass\n",
        "        gc.collect()\n",
        "\n",
        "# ---------------- Save ----------------\n",
        "if all_dfs:\n",
        "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
        "    logging.info(f\"âœ… Combined dataset shape: {final_df.shape}\")\n",
        "\n",
        "    # save locally\n",
        "    final_df.to_csv(LOCAL_OUT, index=False)\n",
        "    logging.info(f\"Saved local CSV: {LOCAL_OUT}\")\n",
        "\n",
        "    # save to Drive\n",
        "    drive_csv = os.path.join(DRIVE_DIR, \"multi_stock_feature_engineered_dataset.csv\")\n",
        "    final_df.to_csv(drive_csv, index=False)\n",
        "    logging.info(f\"Saved to Google Drive: {drive_csv}\")\n",
        "\n",
        "    # free memory\n",
        "    del all_dfs, final_df\n",
        "    gc.collect()\n",
        "else:\n",
        "    logging.warning(\"No usable data found for any ticker.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47KOlGdVbGMl"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"multi_stock_feature_engineered_dataset.csv\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === PPO Walkforward with Runtime Timing, Full Model Saving, and Logging ===\n",
        "import os, gc, time, json, torch, logging, joblib\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
        "import heapq\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from gym_anytrading.envs import StocksEnv\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "set_random_seed(42)\n",
        "\n",
        "# === Configuration ===\n",
        "\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/Results_May_2025/ppo_walkforward_results\"\n",
        "FINAL_MODEL_DIR = os.path.join(RESULTS_DIR, \"models\")\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# === Logging Setup ===\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# === Flags ===\n",
        "\n",
        "ENABLE_SENTIMENT = False\n",
        "ENABLE_SLO = True\n",
        "ENABLE_WAVELET = True\n",
        "test_mode = True\n",
        "ENABLE_PLOTS = False\n",
        "LIVE_MODE = False           # simple live/paper inference loop (set True to run)\n",
        "SIM_LATENCY_MS = 150        # network/broker latency simulation; 0 = off\n",
        "BROKER = \"log\"              # \"log\" = do not place orders, just log\n",
        "\n",
        "\n",
        "# === Load Dataset ===\n",
        "\n",
        "if not os.path.exists(\"multi_stock_feature_engineered_dataset.csv\"):\n",
        "raise FileNotFoundError(\"Required feature-engineered dataset not found!\")\n",
        "df = pd.read_csv(\"multi_stock_feature_engineered_dataset.csv\")\n",
        "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
        "\n",
        "# === PPO-Compatible Environment ===\n",
        "class ContinuousTradingEnv(StocksEnv):\n",
        "    def __init__(self, df, frame_bound, window_size):\n",
        "        super().__init__(df=df.reset_index(drop=True), frame_bound=frame_bound, window_size=window_size)\n",
        "        self.action_space = Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        try:\n",
        "            # ---- map continuous -> discrete ----\n",
        "            a = float(np.array(action).squeeze())\n",
        "            if a < -0.3:\n",
        "                discrete_action = 0   # sell\n",
        "            elif a > 0.1:\n",
        "                discrete_action = 1   # buy\n",
        "            else:\n",
        "                discrete_action = 2   # hold\n",
        "\n",
        "            # parent step (gym-anytrading may return 4- or 5-tuple)\n",
        "            step_result = super().step(discrete_action)\n",
        "            if len(step_result) == 5:\n",
        "                obs, reward, terminated, truncated, info = step_result\n",
        "                done = bool(terminated or truncated)\n",
        "            else:\n",
        "                obs, reward, done, info = step_result\n",
        "\n",
        "            # ---- execution pricing (with optional slippage) ----\n",
        "            cur  = self.df.loc[self._current_tick, 'Close']\n",
        "            prev = self.df.loc[max(self._current_tick - 1, 0), 'Close']\n",
        "\n",
        "            slippage_pct = 0.002 if ENABLE_SLO else 0.0\n",
        "            if discrete_action == 1:       # buy\n",
        "                fill = cur * (1 + slippage_pct)\n",
        "            elif discrete_action == 0:     # sell\n",
        "                fill = cur * (1 - slippage_pct)\n",
        "            else:                          # hold\n",
        "                fill = cur\n",
        "\n",
        "            # base PnL shaping (directional)\n",
        "            reward += (fill - prev) * (1 if discrete_action == 1 else (-1 if discrete_action == 0 else 0))\n",
        "            # small encouragement for decisive actions\n",
        "            reward += abs(a) * 0.01\n",
        "\n",
        "            # optional wavelet trend shaping (tiny nudge)\n",
        "            if ENABLE_WAVELET and 'Denoised_Close' in self.df.columns:\n",
        "                prev_idx = max(self._current_tick - 1, 0)\n",
        "                dn_delta = float(self.df.loc[self._current_tick, 'Denoised_Close'] -\n",
        "                                 self.df.loc[prev_idx, 'Denoised_Close'])\n",
        "                reward += 0.001 * np.tanh(dn_delta)\n",
        "\n",
        "            # optional sentiment shaping\n",
        "            if ENABLE_SENTIMENT and 'SentimentScore' in self.df.columns:\n",
        "                sent = float(self.df.loc[self._current_tick, 'SentimentScore'])\n",
        "                reward += sent * 0.01\n",
        "\n",
        "            # final clip + return 4-tuple for SB3 VecEnv\n",
        "            return obs, float(np.clip(reward, -1.0, 1.0)), done, info\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Step error: {e}\")\n",
        "            try:\n",
        "                obs = self.reset()\n",
        "            except Exception:\n",
        "                obs = super().reset() if hasattr(super(), \"reset\") else None\n",
        "            return obs, 0.0, True, {}\n",
        "\n",
        "# === Utilities ===\n",
        "\n",
        "def plot_performance(ticker, portfolio, hold_value):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(portfolio, label=\"PPO Portfolio\")\n",
        "    plt.axhline(y=hold_value, color=\"r\", linestyle=\"--\", label=\"Buy & Hold\")\n",
        "    plt.title(f\"{ticker} - PPO vs Buy & Hold\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, f\"{ticker}_performance.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def get_walk_forward_windows(df, window_size=3000, step_size=500, min_len=1000):\n",
        "    return [\n",
        "        (start, start + window_size)\n",
        "        for start in range(0, len(df) - min_len, step_size)\n",
        "        if start + window_size < len(df)\n",
        "    ]\n",
        "\n",
        "# === Save QuantConnect-Compatible Artifacts (fixed) ===\n",
        "\n",
        "def save_quantconnect_model(artifact, prefix, save_dir):\n",
        "    model_obj = artifact.get(\"model\", None)\n",
        "    model_path = os.path.join(save_dir, f\"{prefix}_model.zip\")\n",
        "    if model_obj is not None and not os.path.exists(model_path):\n",
        "        model_obj.save(model_path)\n",
        "\n",
        "    vecnorm_src = artifact.get(\"vecnorm_path\")\n",
        "    if vecnorm_src:\n",
        "        try:\n",
        "            vecnorm_dst = os.path.join(save_dir, f\"{prefix}_vecnorm.pkl\")\n",
        "            if os.path.abspath(vecnorm_src) != os.path.abspath(vecnorm_dst):\n",
        "                import shutil\n",
        "                shutil.copyfile(vecnorm_src, vecnorm_dst)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"VecNormalize file handling issue for {prefix}: {e}\")\n",
        "\n",
        "    with open(os.path.join(save_dir, f\"{prefix}_features.json\"), \"w\") as f:\n",
        "        json.dump({\"features\": artifact['features']}, f)\n",
        "\n",
        "    with open(os.path.join(save_dir, f\"{prefix}_probability_config.json\"), \"w\") as f:\n",
        "        json.dump({\"threshold\": 0.05, \"use_confidence\": True, \"inference_mode\": \"deterministic\"}, f)\n",
        "\n",
        "    with open(os.path.join(save_dir, f\"{prefix}_model_info.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            \"model\": \"PPO\",\n",
        "            \"ticker\": artifact['result']['Ticker'],\n",
        "            \"window\": artifact['result']['Window'],\n",
        "            \"date_trained\": datetime.today().strftime(\"%Y-%m-%d\"),\n",
        "            \"framework\": \"stable-baselines3\",\n",
        "            \"input_features\": artifact['features'],\n",
        "            \"final_portfolio\": artifact['result']['PPO_Portfolio'],\n",
        "            \"buy_hold\": artifact['result']['BuyHold'],\n",
        "            \"sharpe\": artifact['result']['Sharpe']\n",
        "        }, f)\n",
        "\n",
        "    logging.info(f\"Saved QuantConnect-compatible model for {artifact['result']['Ticker']} | {artifact['result']['Window']}\")\n",
        "\n",
        "# ----- Live inference helpers (uses saved PPO + VecNormalize) -----\n",
        "\n",
        "def load_model_and_env(prefix):\n",
        "    \"\"\"Load a trained PPO and create a factory to build a matching env window.\"\"\"\n",
        "    model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "    vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "    model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "    def make_env(df_window):\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "        e = DummyVecEnv([lambda: ContinuousTradingEnv(\n",
        "            df=df_window, frame_bound=frame_bound, window_size=10\n",
        "        )])\n",
        "        if os.path.exists(vec_path):\n",
        "            e = VecNormalize.load(vec_path, e)\n",
        "        # eval mode: do not update running stats\n",
        "        e.training = False\n",
        "        e.norm_reward = False\n",
        "        return e\n",
        "\n",
        "    return model, make_env\n",
        "\n",
        "def latest_df_for_symbol(symbol, horizon_days=5, interval=\"1m\"):\n",
        "    \"\"\"Fetch fresh bars and rebuild features exactly like training.\"\"\"\n",
        "    end = datetime.utcnow()\n",
        "    start = end - timedelta(days=horizon_days)\n",
        "    df = yf.download(\n",
        "        symbol,\n",
        "        start=start.strftime(\"%Y-%m-%d\"),\n",
        "        end=end.strftime(\"%Y-%m-%d\"),\n",
        "        interval=interval,\n",
        "        progress=False,\n",
        "        auto_adjust=False\n",
        "    )\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "    df = df.reset_index()\n",
        "    df['Symbol'] = symbol\n",
        "    # IMPORTANT: reuse your same feature function\n",
        "    return compute_enhanced_features(df)\n",
        "\n",
        "def predict_latest(symbol, prefix):\n",
        "    \"\"\"Build last window, fastâ€‘forward env, call model.predict(), return a signal.\"\"\"\n",
        "    model, make_env = load_model_and_env(prefix)\n",
        "\n",
        "    live_df = latest_df_for_symbol(symbol)\n",
        "    if live_df is None or len(live_df) < 100:\n",
        "        logging.warning(\"No fresh data yet for live inference.\")\n",
        "        return None\n",
        "\n",
        "    df_window = live_df.iloc[-2500:].reset_index(drop=True) if len(live_df) > 2500 else live_df.copy()\n",
        "\n",
        "    env = make_env(df_window)\n",
        "    obs = env.reset()\n",
        "    # fastâ€‘forward to end (no-op actions just to advance)\n",
        "    for _ in range(len(df_window) - 1):\n",
        "        obs, _, done, _ = env.step([0.0])\n",
        "        if isinstance(done, (np.ndarray, list)) and done[0]:\n",
        "            break\n",
        "\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    a = float(np.array(action).squeeze())\n",
        "    signal = \"BUY\" if a > 0.1 else (\"SELL\" if a < -0.3 else \"HOLD\")\n",
        "    conf = abs(a)\n",
        "    ts = df_window['Datetime'].iloc[-1] if 'Datetime' in df_window.columns else None\n",
        "    price = float(df_window['Close'].iloc[-1])\n",
        "    return dict(signal=signal, confidence=conf, action=a, ts=ts, price=price)\n",
        "\n",
        "def place_order(signal, qty=1):\n",
        "    \"\"\"Stub broker router with latency simulation; logs in Colab.\"\"\"\n",
        "    if SIM_LATENCY_MS > 0:\n",
        "        time.sleep(SIM_LATENCY_MS / 1000.0)\n",
        "    if BROKER == \"log\":\n",
        "        logging.info(f\"[PAPER] {signal} x{qty}\")\n",
        "    else:\n",
        "        # Hook up real broker SDK here (Alpaca, IB, etc.)\n",
        "        logging.info(f\"[BROKER={BROKER}] {signal} x{qty} (not implemented)\")\n",
        "\n",
        "def live_loop(symbol, best_prefix):\n",
        "    \"\"\"Simple polling loopâ€”set LIVE_MODE=True to run.\"\"\"\n",
        "    while LIVE_MODE:\n",
        "        try:\n",
        "            pred = predict_latest(symbol, best_prefix)\n",
        "            if pred:\n",
        "                logging.info(\n",
        "                    f\"{symbol} {pred['ts']} | {pred['signal']} @ {pred['price']:.2f} (conf {pred['confidence']:.2f})\"\n",
        "                )\n",
        "                place_order(pred['signal'], qty=1)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Live loop error: {e}\")\n",
        "        time.sleep(60)  # poll each minute\n",
        "\n",
        "TOP_N_WINDOWS = 3\n",
        "\n",
        "# === PPO Walkforward Function ===\n",
        "\n",
        "def walkforward_ppo(df, ticker, window_size=2500, step_size=500,\n",
        "                    timesteps=150_000, learning_rate=2e-5, ppo_overrides=None):\n",
        "    if ppo_overrides is None:\n",
        "        ppo_overrides = {}\n",
        "    if len(df) < window_size:\n",
        "        logging.warning(f\"Skipping {ticker}: only {len(df)} rows (min required: {window_size})\")\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    windows = get_walk_forward_windows(df, window_size, step_size)\n",
        "    top_heap = []  # (Sharpe, prefix, meta)\n",
        "\n",
        "    for w_idx, (start, end) in enumerate(windows):\n",
        "        window_start_time = time.time()\n",
        "        gc.collect()\n",
        "\n",
        "        df_window = df.iloc[start:end].reset_index(drop=True)\n",
        "        if len(df_window) <= 52 or len(df_window) % 2 != 0:\n",
        "            df_window = df_window.iloc[:-1]\n",
        "\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "        env = DummyVecEnv([lambda: ContinuousTradingEnv(df=df_window, frame_bound=frame_bound, window_size=10)])\n",
        "        env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
        "\n",
        "        try:\n",
        "            model = PPO(\n",
        "                \"MlpPolicy\",\n",
        "                env,\n",
        "                verbose=0,\n",
        "                device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "                learning_rate=ppo_overrides.get(\"lr\", learning_rate),\n",
        "                n_steps=ppo_overrides.get(\"n_steps\", 256),\n",
        "                batch_size=ppo_overrides.get(\"batch\", 64),\n",
        "                n_epochs=8,\n",
        "                gamma=0.99,\n",
        "                gae_lambda=0.95,\n",
        "                clip_range=ppo_overrides.get(\"clip\", 0.2),\n",
        "                ent_coef=ppo_overrides.get(\"ent\", 0.005),\n",
        "                policy_kwargs=dict(net_arch=[64, 64]),\n",
        "            )\n",
        "\n",
        "            logging.info(f\"Training {ticker} Window {w_idx+1}/{len(windows)}\")\n",
        "            model.learn(total_timesteps=timesteps)\n",
        "\n",
        "            # === Prepare env for evaluation (raw rewards, no running stats updates) ===\n",
        "            env.training = False\n",
        "            env.norm_reward = False\n",
        "\n",
        "            # === Evaluation ===\n",
        "            obs = env.reset()\n",
        "            pred_rows = []\n",
        "            position, balance, portfolio = 0.0, 100000.0, []\n",
        "            trade_log = []\n",
        "            entry_price, last_trade_step = None, -10\n",
        "            confidence_buckets = {'Low': 0, 'Mid': 0, 'High': 0}\n",
        "            cooldown_skips = []\n",
        "            TRADE_COOLDOWN_STEPS = max(1, int(0.3 * len(df_window) / 100))\n",
        "\n",
        "            # define prefix BEFORE we save predictions later\n",
        "            prefix = f\"ppo_{ticker}_window{w_idx+1}\"\n",
        "\n",
        "            for i in range(len(df_window)):\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                obs, reward, done, info = env.step(action)\n",
        "                if isinstance(done, (np.ndarray, list)):\n",
        "                    done = bool(done[0])\n",
        "\n",
        "                a = float(np.array(action).squeeze())\n",
        "                aa = abs(a)  # <-- add this\n",
        "\n",
        "                # map to string signal for logging\n",
        "                if a < -0.3:\n",
        "                    pred_signal = \"SELL\"\n",
        "                elif a > 0.1:\n",
        "                    pred_signal = \"BUY\"\n",
        "                else:\n",
        "                    pred_signal = \"HOLD\"\n",
        "                confidence = aa\n",
        "\n",
        "                price = float(df_window['Close'].iloc[i])\n",
        "                STOP_LOSS_PCT, TAKE_PROFIT_PCT = 0.04, 0.12\n",
        "                prior_position = position\n",
        "\n",
        "                # stop-loss / take-profit\n",
        "                if position > 0 and entry_price is not None:\n",
        "                    change = (price - entry_price) / entry_price\n",
        "                    if change < -STOP_LOSS_PCT or change > TAKE_PROFIT_PCT:\n",
        "                        balance = position * price\n",
        "                        position = 0.0\n",
        "                        trade_log.append(\"STOP-LOSS\" if change < -STOP_LOSS_PCT else \"TAKE-PROFIT\")\n",
        "                        entry_price = None\n",
        "\n",
        "                # trading logic with cooldown\n",
        "                if aa > 0.02 and (i - last_trade_step) > TRADE_COOLDOWN_STEPS:\n",
        "                    if a > 0 and position == 0.0:\n",
        "                        position = balance / price\n",
        "                        balance = 0.0\n",
        "                        entry_price = price\n",
        "                        trade_log.append(\"BUY\")\n",
        "                        last_trade_step = i\n",
        "                    elif a < 0 and position > 0.0:\n",
        "                        balance = position * price\n",
        "                        position = 0.0\n",
        "                        trade_log.append(\"SELL\")\n",
        "                        last_trade_step = i\n",
        "                    else:\n",
        "                        trade_log.append(\"HOLD\")\n",
        "                else:\n",
        "                    trade_log.append(\"HOLD\")\n",
        "                    if (i - last_trade_step) <= TRADE_COOLDOWN_STEPS:\n",
        "                        cooldown_skips.append(i)\n",
        "\n",
        "                portfolio.append(balance if balance > 0 else position * price)\n",
        "\n",
        "                # log this step\n",
        "                pred_rows.append({\n",
        "                    \"Index\": i,\n",
        "                    \"Datetime\": df_window[\"Datetime\"].iloc[i] if \"Datetime\" in df_window.columns else None,\n",
        "                    \"Close\": price,\n",
        "                    \"Action\": a,\n",
        "                    \"Signal\": pred_signal,\n",
        "                    \"Confidence\": confidence,\n",
        "                    \"Balance\": balance,\n",
        "                    \"Position\": position,\n",
        "                    \"PortfolioValue\": portfolio[-1],\n",
        "                    \"Reward\": float(np.array(reward).squeeze())\n",
        "                })\n",
        "\n",
        "            if not portfolio:\n",
        "                logging.warning(f\"Empty portfolio for {ticker} | Window {start}-{end}\")\n",
        "                continue\n",
        "\n",
        "            # save step-by-step predictions AFTER the loop (prefix already defined)\n",
        "            pred_df = pd.DataFrame(pred_rows)\n",
        "            pred_path = os.path.join(RESULTS_DIR, f\"{prefix}_predictions.csv\")\n",
        "            pred_df.to_csv(pred_path, index=False)\n",
        "            logging.info(f\"Saved predictions to {pred_path}\")\n",
        "\n",
        "            # === Metrics (unchanged) ===\n",
        "            final_value = float(portfolio[-1])\n",
        "            hold_value  = float((100000.0 / df_window['Close'].iloc[0]) * df_window['Close'].iloc[-1])\n",
        "            returns = pd.Series(portfolio).pct_change().fillna(0)\n",
        "            sharpe  = float((returns.mean() / (returns.std() + 1e-6)) * np.sqrt(252))\n",
        "            drawdown = float(((pd.Series(portfolio).cummax() - pd.Series(portfolio)) /\n",
        "                              pd.Series(portfolio).cummax()).max() * 100)\n",
        "\n",
        "            prefix = f\"ppo_{ticker}_window{w_idx+1}\"\n",
        "\n",
        "            # Save VecNormalize snapshot\n",
        "            vecnorm_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "            try:\n",
        "                env.save(vecnorm_path)\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Could not save VecNormalize for {ticker} {start}-{end}: {e}\")\n",
        "                vecnorm_path = None\n",
        "\n",
        "            # Persist the model immediately (we may discard later if not top-N)\n",
        "            model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "            model.save(model_path)\n",
        "\n",
        "            result_row = {\n",
        "                \"Ticker\": ticker,\n",
        "                \"Window\": f\"{start}-{end}\",\n",
        "                \"PPO_Portfolio\": round(final_value, 2),\n",
        "                \"BuyHold\": round(hold_value, 2),\n",
        "                \"Sharpe\": round(sharpe, 3),\n",
        "                \"Drawdown_%\": round(drawdown, 2),\n",
        "                \"Winner\": \"PPO\" if final_value > hold_value else \"Buy & Hold\"\n",
        "            }\n",
        "            results.append(result_row)\n",
        "\n",
        "            meta = {\n",
        "                \"result\": result_row,\n",
        "                \"features\": df_window.columns.tolist(),\n",
        "                \"prefix\": prefix,\n",
        "                \"model_path\": model_path,\n",
        "                \"vecnorm_path\": vecnorm_path\n",
        "            }\n",
        "\n",
        "            # Maintain a top-N heap by Sharpe\n",
        "            item = (result_row[\"Sharpe\"], prefix, meta)\n",
        "            if len(top_heap) < TOP_N_WINDOWS:\n",
        "                heapq.heappush(top_heap, item)\n",
        "            else:\n",
        "                if item[0] > top_heap[0][0]:\n",
        "                    _, _, old_meta = heapq.heappop(top_heap)\n",
        "                    try:\n",
        "                        if os.path.exists(old_meta[\"model_path\"]): os.remove(old_meta[\"model_path\"])\n",
        "                        if old_meta[\"vecnorm_path\"] and os.path.exists(old_meta[\"vecnorm_path\"]):\n",
        "                            os.remove(old_meta[\"vecnorm_path\"])\n",
        "                    except Exception as e:\n",
        "                        logging.warning(f\"Cleanup of old top-N files failed: {e}\")\n",
        "                    heapq.heappush(top_heap, item)\n",
        "\n",
        "            # Diagnostics (once)\n",
        "            logging.info(f\"Confidence Buckets for {ticker} | Window {start}-{end}: {confidence_buckets}\")\n",
        "            successful_trades = sum(t == 'TAKE-PROFIT' for t in trade_log)\n",
        "            total_trades = sum(t in ['SELL', 'STOP-LOSS', 'TAKE-PROFIT'] for t in trade_log)\n",
        "            hit_ratio = successful_trades / (total_trades + 1e-6)\n",
        "            logging.info(f\"Hit Ratio: {hit_ratio:.2%}\")\n",
        "            if cooldown_skips:\n",
        "                logging.info(f\"Cooldown skips: {len(cooldown_skips)} | Sample: {cooldown_skips[:5]}\")\n",
        "            logging.info(f\"{ticker} | Window {w_idx+1} runtime: {round(time.time()-window_start_time, 2)}s\")\n",
        "\n",
        "        finally:\n",
        "            try: env.close()\n",
        "            except Exception: pass\n",
        "            del env\n",
        "            try: del model\n",
        "            except Exception: pass\n",
        "            gc.collect()\n",
        "            try: torch.cuda.empty_cache()\n",
        "            except Exception: pass\n",
        "\n",
        "    # Save top-N (artifact wrapper reads from paths)\n",
        "    top_list = sorted(top_heap, key=lambda t: t[0], reverse=True)\n",
        "    for _, _, meta in top_list:\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,  # already on disk\n",
        "            \"vecnorm_path\": meta[\"vecnorm_path\"],\n",
        "            \"features\": meta[\"features\"],\n",
        "            \"result\": meta[\"result\"],\n",
        "            \"prefix\": meta[\"prefix\"]\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, meta[\"prefix\"], FINAL_MODEL_DIR)\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Per-bucket PPO configs ---\n",
        "\n",
        "FAST = {\"lr\": 5e-5, \"n_steps\": 1024, \"batch\": 128, \"clip\": 0.25, \"ent\": 0.015}\n",
        "SLOW = {\"lr\": 1.5e-5, \"n_steps\": 2048, \"batch\": 64, \"clip\": 0.16, \"ent\": 0.0075}\n",
        "\n",
        "# Tickers (strings!)\n",
        "\n",
        "fast_names = {\n",
        "\"TSLA\",\"NVDA\",\"AMD\",\"AVGO\",\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"ADBE\",\"CRM\",\n",
        "\"INTC\",\"QCOM\",\"TXN\",\"ORCL\",\"NEE\",\"GE\",\"XOM\",\"CVX\",\"LLY\",\"NKE\",\"SBUX\"\n",
        "}\n",
        "slow_names = {\n",
        "\"BRK-B\",\"JPM\",\"BAC\",\"JNJ\",\"UNH\",\"MRK\",\"PFE\",\"ABBV\",\"ABT\",\"AMGN\",\"PG\",\"PEP\",\"KO\",\n",
        "\"V\",\"MA\",\"WMT\",\"MCD\",\"TMO\",\"DHR\",\"ACN\",\"IBM\",\"LIN\",\"PM\",\"RTX\",\"UPS\",\"UNP\",\"COST\",\"HD\",\"LOW\"\n",
        "}\n",
        "\n",
        "def pick_params(symbol: str):\n",
        "    return FAST if symbol in fast_names else SLOW\n",
        "\n",
        "def process_ticker(ticker):\n",
        "    hp = pick_params(ticker)\n",
        "    return walkforward_ppo(\n",
        "        df[df['Symbol'] == ticker].copy(),\n",
        "        ticker,\n",
        "        window_size=2500,\n",
        "        timesteps=150_000,\n",
        "        learning_rate=hp[\"lr\"],\n",
        "        ppo_overrides=hp\n",
        "    )\n",
        "\n",
        "def run_parallel_tickers(tickers, out_path=os.path.join(RESULTS_DIR, \"summary.csv\"), max_workers=6):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        for res in ex.map(process_ticker, tickers):\n",
        "            if res:\n",
        "                results.extend(res)\n",
        "                pd.DataFrame(results).to_csv(out_path, index=False)\n",
        "    logging.info(\"All tickers processed.\")\n",
        "    return results\n",
        "\n",
        "# === Execution Block ===\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    CONFIG = {'symbols': df['Symbol'].unique().tolist()}\n",
        "    all_results = []\n",
        "\n",
        "    if test_mode:\n",
        "        test_stocks = ['AAPL', 'NVDA', 'MSFT']\n",
        "        for stock in test_stocks:\n",
        "            logging.info(f\">>> Running test_mode on {stock}\")\n",
        "            results = process_ticker(stock)\n",
        "            all_results.extend(results)\n",
        "\n",
        "        test_summary_path = os.path.join(RESULTS_DIR, \"summary_test_mode.csv\")\n",
        "        pd.DataFrame(all_results).to_csv(test_summary_path, index=False)\n",
        "        logging.info(f\"Test summary saved to {test_summary_path}\")\n",
        "    else:\n",
        "        summary_results = run_parallel_tickers(CONFIG['symbols'])\n",
        "        if not summary_results:\n",
        "            logging.warning(\"No results generated.\")\n",
        "        else:\n",
        "            path = os.path.join(RESULTS_DIR, \"summary.csv\")\n",
        "            pd.DataFrame(summary_results).to_csv(path, index=False)\n",
        "            logging.info(f\"Summary saved to {path}\")\n",
        "\n",
        "    logging.info(\"Script finished execution.\")\n",
        "    gc.collect()\n",
        "    try:\n",
        "        torch.cuda.empty_cache()\n",
        "    except Exception:\n",
        "        pass\n",
        "    # --- Optional simple live inference loop ---\n",
        "    # Pick the best saved prefix for a symbol (e.g., best Sharpe window you kept)\n",
        "    # Example: best_prefix = \"ppo_AAPL_window3\"\n",
        "    best_prefix = None  # <-- set one if you want to run live\n",
        "    if LIVE_MODE and best_prefix is not None:\n",
        "        live_loop(\"AAPL\", best_prefix)\n"
      ],
      "metadata": {
        "id": "a9FCqXbnzHQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# === Enable Logging Format ===\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# === Paths ===\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/Results_May_2025/ppo_walkforward_results\"\n",
        "MODEL_DIR = os.path.join(RESULTS_DIR, \"models\")\n",
        "TICKER = \"AAPL\"\n",
        "WINDOW = \"window1\"\n",
        "\n",
        "# === Filenames ===\n",
        "model_path = os.path.join(MODEL_DIR, f\"ppo_{TICKER}_{WINDOW}_model.zip\")\n",
        "vecnorm_path = os.path.join(MODEL_DIR, f\"ppo_{TICKER}_{WINDOW}_vecnorm.pkl\")\n",
        "summary_path = os.path.join(RESULTS_DIR, \"summary_results.csv\")\n",
        "\n",
        "# === Load PPO Model ===\n",
        "if os.path.exists(model_path):\n",
        "    model = PPO.load(model_path)\n",
        "    logging.info(f\"Loaded PPO model from: {model_path}\")\n",
        "else:\n",
        "    logging.warning(f\"Model file not found at: {model_path}\")\n",
        "    model = None\n",
        "\n",
        "# === Load VecNormalize ===\n",
        "if os.path.exists(vecnorm_path):\n",
        "    vec_env = joblib.load(vecnorm_path)\n",
        "    logging.info(f\"Loaded VecNormalize from: {vecnorm_path}\")\n",
        "else:\n",
        "    logging.warning(f\"VecNormalize file not found at: {vecnorm_path}\")\n",
        "    vec_env = None\n",
        "\n",
        "# === PPO Model Architecture ===\n",
        "if model:\n",
        "    logging.info(\"PPO Model Architecture:\")\n",
        "    logging.info(str(model.policy))\n",
        "\n",
        "# === VecNormalize Diagnostics ===\n",
        "if vec_env:\n",
        "    try:\n",
        "        logging.info(\"VecNormalize Statistics:\")\n",
        "        logging.info(f\"Mean: {vec_env.obs_rms.mean}\")\n",
        "        logging.info(f\"Var: {vec_env.obs_rms.var}\")\n",
        "        logging.info(f\"Clip Range: {vec_env.clip_obs}\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not extract VecNormalize stats: {e}\")\n",
        "\n",
        "# === Summary Results (Top 3 by Sharpe) ===\n",
        "if os.path.exists(summary_path):\n",
        "    try:\n",
        "        summary_df = pd.read_csv(summary_path)\n",
        "        top3 = summary_df.sort_values(by=\"Sharpe\", ascending=False).head(3)\n",
        "        logging.info(\"Top 3 Results by Sharpe Ratio:\")\n",
        "        for i, row in top3.iterrows():\n",
        "            logging.info(f\"{row['Ticker']} | Window: {row['Window']} | Sharpe: {row['Sharpe']:.3f} | Portfolio: {row['PPO_Portfolio']:.2f} | Drawdown: {row['Drawdown_%']:.2f}%\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error reading summary file: {e}\")\n",
        "else:\n",
        "    logging.warning(\"No summary_results.csv found. Skipping top-3 summary.\")\n"
      ],
      "metadata": {
        "id": "WQzMmdUL4WHN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}