{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/exploratory_daytrading/blob/main/Model_Selector_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# MODEL SELECTOR â€“ COLAB E2E\n",
        "# =========================\n",
        "\n",
        "# === IMPORTS (Colab + Data) ===\n",
        "import os\n",
        "import sys\n",
        "import ast\n",
        "import json\n",
        "import argparse\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If running on Colab, import and mount Drive\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # === MOUNT GOOGLE DRIVE ===\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# === CONFIGURATION (adjust paths to your Drive folder) ===\n",
        "SELECTOR_DIR = \"/content/drive/MyDrive/Model_Selector/Result\" if IN_COLAB else \"./Result\"\n",
        "COMBINED_CSV = os.path.join(SELECTOR_DIR, \"all_models_selector_ready.csv\")\n",
        "OUTDIR = os.path.join(SELECTOR_DIR, \"out\")  # will be created if missing\n",
        "\n",
        "# =========================\n",
        "# STEP A: PREP INPUT (gather *_model_selector.csv and combine)\n",
        "# =========================\n",
        "os.makedirs(SELECTOR_DIR, exist_ok=True)\n",
        "model_files = [f for f in os.listdir(SELECTOR_DIR) if f.endswith('_model_selector.csv')]\n",
        "model_dfs = []\n",
        "\n",
        "for file in model_files:\n",
        "    file_path = os.path.join(SELECTOR_DIR, file)\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        # If 'Model' column not set, derive from filename:\n",
        "        if \"Model\" not in df.columns:\n",
        "            df[\"Model\"] = file.replace(\"_model_selector.csv\", \"\")\n",
        "        model_dfs.append(df)\n",
        "        print(f\" Loaded: {file}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading {file}: {e}\")\n",
        "\n",
        "if not model_dfs:\n",
        "    raise ValueError(\"No model selector CSVs found in folder. \"\n",
        "                     \"Place files ending with *_model_selector.csv into SELECTOR_DIR.\")\n",
        "\n",
        "# Combine and save the master CSV (used by the selector pipeline below)\n",
        "master_df = pd.concat(model_dfs, ignore_index=True)\n",
        "master_df.to_csv(COMBINED_CSV, index=False)\n",
        "print(f\"\\nðŸ“‚ Combined model selector results saved to:\\n{COMBINED_CSV}\")\n",
        "\n",
        "# Optional quick cleaning in this prep stage (safe and minimal)\n",
        "df_pre = master_df.copy()\n",
        "# Fix 'Signal Map' if exists (stringified dicts)\n",
        "if 'Signal Map' in df_pre.columns:\n",
        "    df_pre['Signal Map'] = df_pre['Signal Map'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"{\") else x\n",
        "    )\n",
        "# Drop legacy 'score' (lowercase) if present to avoid confusion with 'Score'\n",
        "if 'score' in df_pre.columns:\n",
        "    df_pre.drop(columns=['score'], inplace=True)\n",
        "print(f\"ðŸ“ Combined shape: {df_pre.shape}\")\n",
        "\n",
        "# =========================\n",
        "# STEP B: SELECTOR PIPELINE (full, with argparse + try/except)\n",
        "# =========================\n",
        "\n",
        "# -------------------------------- logging --------------------------------\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "log = logging.getLogger(\"selector\")\n",
        "try:\n",
        "    from scipy.stats import wilcoxon  # noqa: F401\n",
        "except Exception:\n",
        "    log.warning(\"SciPy not available; falling back to mean-edge rule for incumbent significance.\")\n",
        "\n",
        "import operator as _op\n",
        "\n",
        "# -------------------------------- utils ----------------------------------\n",
        "\n",
        "def parse_weights(s: str) -> Dict[str, float]:\n",
        "    out: Dict[str, float] = {}\n",
        "    if not s:\n",
        "        return out\n",
        "    for part in s.split(\",\"):\n",
        "        if \"=\" not in part:\n",
        "            continue\n",
        "        k, v = part.split(\"=\", 1)\n",
        "        try:\n",
        "            out[k.strip()] = float(v.strip())\n",
        "        except ValueError:\n",
        "            pass\n",
        "    return out\n",
        "\n",
        "def winsorize(series: pd.Series, p=0.01) -> pd.Series:\n",
        "    if p <= 0:\n",
        "        return series\n",
        "    lo, hi = series.quantile(p), series.quantile(1 - p)\n",
        "    return series.clip(lower=lo, upper=hi)\n",
        "\n",
        "def zscore(series: pd.Series) -> pd.Series:\n",
        "    s = pd.to_numeric(series, errors=\"coerce\").astype(float)\n",
        "    mu = s.mean()\n",
        "    sd = s.std(ddof=0)\n",
        "    if not np.isfinite(sd) or sd == 0:\n",
        "        return pd.Series(np.zeros(len(s)), index=s.index)\n",
        "    return (s - mu) / sd\n",
        "\n",
        "def robust_z(series: pd.Series) -> pd.Series:\n",
        "    x = pd.to_numeric(series, errors=\"coerce\")\n",
        "    med = x.median()\n",
        "    mad = (x - med).abs().median()\n",
        "    if not np.isfinite(mad) or mad == 0:\n",
        "        return pd.Series(np.zeros(len(x)), index=x.index)\n",
        "    return 0.6745 * (x - med) / mad\n",
        "\n",
        "def coerce_numeric(df: pd.DataFrame, cols) -> pd.DataFrame:\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def compute_score_frame(df: pd.DataFrame, weights: Dict[str, float], scale_mode=\"z\", winsor_p=0.0) -> pd.DataFrame:\n",
        "    metrics = [m for m in [\"Sharpe\", \"Return\", \"Accuracy\", \"Drawdown\", \"HitRatio\", \"WinRate\", \"Final_Portfolio\", \"Turnover\"]\n",
        "               if m in df.columns]\n",
        "    df = coerce_numeric(df, metrics)\n",
        "    if winsor_p > 0:\n",
        "        for m in metrics:\n",
        "            df[m] = winsorize(df[m], winsor_p)\n",
        "    scaler = robust_z if scale_mode.lower() == \"robust\" else (zscore if scale_mode.lower() == \"z\" else None)\n",
        "    scaled: Dict[str, pd.Series] = {}\n",
        "    for m in metrics:\n",
        "        v = df[m].fillna(0.0)\n",
        "        scaled[m] = scaler(v) if scaler else v\n",
        "    score = pd.Series(0.0, index=df.index)\n",
        "    for m, s in scaled.items():\n",
        "        w = float(weights.get(m, 0.0))\n",
        "        score = score + w * s\n",
        "    df[\"Score\"] = pd.to_numeric(score, errors=\"coerce\").fillna(0.0)\n",
        "    return df\n",
        "\n",
        "# ---------- universal helpers (family, annualization, resources, family-weights) ----------\n",
        "\n",
        "def infer_family_from_model(name: str) -> str:\n",
        "    n = (name or \"\").strip().lower()\n",
        "    rl = {\"ppo\",\"a2c\",\"ddpg\",\"td3\",\"sac\",\"dqn\",\"deep sarsa\",\"sarsa\",\"reinforce\"}\n",
        "    tree = {\"xgboost\",\"xgb\",\"lightgbm\",\"lgbm\",\"randomforest\",\"random_forest\",\"rf\",\"catboost\",\"cb\"}\n",
        "    cluster = {\"kmeans\",\"k-means\",\"k_means\"}\n",
        "    if any(k in n for k in rl): return \"RL\"\n",
        "    if any(k in n for k in tree): return \"Tree\"\n",
        "    if any(k in n for k in cluster): return \"Cluster\"\n",
        "    return \"Other\"\n",
        "\n",
        "def harmonize_accuracy_like(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if \"Accuracy\" not in df.columns:\n",
        "        if \"HitRatio\" in df.columns:\n",
        "            df[\"Accuracy\"] = pd.to_numeric(df[\"HitRatio\"], errors=\"coerce\")\n",
        "        elif \"WinRate\" in df.columns:\n",
        "            df[\"Accuracy\"] = pd.to_numeric(df[\"WinRate\"], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def annualize_metrics(df: pd.DataFrame, period_col: str, trading_days: int) -> pd.DataFrame:\n",
        "    if period_col in df.columns:\n",
        "        pdays = pd.to_numeric(df[period_col], errors=\"coerce\").replace(0, np.nan)\n",
        "        if \"Sharpe\" in df.columns:\n",
        "            df[\"Sharpe\"] = pd.to_numeric(df[\"Sharpe\"], errors=\"coerce\") * np.sqrt(trading_days / pdays)\n",
        "        if \"Return\" in df.columns:\n",
        "            r = pd.to_numeric(df[\"Return\"], errors=\"coerce\")\n",
        "            df[\"Return\"] = np.sign(r) * (np.power(1.0 + r.abs(), trading_days / pdays) - 1.0)\n",
        "    return df\n",
        "\n",
        "def apply_resource_gates(df: pd.DataFrame, require_artifacts: bool, max_latency_ms: int, max_mem_mb: int) -> pd.DataFrame:\n",
        "    m = pd.Series(True, index=df.index)\n",
        "    if require_artifacts:\n",
        "        have_any = False\n",
        "        for c in [\"artifact_path\",\"vecnorm_path\",\"features_path\"]:\n",
        "            if c in df.columns:\n",
        "                have_any = True\n",
        "                m &= df[c].notna() & (df[c].astype(str).str.len() > 0)\n",
        "        if not have_any:\n",
        "            pass  # don't drop everything if columns don't exist\n",
        "    if max_latency_ms > 0 and \"latency_ms\" in df.columns:\n",
        "        m &= pd.to_numeric(df[\"latency_ms\"], errors=\"coerce\") <= max_latency_ms\n",
        "    if max_mem_mb > 0 and \"mem_mb\" in df.columns:\n",
        "        m &= pd.to_numeric(df[\"mem_mb\"], errors=\"coerce\") <= max_mem_mb\n",
        "    out = df[m].fillna(False)\n",
        "    out = df[out]\n",
        "    return out if not out.empty else df\n",
        "\n",
        "def parse_family_weights(s: str) -> dict:\n",
        "    if not s: return {}\n",
        "    try:\n",
        "        j = json.loads(s)\n",
        "        out = {}\n",
        "        for fam, spec in j.items():\n",
        "            if isinstance(spec, str):\n",
        "                out[fam] = parse_weights(spec)\n",
        "            elif isinstance(spec, dict):\n",
        "                out[fam] = {k: float(v) for k, v in spec.items()}\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        log.warning(\"Failed to parse --family_weights JSON: %s\", e)\n",
        "        return {}\n",
        "\n",
        "def compute_score_frame_with_family_weights(df: pd.DataFrame,\n",
        "                                            default_weights: dict,\n",
        "                                            family_weights: dict,\n",
        "                                            scale_mode=\"z\",\n",
        "                                            winsor_p=0.0) -> pd.DataFrame:\n",
        "    metrics = [m for m in [\"Sharpe\",\"Return\",\"Accuracy\",\"Drawdown\",\"HitRatio\",\"WinRate\",\"Final_Portfolio\",\"Turnover\"]\n",
        "               if m in df.columns]\n",
        "    df = coerce_numeric(df, metrics)\n",
        "    if winsor_p > 0:\n",
        "        for m in metrics:\n",
        "            df[m] = winsorize(df[m], winsor_p)\n",
        "    scaler = robust_z if scale_mode.lower() == \"robust\" else (zscore if scale_mode.lower() == \"z\" else None)\n",
        "    scaled = {}\n",
        "    for m in metrics:\n",
        "        v = df[m].fillna(0.0)\n",
        "        scaled[m] = scaler(v) if scaler else v\n",
        "    S = pd.DataFrame({m: scaled[m] for m in metrics}, index=df.index)\n",
        "    fam = df[\"Family\"] if \"Family\" in df.columns else df[\"Model\"].astype(str).apply(infer_family_from_model)\n",
        "    scores = pd.Series(0.0, index=df.index)\n",
        "    for i in df.index:\n",
        "        f = fam.loc[i]\n",
        "        w = family_weights.get(f, default_weights)\n",
        "        val = 0.0\n",
        "        for m in metrics:\n",
        "            if m in S.columns:\n",
        "                val += float(w.get(m, 0.0)) * float(S.loc[i, m])\n",
        "        scores.loc[i] = val\n",
        "    df[\"Score\"] = pd.to_numeric(scores, errors=\"coerce\").fillna(0.0)\n",
        "    return df\n",
        "\n",
        "# -------------------------------- gates / schema / hygiene --------------------------------\n",
        "_OPS = {\"<=\":_op.le, \">=\":_op.ge, \"<\":_op.lt, \">\":_op.gt, \"==\":_op.eq, \"!=\":_op.ne}\n",
        "\n",
        "def apply_gates_expr(df: pd.DataFrame, spec: str) -> pd.DataFrame:\n",
        "    if not spec: return df\n",
        "    mask = pd.Series(True, index=df.index)\n",
        "    for tok in spec.split(\",\"):\n",
        "        tok = tok.strip()\n",
        "        if not tok: continue\n",
        "        for sym in [\"<=\",\">=\",\"!=\",\"==\",\"<\",\">\"]:\n",
        "            if sym in tok:\n",
        "                col, val = tok.split(sym, 1)\n",
        "                col, val = col.strip(), val.strip()\n",
        "                if col in df.columns:\n",
        "                    v_num = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "                    try:\n",
        "                        fval = float(val)\n",
        "                        mask &= _OPS[sym](v_num, fval)\n",
        "                    except ValueError:\n",
        "                        if sym in (\"==\", \"!=\"):\n",
        "                            val_clean = val.strip().strip('\"\\'')\n",
        "                            v_str = df[col].astype(str)\n",
        "                            left  = v_str.str.lower()\n",
        "                            right = val_clean.lower()\n",
        "                            mask &= (left == right) if sym == \"==\" else (left != right)\n",
        "                break\n",
        "    out = df[mask.fillna(False)]\n",
        "    return out if not out.empty else df\n",
        "\n",
        "def validate_schema(df: pd.DataFrame):\n",
        "    REQUIRED_ANY = [[\"Ticker\",\"Symbol\"], [\"Model\"]]\n",
        "    OPTIONAL = {\"Sharpe\",\"Return\",\"Accuracy\",\"Drawdown\",\"Max_Drawdown\",\"Sortino\",\n",
        "                \"HitRatio\",\"WinRate\",\"Final_Portfolio\",\"Turnover\",\"Trade_Count\",\n",
        "                \"EndDate\",\"artifact_path\",\"vecnorm_path\",\"features_path\",\n",
        "                \"latency_ms\",\"mem_mb\",\"regime\",\"rl_profile\",\"PeriodDays\"}\n",
        "    for group in REQUIRED_ANY:\n",
        "        if not any(col in df.columns for col in group):\n",
        "            raise SystemExit(f\"Missing one of required columns: {group}\")\n",
        "    unknown = [c for c in df.columns if c not in OPTIONAL and c not in {\"Ticker\",\"Symbol\",\"Model\",\"Family\"}]\n",
        "    if unknown:\n",
        "        log.info(\"Extra columns (ok): %s\", \", \".join(sorted(unknown)))\n",
        "\n",
        "def normalize_drawdown_sign(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for c in [\"Drawdown\",\"Max_Drawdown\",\"MaxDD_Mean\"]:\n",
        "        if c in df.columns:\n",
        "            x = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "            if (x.dropna() > 0).mean() > 0.8:\n",
        "                df[c] = -x\n",
        "    return df\n",
        "\n",
        "def dedupe(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if {\"Ticker\",\"Model\"}.issubset(df.columns):\n",
        "        before = len(df)\n",
        "        df = (df.sort_values(\"Score\", ascending=False)\n",
        "                .drop_duplicates([\"Ticker\",\"Model\"], keep=\"first\"))\n",
        "        if len(df) < before:\n",
        "            log.info(\"Deduped (Ticker,Model): %d -> %d\", before, len(df))\n",
        "    return df\n",
        "\n",
        "# -------------------------------- safety gates --------------------------------\n",
        "\n",
        "def apply_safety_gates(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    maxdd_col = \"Max_Drawdown\" if \"Max_Drawdown\" in df.columns else (\"Drawdown\" if \"Drawdown\" in df.columns else None)\n",
        "    m = pd.Series(True, index=df.index)\n",
        "    if \"Sharpe\" in df.columns:\n",
        "        m &= pd.to_numeric(df[\"Sharpe\"], errors=\"coerce\") > 0.0\n",
        "    if maxdd_col:\n",
        "        m &= pd.to_numeric(df[maxdd_col], errors=\"coerce\") > -0.50\n",
        "    if \"Return\" in df.columns:\n",
        "        m &= pd.to_numeric(df[\"Return\"], errors=\"coerce\") > -0.20\n",
        "    out = df[m].fillna(False)\n",
        "    out = df[out]\n",
        "    if len(out) == 0:\n",
        "        log.warning(\"Safety gates removed all rows; keeping original frame to avoid empty selection.\")\n",
        "        return df\n",
        "    return out\n",
        "\n",
        "# -------------------------------- significance / FDR --------------------------------\n",
        "\n",
        "def should_switch(diff_series: pd.Series) -> bool:\n",
        "    \"\"\"Fallback rule when no p-value: require >5% mean edge.\"\"\"\n",
        "    diff = pd.to_numeric(diff_series, errors=\"coerce\").dropna()\n",
        "    if len(diff) == 0:\n",
        "        return True\n",
        "    return diff.mean() > 0.05\n",
        "\n",
        "def diff_series_for_ticker(ticker: str, inc_model: str, chal_model: str, df_win: pd.DataFrame) -> pd.Series:\n",
        "    sub = df_win[df_win[\"Ticker\"] == ticker] if \"Ticker\" in df_win.columns else df_win[df_win[\"Symbol\"] == ticker]\n",
        "    if sub.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    base_col = (\"Score\" if \"Score\" in sub.columns else\n",
        "                (\"Sharpe\" if \"Sharpe\" in sub.columns else\n",
        "                 (\"Return\" if \"Return\" in sub.columns else None)))\n",
        "    if base_col is None:\n",
        "        return pd.Series(dtype=float)\n",
        "    idx = pd.to_datetime(sub[\"EndDate\"], errors=\"coerce\") if \"EndDate\" in sub.columns else sub.index\n",
        "    piv = (sub[sub[\"Model\"].isin([inc_model, chal_model])]\n",
        "           .assign(_idx=idx)\n",
        "           .pivot_table(index=\"_idx\", columns=\"Model\", values=base_col, aggfunc=\"first\")\n",
        "           .sort_index())\n",
        "    if inc_model not in piv.columns or chal_model not in piv.columns:\n",
        "        return pd.Series(dtype=float)\n",
        "    return (piv[chal_model] - piv[inc_model]).dropna()\n",
        "\n",
        "def softmax(x: np.ndarray) -> np.ndarray:\n",
        "    x = np.array(x, dtype=float)\n",
        "    if not np.isfinite(x).any():\n",
        "        return np.ones_like(x) / len(x)\n",
        "    z = x - np.nanmax(x)\n",
        "    e = np.exp(z)\n",
        "    s = np.nansum(e)\n",
        "    return e / s if s > 0 else np.ones_like(x) / len(x)\n",
        "\n",
        "def fdr_bh(pvals: pd.Series, alpha=0.10) -> float:\n",
        "    \"\"\"Return BH cutoff; p <= cutoff are accepted. If none, returns -1.\"\"\"\n",
        "    p = pvals.dropna().sort_values()\n",
        "    m = len(p)\n",
        "    if m == 0:\n",
        "        return -1\n",
        "    thresh = (np.arange(1, m+1) / m) * alpha\n",
        "    passed = p.values <= thresh\n",
        "    cutoff = p.values[passed].max() if passed.any() else -1\n",
        "    return cutoff\n",
        "\n",
        "# -------------------------------- selection / export --------------------------------\n",
        "\n",
        "def build_selection(df_sorted: pd.DataFrame,\n",
        "                    df_win: Optional[pd.DataFrame],\n",
        "                    incumbents: Dict[str, Any],\n",
        "                    eps: float,\n",
        "                    eps_mode: str = \"relative\",\n",
        "                    ensemble_multi: bool = False,\n",
        "                    fdr_alpha: float = 0.0,\n",
        "                    wilcoxon_alpha: float = 0.10,\n",
        "                    ensemble_max: int = 5) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "\n",
        "    \"\"\"\n",
        "    Build per-ticker selection with ensemble ties and optional incumbent significance + FDR control.\n",
        "    Returns (chosen_rows_df, selection_json_map).\n",
        "    \"\"\"\n",
        "    # Pass 1: compute proposed switches and p-values\n",
        "    pvals = {}\n",
        "    rows_by_ticker = {}\n",
        "\n",
        "    for ticker, tdf in df_sorted.groupby(\"Ticker\"):\n",
        "        tdf = tdf.sort_values(\"Score\", ascending=False).reset_index(drop=True)\n",
        "        rows_by_ticker[str(ticker)] = tdf\n",
        "        top = tdf.iloc[0]\n",
        "        inc = incumbents.get(str(ticker), {}) if isinstance(incumbents, dict) else {}\n",
        "        inc_model = inc.get(\"model\") or inc.get(\"Model\")\n",
        "\n",
        "        if df_win is not None and inc_model and str(inc_model) != str(top[\"Model\"]):\n",
        "            ds = diff_series_for_ticker(str(ticker), str(inc_model), str(top[\"Model\"]), df_win)\n",
        "            p = None\n",
        "            if len(ds) >= 5:\n",
        "                try:\n",
        "                    from scipy.stats import wilcoxon\n",
        "                    _, p = wilcoxon(ds)\n",
        "                except Exception:\n",
        "                    p = None\n",
        "            if p is not None and ds.mean() > 0:\n",
        "                pvals[str(ticker)] = float(p)\n",
        "            else:\n",
        "                pvals[str(ticker)] = np.nan\n",
        "\n",
        "    cutoff = None\n",
        "    if fdr_alpha and len([v for v in pvals.values() if pd.notna(v)]) > 0:\n",
        "        cutoff = fdr_bh(pd.Series(pvals), alpha=float(fdr_alpha))\n",
        "        log.info(\"FDR BH cutoff (alpha=%.3f): %s\", fdr_alpha, cutoff)\n",
        "\n",
        "    # Pass 2: finalize selections\n",
        "    chosen_rows = []\n",
        "    out_map: Dict[str, Any] = {}\n",
        "\n",
        "    for ticker, tdf in rows_by_ticker.items():\n",
        "        tdf = tdf.sort_values(\"Score\", ascending=False).reset_index(drop=True)\n",
        "        top = tdf.iloc[0]\n",
        "        second = tdf.iloc[1] if len(tdf) > 1 else None\n",
        "\n",
        "        top_score = float(top[\"Score\"])\n",
        "        thr = float(eps) if eps_mode == \"absolute\" else max(0.0, abs(top_score) * float(eps))\n",
        "\n",
        "        # Ensemble logic\n",
        "        selection: Dict[str, Any] = {\"mode\": \"single\"}\n",
        "        if ensemble_multi:\n",
        "            within = tdf[(top_score - tdf[\"Score\"]) <= thr].copy()\n",
        "            within = within[np.isfinite(within[\"Score\"])]\n",
        "            if len(within) >= 2:\n",
        "                within = within.sort_values(\"Score\", ascending=False).head(ensemble_max)\n",
        "                models = within[\"Model\"].astype(str).tolist()\n",
        "                w = softmax(within[\"Score\"].values)\n",
        "                selection = {\"mode\": \"ensemble\", \"members\": models, \"weights\": [float(x) for x in w.tolist()]}\n",
        "        else:\n",
        "            if second is not None and (top_score - float(second[\"Score\"])) <= thr:\n",
        "                selection = {\"mode\": \"ensemble\", \"secondary\": str(second[\"Model\"])}\n",
        "\n",
        "        chosen_row = top\n",
        "        chosen_model = str(top[\"Model\"])\n",
        "\n",
        "        # Significance guard vs incumbent (with optional FDR)\n",
        "        inc = incumbents.get(ticker, {})\n",
        "        inc_model = inc.get(\"model\") or inc.get(\"Model\")\n",
        "        if inc_model and inc_model != chosen_model and df_win is not None:\n",
        "            ds = diff_series_for_ticker(ticker, str(inc_model), chosen_model, df_win)\n",
        "            allow_switch = False\n",
        "            used_p = None\n",
        "            if len(ds) >= 5:\n",
        "                try:\n",
        "                    from scipy.stats import wilcoxon\n",
        "                    _, p = wilcoxon(ds)\n",
        "                    used_p = float(p)\n",
        "                    if ds.mean() > 0 and p <= wilcoxon_alpha:\n",
        "                        allow_switch = True\n",
        "                        if cutoff is not None and not np.isnan(cutoff):\n",
        "                            allow_switch = allow_switch and (p <= cutoff)\n",
        "                except Exception:\n",
        "                    allow_switch = should_switch(ds)\n",
        "            else:\n",
        "                allow_switch = should_switch(ds)\n",
        "\n",
        "            if not allow_switch:\n",
        "                if (tdf[\"Model\"] == inc_model).any():\n",
        "                    chosen_row = tdf[tdf[\"Model\"] == inc_model].iloc[0]\n",
        "                    chosen_model = str(inc_model)\n",
        "                    note = \"kept_incumbent\"\n",
        "                    if used_p is not None:\n",
        "                        note += f\"_p={used_p:.3f}\"\n",
        "                        if cutoff is not None and not np.isnan(cutoff):\n",
        "                            note += f\"_bh_cutoff={cutoff:.3f}\"\n",
        "                    selection = {\"mode\": \"single\", \"note\": note}\n",
        "\n",
        "        # Stability & artifacts (if available)\n",
        "        maxdd = (chosen_row[\"Max_Drawdown\"] if \"Max_Drawdown\" in chosen_row and pd.notna(chosen_row[\"Max_Drawdown\"])\n",
        "                 else (chosen_row[\"Drawdown\"] if \"Drawdown\" in chosen_row else None))\n",
        "\n",
        "        stability = {}\n",
        "        if \"Recent_Wins_3mo\" in tdf.columns:\n",
        "            val = pd.to_numeric(chosen_row.get(\"Recent_Wins_3mo\", np.nan), errors=\"coerce\")\n",
        "            stability[\"win_rate_k_of_n\"] = float(val) if pd.notna(val) else 0.0\n",
        "        if \"Sharpe_Var\" in tdf.columns:\n",
        "            val = pd.to_numeric(chosen_row.get(\"Sharpe_Var\", np.nan), errors=\"coerce\")\n",
        "            stability[\"sharpe_var\"] = float(val) if pd.notna(val) else None\n",
        "\n",
        "        artifact = {}\n",
        "        exists = {}\n",
        "        for k, col in [(\"path\",\"artifact_path\"), (\"vecnorm\",\"vecnorm_path\"), (\"features\",\"features_path\")]:\n",
        "            if col in tdf.columns:\n",
        "                artifact[k] = chosen_row.get(col)\n",
        "                if isinstance(artifact[k], str):\n",
        "                    exists[k] = Path(artifact[k]).exists()\n",
        "        if artifact:\n",
        "            if \"load_ms\" in tdf.columns:\n",
        "                artifact[\"load_ms\"] = int(pd.to_numeric(chosen_row.get(\"load_ms\", 0), errors=\"coerce\") or 0)\n",
        "            if \"mem_mb\" in tdf.columns:\n",
        "                artifact[\"mem_mb\"] = int(pd.to_numeric(chosen_row.get(\"mem_mb\", 0), errors=\"coerce\") or 0)\n",
        "            if exists:\n",
        "                artifact[\"exists\"] = exists\n",
        "\n",
        "        out_map[str(ticker)] = {\n",
        "            \"model\": chosen_model,\n",
        "            \"family\": infer_family_from_model(str(chosen_model)),\n",
        "            \"regime\": (str(chosen_row.get(\"regime\")) if \"regime\" in chosen_row else None),\n",
        "            \"rl_profile\": (str(chosen_row.get(\"rl_profile\")) if \"rl_profile\" in chosen_row else None),\n",
        "            \"score\": float(chosen_row.get(\"Score\", 0.0)),\n",
        "            \"sharpe\": float(pd.to_numeric(chosen_row.get(\"Sharpe\", np.nan), errors=\"coerce\")) if \"Sharpe\" in chosen_row else None,\n",
        "            \"return\": float(pd.to_numeric(chosen_row.get(\"Return\", np.nan), errors=\"coerce\")) if \"Return\" in chosen_row else None,\n",
        "            \"max_drawdown\": float(pd.to_numeric(maxdd, errors=\"coerce\")) if maxdd is not None else None,\n",
        "            \"sortino\": float(pd.to_numeric(chosen_row.get(\"Sortino_Mean\", np.nan), errors=\"coerce\")) if \"Sortino_Mean\" in chosen_row else None,\n",
        "            \"trade_count\": int(pd.to_numeric(chosen_row.get(\"Trade_Count\", np.nan), errors=\"coerce\")) if \"Trade_Count\" in chosen_row else None,\n",
        "            \"turnover\": float(pd.to_numeric(chosen_row.get(\"Turnover\", np.nan), errors=\"coerce\")) if \"Turnover\" in chosen_row else None,\n",
        "            \"stability\": stability if stability else None,\n",
        "            \"artifact\": artifact if artifact else None,\n",
        "            \"selection\": selection,\n",
        "        }\n",
        "        chosen_rows.append(chosen_row)\n",
        "\n",
        "    chosen_df = pd.DataFrame(chosen_rows).reset_index(drop=True)\n",
        "    return chosen_df, out_map\n",
        "\n",
        "def summarize_by_model(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    summary_cols = [c for c in [\"Score\", \"Sharpe\", \"Return\", \"Accuracy\", \"Drawdown\", \"Final_Portfolio\"] if c in df.columns]\n",
        "    agg = df.groupby(\"Model\", as_index=False)[summary_cols].mean(numeric_only=True)\n",
        "    return agg.sort_values(\"Score\", ascending=False) if \"Score\" in agg.columns else agg\n",
        "\n",
        "def pick_best_per_ticker(df: pd.DataFrame, k: int = 1) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    if \"Ticker\" not in df.columns and \"Symbol\" in df.columns:\n",
        "        df = df.rename(columns={\"Symbol\": \"Ticker\"})\n",
        "    if \"Ticker\" in df.columns:\n",
        "        df[\"Ticker\"] = df[\"Ticker\"].astype(str).str.upper().str.strip()\n",
        "        df[\"Model\"] = df[\"Model\"].astype(str).str.strip()\n",
        "    if \"Ticker\" not in df.columns:\n",
        "        raise ValueError(\"Expected 'Ticker' or 'Symbol' in input CSV.\")\n",
        "\n",
        "    sort_cols, sort_asc = [], []\n",
        "    def add(col, asc=False):\n",
        "        if col in df.columns:\n",
        "            sort_cols.append(col); sort_asc.append(asc)\n",
        "\n",
        "    add(\"Score\", asc=False)\n",
        "    add(\"Sharpe\", asc=False)\n",
        "    add(\"Return\", asc=False)\n",
        "    add(\"Sortino_Mean\", asc=False)\n",
        "    add(\"Drawdown\", asc=True)\n",
        "    add(\"MaxDD_Mean\", asc=True)\n",
        "    add(\"N_Windows\", asc=False)\n",
        "    add(\"Turnover\", asc=True)\n",
        "    add(\"Accuracy\", asc=False)\n",
        "    add(\"Final_Portfolio\", asc=False)\n",
        "\n",
        "    if not sort_cols:\n",
        "        raise ValueError(\"No comparable columns to sort by. Provide metrics or weights.\")\n",
        "\n",
        "    df_sorted = df.sort_values([\"Ticker\"] + sort_cols, ascending=[True] + sort_asc)\n",
        "\n",
        "    if k == 1:\n",
        "        best = df_sorted.groupby(\"Ticker\", as_index=False).first()\n",
        "    else:\n",
        "        best = df_sorted.groupby(\"Ticker\").head(k).reset_index(drop=True)\n",
        "    return best, df_sorted\n",
        "\n",
        "def write_outputs(best: pd.DataFrame, outdir: Path, tag: str, sel_map: Dict[str, Any]):\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "    csv_path = outdir / f\"model_selector_summary_{tag}.csv\"\n",
        "    xlsx_path = outdir / f\"model_selector_summary_{tag}.xlsx\"\n",
        "    json_path = outdir / f\"selected_models_{tag}.json\"\n",
        "\n",
        "    best.to_csv(csv_path, index=False)\n",
        "    try:\n",
        "        best.to_excel(xlsx_path, index=False)\n",
        "    except Exception as e:\n",
        "        log.warning(\"Excel write failed (%s); continuing with CSV+JSON.\", e)\n",
        "\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(sel_map, f, indent=2)\n",
        "\n",
        "    return str(csv_path), str(xlsx_path), str(json_path)\n",
        "\n",
        "def write_meta(outdir: Path, args):\n",
        "    meta = {\n",
        "        \"timestamp\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
        "        \"version\": \"4.1.x\",\n",
        "        \"args\": vars(args)\n",
        "    }\n",
        "    try:\n",
        "        (outdir / \"run_meta.json\").write_text(json.dumps(meta, indent=2))\n",
        "    except Exception as e:\n",
        "        log.warning(\"Failed to write run_meta.json (%s)\", e)\n",
        "\n",
        "def apply_family_cap(df_sorted: pd.DataFrame, cap: int) -> pd.DataFrame:\n",
        "    if cap <= 0 or \"Family\" not in df_sorted.columns:\n",
        "        return df_sorted\n",
        "    keep = []\n",
        "    for _, g in df_sorted.groupby(\"Ticker\", sort=False):\n",
        "        counts: Dict[str,int] = {}\n",
        "        for _, r in g.iterrows():\n",
        "            f = r[\"Family\"]\n",
        "            counts[f] = counts.get(f, 0) + 1\n",
        "            if counts[f] <= cap:\n",
        "                keep.append(r)\n",
        "    return pd.DataFrame(keep)\n",
        "\n",
        "# -------------------------------- pipeline --------------------------------\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--input\", required=True, help=\"Path to combined CSV (all_models_selector_ready.csv)\")\n",
        "    ap.add_argument(\"--outdir\", required=True, help=\"Output directory for summary files\")\n",
        "\n",
        "    # Scoring & scaling\n",
        "    ap.add_argument(\"--weights\", default=\"Sharpe=0.5,Return=0.3,Accuracy=0.2\",\n",
        "                    help=\"Comma key=val list, e.g. 'Sharpe=0.5,Return=0.3,Accuracy=0.2,Drawdown=-0.1'\")\n",
        "    ap.add_argument(\"--scale\", choices=[\"z\", \"robust\", \"none\"], default=\"z\", help=\"Metric scaling before weighting\")\n",
        "    ap.add_argument(\"--winsor\", type=float, default=0.0, help=\"Winsorization p in [0,0.1]; try 0.01\")\n",
        "\n",
        "    # Data handling\n",
        "    ap.add_argument(\"--low_memory\", action=\"store_true\", help=\"Pass low_memory to pandas read_csv\")\n",
        "    ap.add_argument(\"--dtypes_path\", default=\"\", help=\"Optional JSON file with column->dtype for read_csv\")\n",
        "    ap.add_argument(\"--drop_nan\", action=\"store_true\", help=\"Drop rows with NaNs in key metrics\")\n",
        "\n",
        "    # History / recency / stability\n",
        "    ap.add_argument(\"--windows\", default=None, help=\"(Optional) per-window history CSV for stability/recency/significance\")\n",
        "    ap.add_argument(\"--lambda_decay\", type=float, default=0.02, help=\"Recency exponential decay per day\")\n",
        "    ap.add_argument(\"--halflife_days\", type=float, default=0.0, help=\"If >0, overrides lambda via ln(2)/half_life\")\n",
        "    ap.add_argument(\"--sharpe_se_alpha\", type=float, default=0.5, help=\"Penalty factor for Sharpe standard error\")\n",
        "\n",
        "    # Costs & gates\n",
        "    ap.add_argument(\"--cost_bps\", type=float, default=0.5, help=\"Turnover cost in basis points\")\n",
        "    ap.add_argument(\"--min_windows\", type=int, default=3, help=\"Minimum number of windows required\")\n",
        "    ap.add_argument(\"--min_trades\", type=float, default=1.0, help=\"Minimum average trades required\")\n",
        "    ap.add_argument(\"--gates\", default=\"\", help='Custom gates, e.g. \"Sharpe>0,Max_Drawdown>-0.5,Return>-0.2\"')\n",
        "    ap.add_argument(\"--flip_dd_positive\", action=\"store_true\",\n",
        "                    help=\"If drawdowns are mostly positive values, flip sign to negative.\")\n",
        "\n",
        "    # Ensemble / Incumbents / FDR\n",
        "    ap.add_argument(\"--eps\", type=float, default=0.03, help=\"Tie epsilon\")\n",
        "    ap.add_argument(\"--eps_mode\", choices=[\"relative\",\"absolute\"], default=\"relative\",\n",
        "                    help=\"Use relative (fraction of top score) or absolute EPS for ensembling\")\n",
        "    ap.add_argument(\"--ensemble_multi\", action=\"store_true\",\n",
        "                    help=\"If set, include all models within EPS threshold (up to 5)\")\n",
        "    ap.add_argument(\"--ensemble_max\", type=int, default=5, help=\"Max models in multi-ensemble\")\n",
        "    ap.add_argument(\"--incumbents\", default=\"\", help=\"(Optional) Path to prior selected_models_*.json for significance guard\")\n",
        "    ap.add_argument(\"--fdr_alpha\", type=float, default=0.0, help=\"Benjaminiâ€“Hochberg FDR alpha (0=disable)\")\n",
        "\n",
        "    # Universal / family\n",
        "    ap.add_argument(\"--topk\", type=int, default=1, help=\"Top-K rows per ticker for CSV/XLSX table\")\n",
        "    ap.add_argument(\"--annualize\", action=\"store_true\", help=\"Annualize Sharpe/Return using PeriodDays if present\")\n",
        "    ap.add_argument(\"--period_col\", default=\"PeriodDays\", help=\"Column with period length (days)\")\n",
        "    ap.add_argument(\"--trading_days\", type=int, default=252, help=\"Trading days per year for annualization\")\n",
        "    ap.add_argument(\"--max_latency_ms\", type=int, default=0, help=\"Drop models with latency_ms above this (0=ignore)\")\n",
        "    ap.add_argument(\"--max_mem_mb\", type=int, default=0, help=\"Drop models with mem_mb above this (0=ignore)\")\n",
        "    ap.add_argument(\"--require_artifacts\", action=\"store_true\", help=\"Keep only rows with non-null artifact paths\")\n",
        "    ap.add_argument(\"--family_weights\", default=\"\",\n",
        "                    help='JSON map: family -> weight spec. e.g. '\n",
        "                         '\\'{\"RL\":\"Sharpe=0.6,Return=0.3,Drawdown=-0.1\",\"Tree\":\"Sharpe=0.5,Return=0.4,Turnover=-0.1\"}\\'')\n",
        "    ap.add_argument(\"--family_cap\", type=int, default=0, help=\"Cap models per family per ticker before selection (0=off)\")\n",
        "\n",
        "    # Exports / ops\n",
        "    ap.add_argument(\"--export_sorted\", default=\"\", help=\"Write full sorted table CSV here\")\n",
        "    ap.add_argument(\"--export_parquet\", default=\"\", help=\"Write full sorted table Parquet here\")\n",
        "    ap.add_argument(\"--debug_csv\", default=\"\", help=\"Write intermediate scored DF for audit (path)\")\n",
        "    ap.add_argument(\"--dry_run\", action=\"store_true\", help=\"Compute and validateâ€”do not write outputs\")\n",
        "\n",
        "    # --- your requested try/except around parse_args() ---\n",
        "    try:\n",
        "        args = ap.parse_args()\n",
        "    except SystemExit as e:\n",
        "        raise RuntimeError(\"Missing required args: --input and --outdir\") from e\n",
        "\n",
        "    # Half-life â†’ lambda (if provided)\n",
        "    if args.halflife_days and args.halflife_days > 0:\n",
        "        args.lambda_decay = np.log(2.0) / float(args.halflife_days)\n",
        "        log.info(\"Using lambda_decay from half-life: %.6f (half-life=%.1f days)\", args.lambda_decay, args.halflife_days)\n",
        "\n",
        "    weights = parse_weights(args.weights)\n",
        "    fam_w = parse_family_weights(args.family_weights)\n",
        "    tag = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    log.info(\"Scaling mode: %s\", args.scale)\n",
        "\n",
        "    # Read CSV with optional dtype hints\n",
        "    read_kwargs: Dict[str, Any] = {\"low_memory\": args.low_memory}\n",
        "    if args.dtypes_path and Path(args.dtypes_path).exists():\n",
        "        try:\n",
        "            dtypes = json.loads(Path(args.dtypes_path).read_text())\n",
        "            read_kwargs[\"dtype\"] = dtypes\n",
        "        except Exception as e:\n",
        "            log.warning(\"Failed to load dtypes JSON (%s); continuing without.\", e)\n",
        "\n",
        "    log.info(\"Reading input: %s\", args.input)\n",
        "    df = pd.read_csv(args.input, **read_kwargs)\n",
        "    validate_schema(df)\n",
        "    log.info(\"Shape: %s, columns (first 12): %s\", df.shape, \", \".join(df.columns[:12]))\n",
        "\n",
        "    # Normalize ticker\n",
        "    if \"Ticker\" not in df.columns and \"Symbol\" in df.columns:\n",
        "        df = df.rename(columns={\"Symbol\": \"Ticker\"})\n",
        "    df[\"Ticker\"] = df[\"Ticker\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "    # Family + harmonization + annualization + resource gates\n",
        "    df[\"Family\"] = df[\"Model\"].astype(str).apply(infer_family_from_model)\n",
        "    df = harmonize_accuracy_like(df)\n",
        "    if args.annualize:\n",
        "        df = annualize_metrics(df, args.period_col, args.trading_days)\n",
        "    df = apply_resource_gates(df, args.require_artifacts, args.max_latency_ms, args.max_mem_mb)\n",
        "\n",
        "    # Optional normalize drawdown sign\n",
        "    # (use --flip_dd_positive if drawdowns are mostly positive and should be negative)\n",
        "    if args.flip_dd_positive:\n",
        "        df = normalize_drawdown_sign(df)\n",
        "\n",
        "    # ===== v4.1 history-based stability + recency scoring (optional) =====\n",
        "    df_win = None\n",
        "    if args.windows:\n",
        "        log.info(\"Reading windows history: %s\", args.windows)\n",
        "        log.info(\"Windows composites using %s z-scores\", \"robust\" if args.scale == \"robust\" else \"standard\")\n",
        "        df_win = pd.read_csv(args.windows, **({\"low_memory\": args.low_memory} if args.low_memory else {}))\n",
        "\n",
        "        if \"Ticker\" not in df_win.columns and \"Symbol\" in df_win.columns:\n",
        "            df_win = df_win.rename(columns={\"Symbol\": \"Ticker\"})\n",
        "        if \"Max_Drawdown\" not in df_win.columns and \"Drawdown\" in df_win.columns:\n",
        "            df_win[\"Max_Drawdown\"] = df_win[\"Drawdown\"]\n",
        "\n",
        "        for col in [\"Sharpe\", \"Return\", \"Max_Drawdown\", \"Sortino\", \"Trade_Count\", \"Turnover\", \"Is_Win_Recent\"]:\n",
        "            if col in df_win.columns:\n",
        "                df_win[col] = pd.to_numeric(df_win[col], errors=\"coerce\")\n",
        "\n",
        "        # Aggregates across windows\n",
        "        agg = (df_win\n",
        "               .groupby([\"Ticker\", \"Model\"])\n",
        "               .agg(Sharpe_Mean=(\"Sharpe\", \"mean\"),\n",
        "                    Sharpe_Var=(\"Sharpe\", \"var\"),\n",
        "                    Return_Mean=(\"Return\", \"mean\"),\n",
        "                    MaxDD_Mean=(\"Max_Drawdown\", \"mean\"),\n",
        "                    Sortino_Mean=(\"Sortino\", \"mean\"),\n",
        "                    Trade_Count=(\"Trade_Count\", \"mean\"),\n",
        "                    Turnover=(\"Turnover\", \"mean\"),\n",
        "                    Recent_Wins_3mo=(\"Is_Win_Recent\", \"mean\"))\n",
        "               .reset_index())\n",
        "\n",
        "        # Count windows + sample-size penalty on Sharpe\n",
        "        n_win = (df_win.groupby([\"Ticker\", \"Model\"])[\"Sharpe\"]\n",
        "                      .size()\n",
        "                      .reset_index(name=\"N_Windows\"))\n",
        "        agg = agg.merge(n_win, on=[\"Ticker\", \"Model\"], how=\"left\")\n",
        "        agg[\"Sharpe_SE\"] = np.sqrt((1.0 + 0.5 * np.square(agg[\"Sharpe_Mean\"])) / agg[\"N_Windows\"].clip(lower=1))\n",
        "        agg[\"Sharpe_Mean_adj\"] = agg[\"Sharpe_Mean\"] - float(args.sharpe_se_alpha) * agg[\"Sharpe_SE\"]\n",
        "\n",
        "        # Recency weights\n",
        "        now = pd.Timestamp.utcnow()\n",
        "        if \"EndDate\" in df_win.columns:\n",
        "            df_win[\"age_days\"] = (now - pd.to_datetime(df_win[\"EndDate\"], utc=True)).dt.days.clip(lower=0)\n",
        "        else:\n",
        "            df_win[\"age_days\"] = 0\n",
        "        df_win[\"rec_w\"] = np.exp(-float(args.lambda_decay) * df_win[\"age_days\"].astype(float))\n",
        "\n",
        "        rec = (df_win.groupby([\"Ticker\", \"Model\"])\n",
        "                     .apply(lambda g: pd.Series({\n",
        "                         \"Sharpe_Rec\": (g[\"Sharpe\"] * g[\"rec_w\"]).sum() / max(1e-9, g[\"rec_w\"].sum()),\n",
        "                         \"Return_Rec\": (g[\"Return\"] * g[\"rec_w\"]).sum() / max(1e-9, g[\"rec_w\"].sum()),\n",
        "                     }))\n",
        "                     .reset_index())\n",
        "\n",
        "        # Merge into latest-window frame\n",
        "        df_ext = (df.merge(agg, on=[\"Ticker\", \"Model\"], how=\"left\")\n",
        "                    .merge(rec, on=[\"Ticker\", \"Model\"], how=\"left\"))\n",
        "\n",
        "        # Fallbacks / costs / stability\n",
        "        if \"Sortino_Mean\" not in df_ext.columns or df_ext[\"Sortino_Mean\"].isna().all():\n",
        "            df_ext[\"Sortino_Mean\"] = df_ext.get(\"Sharpe\", 0)\n",
        "\n",
        "        df_ext[\"Turnover\"] = pd.to_numeric(df_ext.get(\"Turnover\", 0), errors=\"coerce\").fillna(0)\n",
        "        df_ext[\"Sharpe_Var\"] = pd.to_numeric(df_ext.get(\"Sharpe_Var\", 0), errors=\"coerce\").fillna(0)\n",
        "        df_ext[\"Recent_Wins_3mo\"] = pd.to_numeric(df_ext.get(\"Recent_Wins_3mo\", 0), errors=\"coerce\").fillna(0)\n",
        "        df_ext[\"CostPenalty\"] = df_ext[\"Turnover\"] * (float(args.cost_bps) / 10000.0)\n",
        "        df_ext[\"StabilityBonus\"] = (0.10 * df_ext[\"Recent_Wins_3mo\"]) - (0.05 * df_ext[\"Sharpe_Var\"])\n",
        "\n",
        "        # z-scores (single pass; robust optional)\n",
        "        def _z_meanstd(s):\n",
        "            s = pd.to_numeric(s, errors=\"coerce\").fillna(0)\n",
        "            mu, sd = s.mean(), s.std(ddof=0)\n",
        "            return (s - mu) / sd if np.isfinite(sd) and sd != 0 else pd.Series(0.0, index=s.index)\n",
        "\n",
        "        z_fun = robust_z if (hasattr(args, \"scale\") and args.scale == \"robust\") else _z_meanstd\n",
        "        for c in [\"Sharpe_Mean_adj\", \"Sharpe_Rec\", \"Return_Rec\", \"Sortino_Mean\", \"MaxDD_Mean\"]:\n",
        "            if c in df_ext.columns:\n",
        "                df_ext[f\"z_{c}\"] = z_fun(df_ext[c])\n",
        "\n",
        "        # Composite score (prefers adjusted Sharpe if present)\n",
        "        df_ext[\"Score\"] = (\n",
        "            0.45 * df_ext.get(\"z_Sharpe_Mean_adj\", df_ext.get(\"z_Sharpe_Rec\", 0)) +\n",
        "            0.25 * df_ext.get(\"z_Return_Rec\", 0) +\n",
        "            0.15 * df_ext.get(\"z_Sortino_Mean\", 0) -\n",
        "            0.15 * df_ext.get(\"z_MaxDD_Mean\", 0) -\n",
        "            df_ext.get(\"CostPenalty\", 0) +\n",
        "            df_ext.get(\"StabilityBonus\", 0)\n",
        "        )\n",
        "\n",
        "        # Minimum-data gates\n",
        "        df_ext = df_ext[df_ext[\"N_Windows\"].fillna(0) >= args.min_windows]\n",
        "        df_ext = df_ext[df_ext[\"Trade_Count\"].fillna(0) >= args.min_trades]\n",
        "\n",
        "        df = df_ext\n",
        "\n",
        "    else:\n",
        "        # Original path (no windows file): family-aware scoring\n",
        "        df = compute_score_frame_with_family_weights(\n",
        "            df,\n",
        "            default_weights=weights,\n",
        "            family_weights=fam_w,\n",
        "            scale_mode=args.scale,\n",
        "            winsor_p=args.winsor\n",
        "        )\n",
        "        # Turnover cost (if present)\n",
        "        if \"Turnover\" in df.columns:\n",
        "            df[\"Score\"] = pd.to_numeric(df[\"Score\"], errors=\"coerce\").fillna(0) - \\\n",
        "                          pd.to_numeric(df[\"Turnover\"], errors=\"coerce\").fillna(0) * (float(args.cost_bps)/10000.0)\n",
        "\n",
        "    # Optional NaN drop on key fields used for sorting\n",
        "    keys = [c for c in [\"Score\", \"Sharpe\", \"Return\", \"Accuracy\", \"Drawdown\", \"Final_Portfolio\"] if c in df.columns]\n",
        "    if args.drop_nan and keys:\n",
        "        df = df.dropna(subset=keys)\n",
        "\n",
        "    # Safety + custom gates + hygiene\n",
        "    before_rows = len(df)\n",
        "    df = apply_safety_gates(df)\n",
        "    df = apply_gates_expr(df, args.gates)\n",
        "    df = dedupe(df)\n",
        "    df[\"Score\"] = pd.to_numeric(df[\"Score\"], errors=\"coerce\").replace([np.inf,-np.inf], np.nan)\n",
        "    if df[\"Score\"].isna().any():\n",
        "        fill_val = (df[\"Score\"].min(skipna=True) or 0.0) - 1e6\n",
        "        df[\"Score\"] = df[\"Score\"].fillna(fill_val)\n",
        "    log.info(\"Rows after gates: %d (from %d)\", len(df), before_rows)\n",
        "\n",
        "    # Sort for selection/tiebreakers\n",
        "    sort_cols, sort_asc = [], []\n",
        "    def add(col, asc=False):\n",
        "        if col in df.columns:\n",
        "            sort_cols.append(col); sort_asc.append(asc)\n",
        "    add(\"Score\", False); add(\"Sharpe\", False); add(\"Return\", False)\n",
        "    add(\"Sortino_Mean\", False); add(\"Drawdown\", True); add(\"Max_Drawdown\", True)\n",
        "    add(\"Accuracy\", False); add(\"Final_Portfolio\", False); add(\"Turnover\", True)\n",
        "\n",
        "    if not sort_cols:\n",
        "        raise SystemExit(\"No comparable columns to sort by. Provide metrics or weights.\")\n",
        "\n",
        "    df_sorted = df.sort_values([\"Ticker\"] + sort_cols, ascending=[True] + sort_asc)\n",
        "    df_sorted = apply_family_cap(df_sorted, args.family_cap)\n",
        "\n",
        "    # Ensure outdir exists before quick-peek write\n",
        "    Path(args.outdir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Quick peek of the top 100 rows (optional)\n",
        "    try:\n",
        "        (Path(args.outdir) / f\"quick_peek_top100_{tag}.csv\").write_text(\n",
        "            df_sorted.head(100).to_csv(index=False)\n",
        "        )\n",
        "        log.info(\"Wrote quick_peek_top100_%s.csv\", tag)\n",
        "    except Exception as e:\n",
        "        log.debug(\"quick_peek export skipped: %s\", e)\n",
        "\n",
        "    if args.export_sorted:\n",
        "        try:\n",
        "            Path(args.export_sorted).parent.mkdir(parents=True, exist_ok=True)\n",
        "            df_sorted.to_csv(args.export_sorted, index=False)\n",
        "            log.info(\"Wrote full sorted table: %s\", args.export_sorted)\n",
        "        except Exception as e:\n",
        "            log.warning(\"Failed to export sorted table (%s)\", e)\n",
        "\n",
        "    if args.export_parquet:\n",
        "        try:\n",
        "            Path(args.export_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
        "            df_sorted.to_parquet(args.export_parquet, index=False)\n",
        "            log.info(\"Wrote parquet: %s\", args.export_parquet)\n",
        "        except Exception as e:\n",
        "            log.warning(\"Parquet export failed (install pyarrow or fastparquet?): %s\", e)\n",
        "\n",
        "    # Load incumbents JSON (optional)\n",
        "    incumbents: Dict[str, Any] = {}\n",
        "    if args.incumbents and Path(args.incumbents).exists():\n",
        "        try:\n",
        "            with open(args.incumbents, \"r\") as f:\n",
        "                incumbents = json.load(f)\n",
        "                incumbents = {str(k).upper().strip(): v for k, v in incumbents.items()}\n",
        "        except Exception as e:\n",
        "            log.warning(\"Failed to load incumbents JSON: %s\", e)\n",
        "\n",
        "    # JSON (ensemble/significance + optional FDR) using df_sorted\n",
        "    _, selection_map = build_selection(\n",
        "        df_sorted=df_sorted,\n",
        "        df_win=df_win,\n",
        "        incumbents=incumbents,\n",
        "        eps=float(args.eps),\n",
        "        eps_mode=args.eps_mode,\n",
        "        ensemble_multi=args.ensemble_multi,\n",
        "        fdr_alpha=float(args.fdr_alpha),\n",
        "        ensemble_max=int(args.ensemble_max),\n",
        "    )\n",
        "\n",
        "    # Top-K table for CSV/XLSX\n",
        "    best_pk, _ = pick_best_per_ticker(df, k=args.topk)\n",
        "\n",
        "    # Summary by model (info log)\n",
        "    try:\n",
        "        model_means = summarize_by_model(df)\n",
        "        log.info(\"Top models (means):\\n%s\", model_means.to_string(index=False))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if args.dry_run:\n",
        "        log.info(\"Dry run: skipping writes\")\n",
        "        return\n",
        "\n",
        "    # Write outputs (CSV/XLSX + richer JSON) + provenance\n",
        "    outdir = Path(args.outdir)\n",
        "    csv_path, xlsx_path, json_path = write_outputs(best_pk, outdir, tag, selection_map)\n",
        "    write_meta(outdir, args)\n",
        "\n",
        "    log.info(\"Saved: %s\", csv_path)\n",
        "    log.info(\"Saved: %s\", xlsx_path)\n",
        "    log.info(\"Saved: %s\", json_path)\n",
        "\n",
        "# =========================\n",
        "# STEP C: RUN PIPELINE\n",
        "# =========================\n",
        "\n",
        "# In Colab, we simulate CLI flags by setting sys.argv before calling main().\n",
        "# If you want to tweak parameters, edit the list below.\n",
        "if IN_COLAB:\n",
        "    os.makedirs(OUTDIR, exist_ok=True)\n",
        "    sys.argv = [\n",
        "        \"colab_kernel_launcher.py\",\n",
        "        \"--input\", COMBINED_CSV,\n",
        "        \"--outdir\", OUTDIR,\n",
        "        # Optional tuning flags (uncomment or edit as needed):\n",
        "        # \"--weights\", \"Sharpe=0.5,Return=0.3,Accuracy=0.2,Drawdown=-0.1\",\n",
        "        # \"--scale\", \"robust\",\n",
        "        # \"--winsor\", \"0.01\",\n",
        "        # \"--gates\", \"Sharpe>0,Max_Drawdown>-0.5,Return>-0.2\",\n",
        "        # \"--family_cap\", \"3\",\n",
        "        # \"--ensemble_multi\",\n",
        "        # \"--eps_mode\", \"relative\",\n",
        "        # \"--eps\", \"0.03\",\n",
        "        # \"--export_sorted\", os.path.join(OUTDIR, \"full_sorted.csv\"),\n",
        "        \"--dry_run\"  # set to comment/remove to actually write the final outputs\n",
        "    ]\n",
        "\n",
        "# Normal script guard (lets this file also run from the command-line)\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# =========================\n",
        "# STEP D: OPTIONAL â€” PEEK AT BEST MODELS (from dry_run data frame path)\n",
        "# If you removed --dry_run above, the outputs will be written under OUTDIR.\n",
        "# =========================\n",
        "\n",
        "print(\"\\nâœ… Pipeline completed. Check your output folder here:\")\n",
        "print(OUTDIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "aFMaZwaqaqE8",
        "outputId": "c287fa7e-3bd2-4e12-cff5-19d9f9adcade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No model selector CSVs found in folder. Place files ending with *_model_selector.csv into SELECTOR_DIR.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-948081845.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_dfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     raise ValueError(\"No model selector CSVs found in folder. \"\n\u001b[0m\u001b[1;32m     57\u001b[0m                      \"Place files ending with *_model_selector.csv into SELECTOR_DIR.\")\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No model selector CSVs found in folder. Place files ending with *_model_selector.csv into SELECTOR_DIR."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Ensure numeric types for plotting ===\n",
        "for col in ['Sharpe', 'Return', 'Accuracy', 'Score']:\n",
        "    df[col] = pd.to_numeric(df.get(col), errors='coerce').fillna(0)\n",
        "\n",
        "# === Compute model averages ===\n",
        "model_means = df.groupby(\"Model\")[['Sharpe', 'Return', 'Accuracy', 'Score']].mean().sort_values(\"Score\", ascending=False)\n",
        "\n",
        "# === Select Top 5 and Bottom 5 Models ===\n",
        "top5_models = model_means.head(5)\n",
        "bottom5_models = model_means.tail(5)\n",
        "\n",
        "# === Combine for Top & Bottom Chart ===\n",
        "top_bottom_combined = pd.concat([top5_models, bottom5_models])\n",
        "\n",
        "# === Plot 1: Top & Bottom 5 Models by Average Score ===\n",
        "plt.figure(figsize=(14, 6))\n",
        "top_bottom_combined[['Score']].sort_values(\"Score\").plot(kind='barh', legend=False, color='steelblue')\n",
        "plt.title(\"Top 5 and Bottom 5 Models by Average Score\")\n",
        "plt.xlabel(\"Average Score\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Plot 2: Average Metric Comparison (Top 5 only) ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "top5_models[['Sharpe', 'Return', 'Accuracy']].plot(kind='bar', figsize=(14, 6))\n",
        "plt.title(\"Average Sharpe, Return, and Accuracy for Top 5 Models\")\n",
        "plt.ylabel(\"Average Value\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Plot 3: Score Distribution per Model ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df, x=\"Model\", y=\"Score\", hue=\"Model\", palette=\"Set3\", dodge=False)\n",
        "plt.title(\"Score Distribution per Model\")\n",
        "plt.legend([],[], frameon=False)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Plot 4: Return Distribution per Model ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df, x=\"Model\", y=\"Return\", hue=\"Model\", palette=\"Set2\", dodge=False)\n",
        "plt.title(\"Return Distribution per Model\")\n",
        "plt.legend([],[], frameon=False)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Plot 5: Sharpe Ratio Distribution per Model ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df, x=\"Model\", y=\"Sharpe\", hue=\"Model\", palette=\"Set1\", dodge=False)\n",
        "plt.title(\"Sharpe Ratio Distribution per Model\")\n",
        "plt.legend([],[], frameon=False)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QlE3ED-QkqaE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}