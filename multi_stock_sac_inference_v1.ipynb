{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/exploratory_daytrading/blob/main/multi_stock_sac_inference_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWICIz6OC5A7"
      },
      "outputs": [],
      "source": [
        "#Fix Potential Library Conflicts\n",
        "!apt-get remove --purge -y cuda* libcuda* nvidia* || echo \"No conflicting CUDA packages\"\n",
        "!apt-get autoremove -y\n",
        "!apt-get clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdBgVFvnC-ht"
      },
      "outputs": [],
      "source": [
        "#Protocol Buffer Fix (for TensorFlow)\n",
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzOhpbF5DAiq"
      },
      "outputs": [],
      "source": [
        "#Update Colab Environment and System Libraries\n",
        "!apt-get update -y && apt-get upgrade -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0agNVkK_DCf0"
      },
      "outputs": [],
      "source": [
        "#Install Correct Version of CUDA for Colab GPU\n",
        "!apt-get update -qq && apt-get install -y \\\n",
        "    libcusolver11 libcusparse11 libcurand10 libcufft10 libnppig10 libnppc10 libnppial10 \\\n",
        "    cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSt-4dWIDFOg"
      },
      "outputs": [],
      "source": [
        "#Set Correct CUDA Paths\n",
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-12.4'\n",
        "os.environ['PATH'] += ':/usr/local/cuda-12.4/bin'\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/cuda-12.4/lib64'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjt_aeX_DISp"
      },
      "outputs": [],
      "source": [
        "#Install RAPIDS and NVIDIA Dependencies\n",
        "!pip install --extra-index-url=https://pypi.nvidia.com \\\n",
        "    cuml-cu12==25.2.0 cudf-cu12==25.2.0 cupy-cuda12x dask-cuda==25.2.0 dask-cudf-cu12==25.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19mK1sF0DMQA"
      },
      "outputs": [],
      "source": [
        "#Install TensorFlow (latest GPU-compatible version)\n",
        "!pip install tensorflow==2.18.0\n",
        "\n",
        "#Install Stable Baselines3 and Trading Libraries\n",
        "!pip install stable-baselines3[extra] gymnasium gym-anytrading yfinance xgboost joblib\n",
        "\n",
        "#Install Miscellaneous Libraries\n",
        "!pip install matplotlib scikit-learn pandas numba==0.61.0\n",
        "\n",
        "#Install PyTorch with GPU Support\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hptpO38jLWsu"
      },
      "outputs": [],
      "source": [
        "# In a notebook cell:\n",
        "!pip uninstall -y jax jaxlib\n",
        "\n",
        "# Then restart the runtime/kernel, and import TF normally:\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh3DtDQbD0r7"
      },
      "outputs": [],
      "source": [
        "#!rm -rf /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkm7fsqpD2du"
      },
      "outputs": [],
      "source": [
        "# ---------- Imports & Logging ----------\n",
        "import os, glob, gc, time, json, logging, warnings, re, sys\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import pywt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Gym has been unmaintained.*\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"jupyter_client.session\")\n",
        "\n",
        "\n",
        "def setup_logger(name=\"download_fe\", level=logging.INFO):\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(level)\n",
        "    # avoid duplicate handlers on re-run\n",
        "    logger.handlers = []\n",
        "    fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\")\n",
        "\n",
        "    sh = logging.StreamHandler(sys.stdout)\n",
        "    sh.setFormatter(fmt); sh.setLevel(level)\n",
        "    logger.addHandler(sh)\n",
        "\n",
        "    os.makedirs(\"./sac_parity/logs\", exist_ok=True)\n",
        "    fh = logging.FileHandler(os.path.join(\"./sac_parity/logs\", f\"{name}.log\"))\n",
        "    fh.setFormatter(fmt); fh.setLevel(level)\n",
        "    logger.addHandler(fh)\n",
        "    return logger\n",
        "\n",
        "log = setup_logger(\"download_fe\", level=logging.INFO)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "log.info(\"yfinance=%s | pandas=%s\", getattr(yf, \"__version__\", \"unknown\"), pd.__version__)\n",
        "\n",
        "# ---------- Paths ----------\n",
        "BASE_DIR   = \"./sac_parity\"\n",
        "DATA_DIR   = os.path.join(BASE_DIR, \"data\")\n",
        "LOG_DIR    = os.path.join(BASE_DIR, \"logs\")\n",
        "for d in (BASE_DIR, DATA_DIR, LOG_DIR):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Try Google Drive (Colab); else local CWD\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    DRIVE_BASE = \"/content/drive/MyDrive\"\n",
        "    log.info(\"Mounted Google Drive at /content/drive\")\n",
        "except Exception:\n",
        "    DRIVE_BASE = os.getcwd()\n",
        "    log.info(\"Google Drive not available; using local working directory\")\n",
        "\n",
        "RESULTS_DIR = os.path.join(DRIVE_BASE, \"Results_May_2025\", \"results_sac_walkforward\")\n",
        "TRADING_DIR = os.path.join(DRIVE_BASE, \"trading_data\")\n",
        "for d in (RESULTS_DIR, TRADING_DIR):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"RESULTS_DIR =\", RESULTS_DIR)\n",
        "print(\"Exists?\", os.path.exists(RESULTS_DIR))\n",
        "print(\"Listdir sample:\", os.listdir(RESULTS_DIR)[:10] if os.path.exists(RESULTS_DIR) else \"N/A\")\n",
        "\n",
        "csvs = sorted(glob.glob(os.path.join(RESULTS_DIR, \"*_sac_ws_sweep_summary.csv\")))\n",
        "print(\"Matched:\", [os.path.basename(p) for p in csvs])\n",
        "\n",
        "for p in csvs:\n",
        "    df = pd.read_csv(p)\n",
        "    print(os.path.basename(p), \"rows:\", len(df))\n",
        "\n",
        "# Primary output filenames (three mirrors)\n",
        "FEATURE_CSV_RESULTS = os.path.join(RESULTS_DIR, \"multi_stock_feature_engineered_dataset.csv\")\n",
        "FEATURE_CSV_TRADING = os.path.join(TRADING_DIR, \"multi_stock_feature_engineered_dataset.csv\")\n",
        "FEATURE_CSV_LOCAL   = \"multi_stock_feature_engineered_dataset.csv\"\n",
        "\n",
        "# Optional Parquet mirrors (local)\n",
        "PARQ_FULL  = \"features_full.parquet\"\n",
        "PARQ_TRAIN = \"train.parquet\"\n",
        "PARQ_VAL   = \"val.parquet\"\n",
        "\n",
        "# ---------- Config / Toggles ----------\n",
        "USE_SENTIMENT = False   # placeholder; off by default\n",
        "USE_REGIME    = True\n",
        "USE_WAVELET   = True\n",
        "USE_EVAL_CALLBACK = False\n",
        "FORCE_RETRAIN = True\n",
        "\n",
        "# Data params\n",
        "INTERVAL     = os.getenv(\"INTERVAL\", \"1h\")\n",
        "PERIOD_DAYS  = int(os.getenv(\"PERIOD_DAYS\", \"720\"))  # ~2 years by default\n",
        "\n",
        "# If you want to prep for a 1y train + 1y test split later, ensure ~744 days\n",
        "TRAIN_DAYS_TARGET = 365\n",
        "TEST_DAYS_TARGET  = 365\n",
        "BUFFER_DAYS       = 14\n",
        "MIN_PERIOD_FOR_TRAINER = TRAIN_DAYS_TARGET + TEST_DAYS_TARGET + BUFFER_DAYS  # ~744\n",
        "\n",
        "# --- Yahoo intraday maximum window helper ---\n",
        "def _max_days_for_interval(interval: str) -> int:\n",
        "    \"\"\"Yahoo intraday windows are restricted; daily/weekly can go further.\"\"\"\n",
        "    intraday_caps = {\n",
        "        \"1m\": 30, \"2m\": 60, \"5m\": 60, \"15m\": 60, \"30m\": 60,\n",
        "        \"60m\": 730, \"90m\": 60, \"1h\": 730\n",
        "    }\n",
        "    return intraday_caps.get(interval.lower(), 3650)\n",
        "\n",
        "\n",
        "TEST_MODE = os.getenv(\"TEST_MODE\", \"1\").strip().lower() not in (\"0\", \"false\", \"no\", \"\")\n",
        "PREP_FOR_SAC = os.getenv(\"PREP_FOR_SAC\", \"1\").lower() not in (\"0\", \"false\", \"no\", \"\")\n",
        "if PREP_FOR_SAC and PERIOD_DAYS < MIN_PERIOD_FOR_TRAINER:\n",
        "    log.info(\"Bumping PERIOD_DAYS %d → %d to support ~1y train / ~1y test\", PERIOD_DAYS, MIN_PERIOD_FOR_TRAINER)\n",
        "    PERIOD_DAYS = MIN_PERIOD_FOR_TRAINER\n",
        "\n",
        "# --- Cap for Yahoo intraday data limits (avoid >730 days error) ---\n",
        "_cap = _max_days_for_interval(INTERVAL)\n",
        "if PERIOD_DAYS > _cap:\n",
        "    log.warning(\"PERIOD_DAYS=%d exceeds Yahoo limit (%d) for interval='%s'; capping.\",\n",
        "                PERIOD_DAYS, _cap, INTERVAL)\n",
        "    PERIOD_DAYS = _cap\n",
        "\n",
        "# ---------- Universe ----------\n",
        "TICKERS_ALL = [\n",
        "    'AAPL','TSLA','MSFT','GOOGL','AMZN','NVDA','META','BRK-B','JPM','JNJ',\n",
        "    'XOM','V','PG','UNH','MA','HD','LLY','MRK','PEP','KO',\n",
        "    'BAC','ABBV','AVGO','PFE','COST','CSCO','TMO','ABT','ACN','WMT',\n",
        "    'MCD','ADBE','DHR','CRM','NKE','INTC','QCOM','NEE','AMD','TXN',\n",
        "    'AMGN','UPS','LIN','PM','UNP','BMY','LOW','RTX','CVX','IBM',\n",
        "    'GE','SBUX','ORCL'\n",
        "]\n",
        "\n",
        "DEFAULT_TEST_TICKERS = ['AAPL','NVDA','MSFT']\n",
        "RUN_SYMBOLS_ENV  = [s.strip().upper() for s in os.getenv(\"RUN_SYMBOLS\", \"\").split(\",\") if s.strip()]\n",
        "TEST_TICKERS_ENV = [s.strip().upper() for s in os.getenv(\"TEST_TICKERS\", \"\").split(\",\") if s.strip()]\n",
        "\n",
        "if RUN_SYMBOLS_ENV:\n",
        "    SYMBOLS = RUN_SYMBOLS_ENV\n",
        "    log.info(\"RUN_SYMBOLS override → %s\", SYMBOLS)\n",
        "elif TEST_MODE:\n",
        "    SYMBOLS = TEST_TICKERS_ENV if TEST_TICKERS_ENV else DEFAULT_TEST_TICKERS\n",
        "    log.info(\"TEST_MODE=on → symbols=%s | period_days=%d\", SYMBOLS, PERIOD_DAYS)\n",
        "else:\n",
        "    SYMBOLS = TICKERS_ALL\n",
        "    log.info(\"TEST_MODE=off → %d symbols | period_days=%d\", len(SYMBOLS), PERIOD_DAYS)\n",
        "\n",
        "# ---------- Helpers: atomic save + verification ----------\n",
        "def _ensure_dir(path: str):\n",
        "    d = os.path.dirname(path)\n",
        "    if d and not os.path.exists(d):\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def save_csv_atomically(df: pd.DataFrame, dest_path: str, max_wait_s: float = 3.0):\n",
        "    \"\"\"Write via dest.tmp then os.replace(); verify non-zero size (Colab/Drive-safe).\"\"\"\n",
        "    _ensure_dir(dest_path)\n",
        "    tmp_path = dest_path + \".tmp\"\n",
        "    df.to_csv(tmp_path, index=False)\n",
        "    os.replace(tmp_path, dest_path)\n",
        "    t0 = time.time()\n",
        "    while (not os.path.exists(dest_path) or os.path.getsize(dest_path) == 0) and (time.time() - t0 < max_wait_s):\n",
        "        time.sleep(0.2)\n",
        "    assert os.path.exists(dest_path) and os.path.getsize(dest_path) > 0, f\"Save failed for {dest_path}\"\n",
        "    return dest_path\n",
        "\n",
        "def _verify_csv_ok(p: str) -> bool:\n",
        "    try:\n",
        "        if not (os.path.exists(p) and os.path.getsize(p) > 0):\n",
        "            return False\n",
        "        _ = pd.read_csv(p, nrows=2)\n",
        "        log.info(\"Verified dataset CSV: %s (%s bytes)\", p, f\"{os.path.getsize(p):,}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        log.warning(\"Dataset candidate failed verification (%s): %s\", p, e)\n",
        "        return False\n",
        "\n",
        "def _log_artifact(p: str):\n",
        "    try:\n",
        "        ok = os.path.exists(p); sz = os.path.getsize(p) if ok else 0\n",
        "        tm = time.ctime(os.path.getmtime(p)) if ok else \"-\"\n",
        "        head = pd.read_csv(p, nrows=2)\n",
        "        log.info(\"Artifact OK: %s | size=%s | mtime=%s | cols=%s\",\n",
        "                 p, f\"{sz:,}\", tm, list(head.columns)[:8])\n",
        "        log.info(\"Head preview:\\n%s\", head.to_string(index=False))\n",
        "    except Exception as e:\n",
        "        log.warning(\"Artifact check failed for %s: %s\", p, e)\n",
        "\n",
        "# ---------- Helpers: schema / normalize ----------\n",
        "def _force_datetime_column(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Ensure tz-naive Datetime column exists; dedupe/sort.\"\"\"\n",
        "    if isinstance(df.index, pd.DatetimeIndex):\n",
        "        try:\n",
        "            if df.index.tz is not None:\n",
        "                df.index = df.index.tz_convert(None)\n",
        "        except Exception:\n",
        "            try:\n",
        "                df.index = df.index.tz_localize(None)\n",
        "            except Exception:\n",
        "                pass\n",
        "        df.index.name = 'Datetime'\n",
        "        df = df.reset_index()\n",
        "    else:\n",
        "        df = df.reset_index()\n",
        "        first = df.columns[0]\n",
        "        if np.issubdtype(df[first].dtype, np.datetime64):\n",
        "            df = df.rename(columns={first: 'Datetime'})\n",
        "        elif 'Date' in df.columns:\n",
        "            df['Datetime'] = pd.to_datetime(df['Date'])\n",
        "        elif 'Datetime' not in df.columns:\n",
        "            df['Datetime'] = pd.to_datetime(df[first], errors='coerce')\n",
        "\n",
        "    if 'Datetime' not in df.columns:\n",
        "        raise KeyError(\"Failed to construct 'Datetime' from data.\")\n",
        "\n",
        "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
        "    return df.drop_duplicates(subset=['Datetime']).sort_values('Datetime').reset_index(drop=True)\n",
        "\n",
        "def _normalize_ohlcv(df_in: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
        "    \"\"\"Flatten MultiIndex; strip ticker tokens; map to canonical OHLCV names.\"\"\"\n",
        "    df = df_in.copy()\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(p) for p in col if p]) for col in df.columns]\n",
        "    df.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in df.columns]\n",
        "\n",
        "    tkr = ticker.upper().replace(\"-\", \"[- ]?\")\n",
        "    cleaned = {}\n",
        "    for c in df.columns:\n",
        "        cu = c.upper()\n",
        "        cu = re.sub(rf\"^(?:{tkr})[\\s/_-]+\", \"\", cu)\n",
        "        cu = re.sub(rf\"[\\s/_-]+(?:{tkr})$\", \"\", cu)\n",
        "        cleaned[c] = cu.title()\n",
        "    if any(cleaned[c] != c for c in df.columns):\n",
        "        df = df.rename(columns=cleaned)\n",
        "\n",
        "    cols_ci = {c.lower(): c for c in df.columns}\n",
        "    wants = {\n",
        "        \"Open\":      [\"open\"],\n",
        "        \"High\":      [\"high\"],\n",
        "        \"Low\":       [\"low\"],\n",
        "        \"Close\":     [\"close\", \"last\", \"close*\"],\n",
        "        \"Adj Close\": [\"adj close\",\"adj_close\",\"adjclose\",\"adjusted close\"],\n",
        "        \"Volume\":    [\"volume\",\"vol\"]\n",
        "    }\n",
        "    rename_map = {}\n",
        "    for desired, alts in wants.items():\n",
        "        if desired.lower() in cols_ci:\n",
        "            rename_map[cols_ci[desired.lower()]] = desired\n",
        "        else:\n",
        "            for a in alts:\n",
        "                if a in cols_ci:\n",
        "                    rename_map[cols_ci[a]] = desired\n",
        "                    break\n",
        "    if rename_map:\n",
        "        df = df.rename(columns=rename_map)\n",
        "    return df\n",
        "\n",
        "# ---------- Downloader (with history() fallback & retries) ----------\n",
        "def download_stock_data(ticker, interval=\"1h\", period_days=720, max_retries=5, sleep_base=3):\n",
        "    \"\"\"\n",
        "    Robust yfinance intraday downloader.\n",
        "    Guarantees: Open, High, Low, Close, Volume (+Adj Close), Datetime, Symbol.\n",
        "    \"\"\"\n",
        "    period_days = int(period_days)\n",
        "    period_str = f\"{period_days}d\"\n",
        "\n",
        "    def _post(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = _normalize_ohlcv(df, ticker)\n",
        "        df = _force_datetime_column(df)\n",
        "        needed = {'Open', 'High', 'Low', 'Close', 'Volume'}\n",
        "        missing = needed - set(df.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing OHLCV columns after normalize: {missing}\")\n",
        "        if 'Adj Close' not in df.columns:\n",
        "            df['Adj Close'] = df['Close']\n",
        "        return df\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            log.info(f\"[{ticker}] Attempt {attempt}: download(period={period_str}, interval={interval})\")\n",
        "            df = yf.download(\n",
        "                tickers=ticker,\n",
        "                period=period_str,\n",
        "                interval=interval,\n",
        "                progress=False,\n",
        "                auto_adjust=False,\n",
        "                group_by='column',\n",
        "                threads=False,\n",
        "                prepost=False,\n",
        "                repair=True\n",
        "            )\n",
        "            if df is None or df.empty:\n",
        "                raise ValueError(\"Empty data from download()\")\n",
        "\n",
        "            df = _post(df)\n",
        "            df['Symbol'] = ticker\n",
        "            log.info(f\"[{ticker}] rows={len(df)} {df['Datetime'].min()} → {df['Datetime'].max()}\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e1:\n",
        "            # --- Yahoo 730-day clamp logic (self-heal) ---\n",
        "            msg = str(e1)\n",
        "            too_long = (\"must be within the last 730 days\" in msg.lower()) or (\"no price data found\" in msg.lower())\n",
        "            if too_long:\n",
        "                clamp_days = _max_days_for_interval(interval)\n",
        "                if period_days > clamp_days:\n",
        "                    log.warning(\"[%s] Yahoo limit hit (%s). Retrying with period_days=%d.\", ticker, e1, clamp_days)\n",
        "                    period_days = clamp_days\n",
        "                    period_str = f\"{period_days}d\"\n",
        "                    try:\n",
        "                        df = yf.download(\n",
        "                            tickers=ticker, period=period_str, interval=interval,\n",
        "                            progress=False, auto_adjust=False, group_by='column',\n",
        "                            threads=False, prepost=False, repair=True\n",
        "                        )\n",
        "                        if df is not None and not df.empty:\n",
        "                            df = _post(df)\n",
        "                            df['Symbol'] = ticker\n",
        "                            log.info(f\"[{ticker}] rows={len(df)} {df['Datetime'].min()} → {df['Datetime'].max()} (clamped)\")\n",
        "                            return df\n",
        "                    except Exception as e1b:\n",
        "                        log.warning(\"[%s] clamp retry failed (%s); will try history() fallback.\", ticker, e1b)\n",
        "\n",
        "            # --- history() fallback with backoff ---\n",
        "            log.warning(f\"[{ticker}] download() error: {e1} | trying Ticker().history()\")\n",
        "            try:\n",
        "                hist = yf.Ticker(ticker).history(\n",
        "                    period=period_str,\n",
        "                    interval=interval,\n",
        "                    auto_adjust=False,\n",
        "                    actions=False\n",
        "                )\n",
        "                if hist is None or hist.empty:\n",
        "                    raise ValueError(\"Empty data from history()\")\n",
        "\n",
        "                df = _post(hist)\n",
        "                df['Symbol'] = ticker\n",
        "                log.info(f\"[{ticker}] (fallback) rows={len(df)} {df['Datetime'].min()} → {df['Datetime'].max()}\")\n",
        "                return df\n",
        "\n",
        "            except Exception as e2:\n",
        "                wait = sleep_base * attempt\n",
        "                log.warning(f\"[{ticker}] history() error: {e2} | retrying in {wait}s\")\n",
        "                time.sleep(wait)\n",
        "\n",
        "    log.error(f\"[{ticker}] Failed after {max_retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# ---------- Feature Engineering ----------\n",
        "def denoise_wavelet(series, wavelet='db1', level=2):\n",
        "    s = pd.Series(series).astype(float).ffill().bfill().to_numpy()\n",
        "    try:\n",
        "        coeffs = pywt.wavedec(s, wavelet, mode='symmetric', level=level)\n",
        "        # Hard smoothing: zero high-frequency detail\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = np.zeros_like(coeffs[i])\n",
        "        rec = pywt.waverec(coeffs, wavelet, mode='symmetric')\n",
        "        return pd.Series(rec[:len(s)], index=series.index)\n",
        "    except Exception as e:\n",
        "        log.warning(f\"Wavelet denoise failed ({e}); returning raw series.\")\n",
        "        return pd.Series(s, index=series.index)\n",
        "\n",
        "def add_regime(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df['Vol20'] = df['Close'].pct_change().rolling(20).std()\n",
        "    df['Ret20'] = df['Close'].pct_change(20)\n",
        "    vol_hi   = (df['Vol20'] > df['Vol20'].median()).astype(int)\n",
        "    trend_hi = (df['Ret20'].abs() > df['Ret20'].abs().median()).astype(int)\n",
        "    df['Regime4'] = vol_hi * 2 + trend_hi  # 0..3\n",
        "    return df\n",
        "\n",
        "def compute_enhanced_features(df: pd.DataFrame) -> tuple[pd.DataFrame, list]:\n",
        "    \"\"\"\n",
        "    Returns (feature_df, FEATURES). Input must contain:\n",
        "      Datetime, Symbol, Open, High, Low, Close, Volume.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    req = {\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"}\n",
        "    assert req.issubset(df.columns), f\"OHLCV columns missing: {req - set(df.columns)}\"\n",
        "\n",
        "    close = df[\"Close\"].astype(\"float64\")\n",
        "    high  = df[\"High\"].astype(\"float64\")\n",
        "    low   = df[\"Low\"].astype(\"float64\")\n",
        "    open_ = df[\"Open\"].astype(\"float64\")\n",
        "    vol   = (df[\"Volume\"].astype(\"float64\") + 1.0)\n",
        "\n",
        "    # Returns & lags\n",
        "    df[\"ret_1\"]     = close.pct_change(1)\n",
        "    df[\"ret_3\"]     = close.pct_change(3)\n",
        "    df[\"ret_5\"]     = close.pct_change(5)\n",
        "    df[\"ret_10\"]    = close.pct_change(10)\n",
        "    df[\"logret_1\"]  = np.log(close).diff(1)\n",
        "\n",
        "    # MAs & volatility\n",
        "    df[\"ma_5\"]      = close.rolling(5).mean()\n",
        "    df[\"ma_10\"]     = close.rolling(10).mean()\n",
        "    df[\"ma_20\"]     = close.rolling(20).mean()\n",
        "    df[\"ema_10\"]    = close.ewm(span=10, adjust=False).mean()\n",
        "    df[\"ema_20\"]    = close.ewm(span=20, adjust=False).mean()\n",
        "    df[\"std_10\"]    = close.rolling(10).std()\n",
        "    df[\"std_20\"]    = close.rolling(20).std()\n",
        "    df[\"ema10_ratio\"] = df[\"ema_10\"] / close - 1.0\n",
        "    df[\"ema20_ratio\"] = df[\"ema_20\"] / close - 1.0\n",
        "    df[\"z_close_20\"] = (close - df[\"ma_20\"]) / df[\"std_20\"].replace(0, np.nan)\n",
        "\n",
        "    # RSI(14)\n",
        "    delta      = close.diff()\n",
        "    up         = delta.clip(lower=0.0)\n",
        "    down       = -delta.clip(upper=0.0)\n",
        "    roll_up    = up.ewm(alpha=1/14, adjust=False).mean()\n",
        "    roll_down  = down.ewm(alpha=1/14, adjust=False).mean()\n",
        "    rs         = roll_up / roll_down.replace(0, np.nan)\n",
        "    df[\"rsi_14\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # Stochastic(14,3)\n",
        "    ll14       = low.rolling(14).min()\n",
        "    hh14       = high.rolling(14).max()\n",
        "    den_14     = (hh14 - ll14).replace(0, np.nan)\n",
        "    df[\"stoch_k\"] = 100 * (close - ll14) / den_14\n",
        "    df[\"stoch_d\"] = df[\"stoch_k\"].rolling(3).mean()\n",
        "\n",
        "    # MACD (12,26,9)\n",
        "    ema12         = close.ewm(span=12, adjust=False).mean()\n",
        "    ema26         = close.ewm(span=26, adjust=False).mean()\n",
        "    macd          = ema12 - ema26\n",
        "    macd_sig      = macd.ewm(span=9, adjust=False).mean()\n",
        "    df[\"macd\"]        = macd\n",
        "    df[\"macd_signal\"] = macd_sig\n",
        "    df[\"macd_hist\"]   = macd - macd_sig\n",
        "\n",
        "    # Bollinger (20,2)\n",
        "    bb_mid      = df[\"ma_20\"]\n",
        "    bb_std      = df[\"std_20\"]\n",
        "    bb_up       = bb_mid + 2 * bb_std\n",
        "    bb_lo       = bb_mid - 2 * bb_std\n",
        "    band_width  = (bb_up - bb_lo)\n",
        "    df[\"bb_perc_b\"]    = (close - bb_lo) / band_width.replace(0, np.nan)\n",
        "    df[\"bb_bandwidth\"] = band_width / bb_mid.replace(0, np.nan)\n",
        "\n",
        "    # ATR(14)\n",
        "    prev_close  = close.shift(1)\n",
        "    tr = np.maximum(high - low,\n",
        "                    np.maximum((high - prev_close).abs(), (low - prev_close).abs()))\n",
        "    df[\"atr_14\"] = tr.rolling(14).mean()\n",
        "\n",
        "    # Volume features\n",
        "    df[\"vol_ma_20\"]    = vol.rolling(20).mean()\n",
        "    df[\"vol_std_20\"]   = vol.rolling(20).std()\n",
        "    df[\"vol_z_20\"]     = (vol - df[\"vol_ma_20\"]) / df[\"vol_std_20\"].replace(0, np.nan)\n",
        "    df[\"vol_change_1\"] = vol.pct_change(1)\n",
        "\n",
        "    # Spreads & crosses\n",
        "    df[\"hl_spread\"] = (high - low) / close.replace(0, np.nan)\n",
        "    df[\"oc_spread\"] = (close - open_) / open_.replace(0, np.nan)\n",
        "    df[\"ema10_gt_ema20\"] = (df[\"ema_10\"] > df[\"ema_20\"]).astype(\"float32\")\n",
        "\n",
        "    # Optional extras\n",
        "    if USE_WAVELET:\n",
        "        df[\"denoised_close\"] = denoise_wavelet(pd.Series(close, index=df.index))\n",
        "    if USE_REGIME:\n",
        "        df = add_regime(df)  # adds 'Regime4'\n",
        "\n",
        "    # Clean up\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df.fillna(0.0, inplace=True)\n",
        "\n",
        "    FEATURES = [\n",
        "        \"ret_1\",\"ret_3\",\"ret_5\",\"ret_10\",\"logret_1\",\n",
        "        \"ma_5\",\"ma_10\",\"ma_20\",\"ema_10\",\"ema_20\",\"ema10_ratio\",\"ema20_ratio\",\n",
        "        \"std_10\",\"std_20\",\"z_close_20\",\n",
        "        \"rsi_14\",\"stoch_k\",\"stoch_d\",\n",
        "        \"macd\",\"macd_signal\",\"macd_hist\",\n",
        "        \"bb_perc_b\",\"bb_bandwidth\",\n",
        "        \"atr_14\",\n",
        "        \"vol_ma_20\",\"vol_std_20\",\"vol_z_20\",\"vol_change_1\",\n",
        "        \"hl_spread\",\"oc_spread\",\"ema10_gt_ema20\",\n",
        "    ]\n",
        "    df[FEATURES] = df[FEATURES].astype(\"float32\")\n",
        "    return df, FEATURES\n",
        "\n",
        "# ---------- Orchestrator ----------\n",
        "VAL_FRACTION = float(os.getenv(\"VAL_FRACTION\", \"0.20\"))  # only for optional local train/val files\n",
        "CANDIDATE_CSVS = [FEATURE_CSV_LOCAL, FEATURE_CSV_RESULTS, FEATURE_CSV_TRADING]\n",
        "\n",
        "def build_features() -> pd.DataFrame:\n",
        "    # ---- PPO-style: re-use any healthy dataset across locations ----\n",
        "    for cand in CANDIDATE_CSVS:\n",
        "        if _verify_csv_ok(cand):\n",
        "            d = pd.read_csv(cand)\n",
        "            d['Datetime'] = pd.to_datetime(d['Datetime'], utc=True)  # keep UTC in memory\n",
        "            log.info(\"Using existing features CSV: %s | rows=%d cols=%d\", cand, len(d), d.shape[1])\n",
        "            return d\n",
        "\n",
        "    all_dfs = []\n",
        "    for i, ticker in enumerate(SYMBOLS, 1):\n",
        "        log.info(f\"[{i}/{len(SYMBOLS)}] {ticker} — downloading\")\n",
        "        raw = download_stock_data(ticker, interval=INTERVAL, period_days=PERIOD_DAYS)\n",
        "        if raw is None or raw.empty:\n",
        "            log.warning(f\"[{ticker}] no data; skipping.\")\n",
        "            continue\n",
        "        try:\n",
        "            feats, _ = compute_enhanced_features(raw)\n",
        "            if feats is not None and not feats.empty:\n",
        "                outp = os.path.join(DATA_DIR, f\"{ticker}.parquet\")\n",
        "                feats.to_parquet(outp, index=False)\n",
        "                log.info(f\"[{ticker}] features={len(feats)} rows → {outp}\")\n",
        "                all_dfs.append(feats)\n",
        "            else:\n",
        "                log.warning(f\"[{ticker}] empty features; skipped.\")\n",
        "        except Exception as e:\n",
        "            log.error(f\"[{ticker}] FE failed: {e}\")\n",
        "        finally:\n",
        "            del raw\n",
        "            try: del feats\n",
        "            except: pass\n",
        "            gc.collect()\n",
        "            time.sleep(0.2)\n",
        "\n",
        "    if not all_dfs:\n",
        "        raise RuntimeError(\"No usable data found for any ticker.\")\n",
        "\n",
        "    full = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "    # ---- Keep UTC in storage; only convert a view to NY for RTH mask ----\n",
        "    full['Datetime'] = pd.to_datetime(full['Datetime'], utc=True)  # tz-aware UTC for storage\n",
        "    dt_ny = full['Datetime'].dt.tz_convert('America/New_York')\n",
        "    rth_mask = (\n",
        "        (dt_ny.dt.weekday < 5) &\n",
        "        (dt_ny.dt.time >= pd.to_datetime(\"09:30\").time()) &\n",
        "        (dt_ny.dt.time <  pd.to_datetime(\"16:00\").time())\n",
        "    )\n",
        "    full = full[rth_mask].reset_index(drop=True)\n",
        "\n",
        "    # ---- Save atomically: local + both Drive locations ----\n",
        "    save_csv_atomically(full, FEATURE_CSV_LOCAL)\n",
        "    save_csv_atomically(full, FEATURE_CSV_RESULTS)\n",
        "    save_csv_atomically(full, FEATURE_CSV_TRADING)\n",
        "\n",
        "    # Parquet mirror (local)\n",
        "    full.to_parquet(PARQ_FULL, index=False)\n",
        "\n",
        "    # Diagnostics: existence + size + small preview\n",
        "    for p in (FEATURE_CSV_LOCAL, FEATURE_CSV_RESULTS, FEATURE_CSV_TRADING):\n",
        "        _log_artifact(p)\n",
        "\n",
        "    log.info(\"Saved combined CSV (rows=%d) →\\n- %s\\n- %s\\n- %s\",\n",
        "             len(full), FEATURE_CSV_LOCAL, FEATURE_CSV_RESULTS, FEATURE_CSV_TRADING)\n",
        "    return full\n",
        "\n",
        "# ---------- Optional: utility used by training scripts ----------\n",
        "def find_or_build_dataset():\n",
        "    \"\"\"Try local/Drive mirrors; if none, build and save to all mirrors.\"\"\"\n",
        "    for cand in CANDIDATE_CSVS:\n",
        "        if _verify_csv_ok(cand):\n",
        "            df = pd.read_csv(cand)\n",
        "            df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], utc=True)\n",
        "            return cand, df\n",
        "    df = build_features()\n",
        "    save_csv_atomically(df, FEATURE_CSV_LOCAL)\n",
        "    save_csv_atomically(df, FEATURE_CSV_RESULTS)\n",
        "    save_csv_atomically(df, FEATURE_CSV_TRADING)\n",
        "    return FEATURE_CSV_LOCAL, df\n",
        "\n",
        "# ---------- Run end-to-end ----------\n",
        "if __name__ == \"__main__\":\n",
        "    df_full = build_features()\n",
        "\n",
        "    # (Optional) Create quick time-based train/val splits locally for inspection\n",
        "    try:\n",
        "        df_full = df_full.sort_values('Datetime').reset_index(drop=True)\n",
        "        cutoff_idx = int((1.0 - VAL_FRACTION) * len(df_full))\n",
        "        cutoff_idx = min(max(1, cutoff_idx), len(df_full) - 1)  # guardrails\n",
        "        cutoff_time = df_full.loc[cutoff_idx, 'Datetime']\n",
        "        train_df = df_full[df_full['Datetime'] <  cutoff_time].reset_index(drop=True)\n",
        "        val_df   = df_full[df_full['Datetime'] >= cutoff_time].reset_index(drop=True)\n",
        "\n",
        "        # Atomic saves everywhere (local + Drive mirror for convenience)\n",
        "        save_csv_atomically(df_full,  FEATURE_CSV_LOCAL)   # ensure latest combined locally\n",
        "        save_csv_atomically(train_df, \"train.csv\")\n",
        "        save_csv_atomically(val_df,   \"val.csv\")\n",
        "\n",
        "        # Drive mirror of splits\n",
        "        save_csv_atomically(train_df, os.path.join(TRADING_DIR, \"train.csv\"))\n",
        "        save_csv_atomically(val_df,   os.path.join(TRADING_DIR, \"val.csv\"))\n",
        "\n",
        "        # Parquet mirrors\n",
        "        train_df.to_parquet(PARQ_TRAIN, index=False)\n",
        "        val_df.to_parquet(PARQ_VAL, index=False)\n",
        "\n",
        "        log.info(\"Time split cutoff @ %s\", cutoff_time)\n",
        "        log.info(\"Train: %s, Val: %s\", train_df.shape, val_df.shape)\n",
        "\n",
        "        for p in (\"train.csv\", \"val.csv\", PARQ_TRAIN, PARQ_VAL):\n",
        "            exists = os.path.exists(p)\n",
        "            sz = os.path.getsize(p) if exists else 0\n",
        "            log.info(\"Saved split artifact: exists=%s | size=%s | path=%s\", exists, sz, p)\n",
        "\n",
        "        # Light summary\n",
        "        try:\n",
        "            sym_counts = df_full['Symbol'].value_counts()\n",
        "            log.info(\"Symbols saved (top 10):\\n%s\", sym_counts.head(10).to_string())\n",
        "            log.info(\"Datetime range: %s → %s\", df_full['Datetime'].min(), df_full['Datetime'].max())\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Log artifacts\n",
        "        for p in (FEATURE_CSV_LOCAL,\n",
        "                  os.path.join(TRADING_DIR, \"train.csv\"),\n",
        "                  os.path.join(TRADING_DIR, \"val.csv\")):\n",
        "            _log_artifact(p)\n",
        "\n",
        "        # Cleanup refs\n",
        "        del train_df, val_df\n",
        "    except Exception as e:\n",
        "        log.warning(\"Train/Val split skipped: %s\", e)\n",
        "\n",
        "    gc.collect()\n",
        "    log.info(\"Download + Feature Build complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjUBMefkD5C3"
      },
      "outputs": [],
      "source": [
        "import os, time, pandas as pd\n",
        "for p in [\n",
        "    \"multi_stock_feature_engineered_dataset.csv\",\n",
        "    \"/content/drive/MyDrive/Results_May_2025/results_sac_walkforward/multi_stock_feature_engineered_dataset.csv\",\n",
        "    \"/content/drive/MyDrive/trading_data/multi_stock_feature_engineered_dataset.csv\",\n",
        "]:\n",
        "    print(p, \"->\", os.path.exists(p))\n",
        "    if os.path.exists(p):\n",
        "        print(\"  size:\", os.path.getsize(p), \"modified:\", time.ctime(os.path.getmtime(p)))\n",
        "        print(\"  cols:\", list(pd.read_csv(p, nrows=1).columns)[:8])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V05et-uD7EN"
      },
      "outputs": [],
      "source": [
        "import os, sys, random, logging\n",
        "\n",
        "# Modes\n",
        "TEST_MODE = os.getenv(\"TEST_MODE\", \"1\").lower() not in (\"0\", \"false\", \"no\")\n",
        "FAST_TEST = os.getenv(\"FAST_TEST\", \"1\").lower() in (\"1\", \"true\", \"yes\")\n",
        "RELAX_CPU_THREADS = TEST_MODE and os.getenv(\"RELAX_CPU_THREADS\", \"0\").lower() in (\"1\", \"true\", \"yes\")\n",
        "\n",
        "# Determinism knobs (must be set before importing numpy/torch)\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"  # cuBLAS determinism (Ampere+)\n",
        "\n",
        "# CPU threading policy (assign so we truly override)\n",
        "_thr = str(min(4, (os.cpu_count() or 4)))\n",
        "for k in (\"OMP_NUM_THREADS\", \"MKL_NUM_THREADS\", \"OPENBLAS_NUM_THREADS\", \"NUMEXPR_NUM_THREADS\"):\n",
        "    os.environ[k] = _thr if RELAX_CPU_THREADS else \"1\"\n",
        "\n",
        "# Logging\n",
        "LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\n",
        "logging.basicConfig(\n",
        "    level=getattr(logging, LOG_LEVEL, logging.INFO),\n",
        "    format=\":%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler(\"sac_run.log\", mode=\"a\")],\n",
        ")\n",
        "log = logging.getLogger(\"SAC-TrainExec\")\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Torch thread caps (respect env above)\n",
        "try:\n",
        "    torch.set_num_interop_threads(1)\n",
        "    torch.set_num_threads(int(os.environ.get(\"OMP_NUM_THREADS\", \"1\")))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = False  # avoid non-deterministic TF32\n",
        "    torch.backends.cudnn.allow_tf32 = False\n",
        "\n",
        "# Deterministic backends\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "try:\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "except Exception as e:\n",
        "    # Some ops/hardware combos may not support full determinism; continue safely.\n",
        "    log.warning(\"Could not force full deterministic algorithms: %s\", e)\n",
        "\n",
        "# ---- libs (no yfinance / no feature-building here) ----\n",
        "import gc, time, json, re, warnings, heapq, glob\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from shutil import copyfile\n",
        "from math import isfinite\n",
        "from collections import defaultdict\n",
        "\n",
        "# gymnasium + SB3\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import (\n",
        "    EvalCallback, StopTrainingOnNoModelImprovement, CheckpointCallback, CallbackList, BaseCallback\n",
        ")\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Gym has been unmaintained.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"jupyter_client.session\")\n",
        "set_random_seed(SEED)\n",
        "\n",
        "# ========= Paths =========\n",
        "DRIVE_BASE  = os.getenv(\"DRIVE_BASE\", \"/content/drive/MyDrive\")  # no auto-mount here\n",
        "BASE_DIR    = \"./sac_parity\"\n",
        "RESULTS_DIR = os.path.join(DRIVE_BASE, \"Results_May_2025\", \"results_sac_walkforward\")\n",
        "SAVE_DIR    = os.path.join(RESULTS_DIR, \"models_sac\")\n",
        "for p in [BASE_DIR, RESULTS_DIR, SAVE_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"RESULTS_DIR =\", RESULTS_DIR)\n",
        "print(\"Exists?\", os.path.exists(RESULTS_DIR))\n",
        "print(\"Listdir sample:\", os.listdir(RESULTS_DIR)[:10] if os.path.exists(RESULTS_DIR) else \"N/A\")\n",
        "\n",
        "csvs = sorted(glob.glob(os.path.join(RESULTS_DIR, \"*_sac_ws_sweep_summary.csv\")))\n",
        "print(\"Matched:\", [os.path.basename(p) for p in csvs])\n",
        "\n",
        "for p in csvs:\n",
        "    df = pd.read_csv(p)\n",
        "    print(os.path.basename(p), \"rows:\", len(df))\n",
        "\n",
        "for sub in [\"scalers\", \"features\", \"signals\", \"plots\", \"vecnorms\", \"tmp\", \"model_info\", \"best\"]:\n",
        "    os.makedirs(os.path.join(RESULTS_DIR, sub), exist_ok=True)\n",
        "\n",
        "# Canonical CSV locations (must already exist, built by data-prep script)\n",
        "FEATURE_CSV_RESULTS = os.path.join(RESULTS_DIR, \"multi_stock_feature_engineered_dataset.csv\")\n",
        "FEATURE_CSV_DRIVE   = os.path.join(DRIVE_BASE, \"trading_data\", \"multi_stock_feature_engineered_dataset.csv\")\n",
        "FEATURE_CSV_LOCAL   = \"multi_stock_feature_engineered_dataset.csv\"\n",
        "\n",
        "# ========= Data: strict read-only load =========\n",
        "CANDIDATES_DATA = [FEATURE_CSV_LOCAL, FEATURE_CSV_RESULTS, FEATURE_CSV_DRIVE]\n",
        "DATA_PATH = next((p for p in CANDIDATES_DATA if os.path.exists(p)), None)\n",
        "\n",
        "if DATA_PATH is None:\n",
        "    log.warning(\"Dataset CSV not found in %s; attempting to build it now...\", CANDIDATES_DATA)\n",
        "    build_features = None\n",
        "\n",
        "    # Try to import from your download/feature-build module if it's a separate file.\n",
        "    try:\n",
        "        from download_fe import build_features as _bf, FEATURE_CSV_LOCAL as _F_LOCAL\n",
        "        build_features = _bf\n",
        "        FEATURE_CSV_LOCAL = _F_LOCAL  # keep paths consistent with the builder, if different\n",
        "        log.info(\"Imported build_features() from download_fe.py\")\n",
        "    except Exception:\n",
        "        # Fall back to a global if you executed the combined, single-script version in the same runtime.\n",
        "        build_features = globals().get(\"build_features\", None)\n",
        "        if build_features:\n",
        "            log.info(\"Using build_features() found in current runtime (combined script).\")\n",
        "\n",
        "    if build_features is None:\n",
        "        raise FileNotFoundError(\n",
        "            \"Prebuilt dataset CSV not found and build_features() is unavailable.\\n\"\n",
        "            \"→ Run the Download + Feature Build script first OR ensure `from download_fe import build_features` works.\"\n",
        "        )\n",
        "\n",
        "    built_df = build_features()\n",
        "    built_df.to_csv(FEATURE_CSV_LOCAL, index=False)\n",
        "    DATA_PATH = FEATURE_CSV_LOCAL\n",
        "    log.info(\"Rebuilt dataset → %s\", DATA_PATH)\n",
        "\n",
        "log.info(\"Using dataset: %s\", DATA_PATH)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "if \"Datetime\" not in df.columns or \"Symbol\" not in df.columns or \"Close\" not in df.columns:\n",
        "    raise ValueError(\"Dataset must contain at least ['Datetime','Symbol','Close'] plus numeric features.\")\n",
        "\n",
        "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], utc=True)\n",
        "\n",
        "# Identify numeric feature columns (observations exclude Symbol/Datetime/Close)\n",
        "feature_cols = [\n",
        "    c for c in df.columns\n",
        "    if c not in [\"Symbol\", \"Datetime\", \"Close\"] and pd.api.types.is_numeric_dtype(df[c])\n",
        "]\n",
        "if not feature_cols:\n",
        "    raise ValueError(\"No numeric feature columns found for observations.\")\n",
        "log.info(\"Feature columns detected: %d\", len(feature_cols))\n",
        "\n",
        "# ========= SAC / Env toggles =========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "policy_kwargs = dict(net_arch=[256, 256])\n",
        "\n",
        "\n",
        "LONG_ONLY      = True  # set False to allow shorts again\n",
        "ENABLE_PLOTS   = False\n",
        "ENABLE_SLO     = True     # SL/TP + cooldown in env\n",
        "LIVE_MODE      = False    # (no live helpers in this script)\n",
        "SIM_LATENCY_MS = 0\n",
        "BROKER         = \"log\"\n",
        "USE_REGIME_TRAIN = False  # (no feature-building here—left for compatibility flags)\n",
        "USE_SENTIMENT_TRAIN = False\n",
        "ENABLE_WAVELET_TRAIN = False\n",
        "\n",
        "# Risk controls & hygiene\n",
        "DEAD_BAND        = 0.01\n",
        "MIN_TRADE_DELTA  = 0.05\n",
        "STOP_LOSS_PCT    = 0.04\n",
        "TAKE_PROFIT_PCT  = 0.08\n",
        "COOLDOWN_STEPS   = 6\n",
        "\n",
        "# Reward shaping\n",
        "ENABLE_WHIPSAW_PENALTY   = True\n",
        "WHIPSAW_PENALTY          = 1e-4\n",
        "ENABLE_COOLDOWN_PENALTY  = True\n",
        "COOLDOWN_STEP_PENALTY    = 0.0\n",
        "CARRY_WEIGHT             = 0.00  # 0.02–0.05\n",
        "IDLE_PENALTY             = 0.0\n",
        "\n",
        "MAX_EXPOSURE    = 1.0\n",
        "COMMISSION_BPS  = 0.5\n",
        "SLIPPAGE_BPS    = 1.0\n",
        "INITIAL_CAPITAL = 100_000.0\n",
        "\n",
        "# Window sweep / schedule\n",
        "CANDIDATE_WS   = [12, 16, 24]\n",
        "TOP_N_WINDOWS  = 1 if TEST_MODE else 3\n",
        "FORCE_RETRAIN  = os.getenv(\"FORCE_RETRAIN\", \"0\").lower() in (\"1\",\"true\",\"yes\")\n",
        "\n",
        "\n",
        "LOG_TRADES_TRAIN  = False\n",
        "LOG_TRADES_EVALCB = False\n",
        "LOG_TRADES_FINAL  = False\n",
        "\n",
        "# Checkpointing\n",
        "CKPT_FREQ_STEPS = int(os.getenv(\"CKPT_FREQ_STEPS\", \"50000\"))  # save every N env steps\n",
        "CKPT_DIR_ROOT   = os.path.join(RESULTS_DIR, \"ckpts\")\n",
        "os.makedirs(CKPT_DIR_ROOT, exist_ok=True)\n",
        "\n",
        "\n",
        "# Base train steps (adjusted below for TEST_MODE/FAST_TEST)\n",
        "TRAIN_TOTAL_STEPS = 50_000\n",
        "MIN_TRAIN_STEPS = 1000\n",
        "\n",
        "FAST = dict(\n",
        "    learning_rate=3e-4, batch_size=256, train_freq=1, gradient_steps=2,\n",
        "    gamma=0.995, tau=0.005, target_update_interval=1, ent_coef=\"auto\",\n",
        "    buffer_size=int(1e6), learning_starts=2_000,\n",
        ")\n",
        "SLOW = dict(\n",
        "    learning_rate=1e-4, batch_size=512, train_freq=4, gradient_steps=2,\n",
        "    gamma=0.997, tau=0.005, target_update_interval=2, ent_coef=\"auto\",\n",
        "    buffer_size=int(2e6), learning_starts=4_000,\n",
        ")\n",
        "\n",
        "# === Modes / flags ===\n",
        "# Base from env\n",
        "USE_EVAL_CALLBACK = os.getenv(\"USE_EVAL_CALLBACK\", \"0\").lower() in (\"1\", \"true\", \"yes\")\n",
        "\n",
        "# Precedence: FAST_TEST forces ON, TEST_MODE forces OFF, otherwise env decides\n",
        "if FAST_TEST:\n",
        "    USE_EVAL_CALLBACK = True\n",
        "elif TEST_MODE:\n",
        "    USE_EVAL_CALLBACK = False\n",
        "\n",
        "# Optional: quick sanity print\n",
        "log.info(f\"Flags → TEST_MODE={TEST_MODE} | FAST_TEST={FAST_TEST} | USE_EVAL_CALLBACK={USE_EVAL_CALLBACK}\")\n",
        "\n",
        "if FAST_TEST:\n",
        "    CANDIDATE_WS  = [10]\n",
        "    TOP_N_WINDOWS = 1\n",
        "    MIN_TRAIN_STEPS = 1000\n",
        "    FAST[\"buffer_size\"] = 50_000; FAST[\"batch_size\"] = 128\n",
        "    SLOW[\"buffer_size\"] = 50_000; SLOW[\"batch_size\"] = 128\n",
        "    LOG_TRADES_FINAL = False\n",
        "\n",
        "# ========= Universe bookkeeping =========\n",
        "symbols_all = sorted({str(s).upper() for s in df[\"Symbol\"].unique()})\n",
        "fast_names = {\n",
        "    \"TSLA\",\"NVDA\",\"AMD\",\"AVGO\",\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"ADBE\",\"CRM\",\n",
        "    \"INTC\",\"QCOM\",\"TXN\",\"ORCL\",\"NEE\",\"GE\",\"XOM\",\"CVX\",\"LLY\",\"NKE\",\"SBUX\"\n",
        "}\n",
        "slow_names = {\n",
        "    \"BRK-B\",\"JPM\",\"BAC\",\"JNJ\",\"UNH\",\"MRK\",\"PFE\",\"ABBV\",\"ABT\",\"AMGN\",\"PG\",\"PEP\",\"KO\",\n",
        "    \"V\",\"MA\",\"WMT\",\"MCD\",\"TMO\",\"DHR\",\"ACN\",\"IBM\",\"LIN\",\"PM\",\"RTX\",\"UPS\",\"UNP\",\"COST\",\"HD\",\"LOW\"\n",
        "}\n",
        "def get_sac_cfg(symbol: str) -> dict:\n",
        "    if symbol in fast_names:\n",
        "        bucket = \"FAST\"; base = FAST\n",
        "    elif symbol in slow_names:\n",
        "        bucket = \"SLOW\"; base = SLOW\n",
        "    else:\n",
        "        bucket = \"DEFAULT\"; base = FAST\n",
        "    log.info(\"Config bucket for %s -> %s\", symbol, bucket)\n",
        "    return base | {\"_bucket\": bucket}\n",
        "\n",
        "# Detect already-trained models to resume/skip\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "ws_by_symbol = defaultdict(set)\n",
        "for f in os.listdir(SAVE_DIR):\n",
        "    m = re.match(r\"^sac_([A-Za-z0-9\\-]+)_ws(\\d+)\\.zip$\", f, re.IGNORECASE)\n",
        "    if m:\n",
        "        ws_by_symbol[m.group(1).upper()].add(int(m.group(2)))\n",
        "\n",
        "completed_symbols = sorted([s for s, wsset in ws_by_symbol.items()\n",
        "                            if set(CANDIDATE_WS).issubset(wsset)])\n",
        "remaining_symbols = [s for s in symbols_all if s not in completed_symbols]\n",
        "log.info(\"Completed: %d | Remaining: %d\", len(completed_symbols), len(remaining_symbols))\n",
        "\n",
        "# ======= Run-list selection (manual/env/TEST_MODE) =======\n",
        "# 1) Manual short list for quick tests (leave [] to disable manual override)\n",
        "RUNLIST_MANUAL: list[str] = ['AAPL', 'NVDA', 'MSFT']\n",
        "\n",
        "# 2) From environment (comma-separated), e.g.:\n",
        "#    export RUN_SYMBOLS=\"TSLA,NVDA\"\n",
        "_env = os.getenv(\"RUN_SYMBOLS\", \"\").strip()\n",
        "RUNLIST_ENV = [s.strip().upper() for s in _env.split(\",\") if s.strip()] if _env else []\n",
        "\n",
        "# 3) TEST_MODE: keep first N remaining symbols if nothing else specified\n",
        "TEST_PICK_N = int(os.getenv(\"TEST_PICK_N\", \"3\"))\n",
        "\n",
        "# Only (re)train symbols that aren't already completed\n",
        "# (computed earlier from SAVE_DIR)\n",
        "universe = remaining_symbols  # <- keep this line as in your script\n",
        "\n",
        "def _restrict_to_universe(cands: list[str]) -> list[str]:\n",
        "    uni = {s.upper(): s for s in universe}\n",
        "    return [uni[s.upper()] for s in cands if s.upper() in uni]\n",
        "\n",
        "if RUNLIST_MANUAL:\n",
        "    run_symbols = _restrict_to_universe(RUNLIST_MANUAL); why = \"manual override\"\n",
        "elif RUNLIST_ENV:\n",
        "    run_symbols = _restrict_to_universe(RUNLIST_ENV);    why = \"RUN_SYMBOLS env\"\n",
        "elif TEST_MODE:\n",
        "    run_symbols = universe[:TEST_PICK_N];                 why = f\"TEST_MODE first {TEST_PICK_N}\"\n",
        "else:\n",
        "    run_symbols = universe;                               why = \"all remaining\"\n",
        "\n",
        "if not run_symbols and universe:\n",
        "    run_symbols = universe[:1]; why += \" (fallback to first remaining)\"\n",
        "\n",
        "log.info(\"SAC run plan → %s | count=%d | %s\", run_symbols, len(run_symbols), why)\n",
        "\n",
        "# ========= Inference config artifact (optional) =========\n",
        "inference_cfg_path = os.path.join(RESULTS_DIR, \"inference_config.json\")\n",
        "if not os.path.exists(inference_cfg_path):\n",
        "    with open(inference_cfg_path, \"w\") as f:\n",
        "        json.dump(\n",
        "            {\n",
        "                \"algo\": \"SAC\",\n",
        "                \"action_mode\": \"deterministic\",\n",
        "                \"conf_source\": \"none\",\n",
        "                \"conf_thresh\": None,\n",
        "                \"n_eval_runs\": 1,\n",
        "                \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
        "            },\n",
        "            f, indent=2\n",
        "        )\n",
        "    log.info(\"Wrote inference_config.json -> %s\", inference_cfg_path)\n",
        "\n",
        "# ========= Env (continuous exposure with costs + risk controls) =========\n",
        "class ContinuousTradingEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Continuous exposure env with costs + risk controls.\n",
        "      - Action a in [-1,1] -> exposure in [-MAX_EXPOSURE, MAX_EXPOSURE]\n",
        "      - Deadband around zero, min trade delta\n",
        "      - Commission + slippage on position change notional\n",
        "      - Stop-loss/Take-profit + sign-flip guard + cooldown\n",
        "    \"\"\"\n",
        "    metadata = {\"render_modes\": []}\n",
        "\n",
        "    def __init__(\n",
        "        self, df: pd.DataFrame, frame_bound: tuple, window_size: int,\n",
        "        max_exposure: float = MAX_EXPOSURE,\n",
        "        commission_bps: float = COMMISSION_BPS,\n",
        "        slippage_bps: float = SLIPPAGE_BPS,\n",
        "        initial_capital: float = INITIAL_CAPITAL,\n",
        "        log_trades: bool = True,\n",
        "        dead_band: float = 0.0,\n",
        "        min_trade_delta: float = 0.0,\n",
        "        stop_loss_pct: float | None = None,\n",
        "        take_profit_pct: float | None = None,\n",
        "        cooldown_steps: int = 0,                 # <- int, not float\n",
        "        whipsaw_penalty: float = 0.0,\n",
        "        cooldown_step_penalty: float = 0.0,\n",
        "        long_only: bool = False                  # <- single declaration\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert \"Close\" in df.columns and \"Datetime\" in df.columns, \"DataFrame must have Close & Datetime\"\n",
        "        self.raw_df = df.sort_values(\"Datetime\").reset_index(drop=True).copy()\n",
        "        self.window_size = int(window_size)\n",
        "        self.start_tick, self.end_tick = int(frame_bound[0]), int(frame_bound[1])\n",
        "        assert self.end_tick <= len(self.raw_df), \"frame_bound[1] exceeds data length\"\n",
        "        assert self.start_tick >= self.window_size, \"start tick must be >= window_size\"\n",
        "\n",
        "        self.prices = self.raw_df[\"Close\"].values.astype(np.float64)\n",
        "        feats = self.raw_df.drop(columns=[\"Symbol\", \"Datetime\", \"Close\"], errors=\"ignore\")\n",
        "        feats = feats.select_dtypes(include=[np.number]).astype(np.float32).values\n",
        "        self.features = feats\n",
        "        n_feat = self.features.shape[1]\n",
        "\n",
        "        self.action_space = Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(self.window_size, n_feat), dtype=np.float32)\n",
        "\n",
        "        self.max_exposure = float(max_exposure)\n",
        "        self.bps_total = (float(commission_bps) + float(slippage_bps)) / 10_000.0\n",
        "        self.initial_capital = float(initial_capital)\n",
        "\n",
        "        self.dead_band = float(dead_band)\n",
        "        self.min_trade_delta = float(min_trade_delta)\n",
        "        self.stop_loss_pct = float(stop_loss_pct) if stop_loss_pct is not None else None\n",
        "        self.take_profit_pct = float(take_profit_pct) if take_profit_pct is not None else None\n",
        "        self.cooldown_steps = int(cooldown_steps)\n",
        "\n",
        "        self.whipsaw_penalty = float(whipsaw_penalty)\n",
        "        self.cooldown_step_penalty = float(cooldown_step_penalty)\n",
        "        self.log_trades = bool(log_trades)\n",
        "        self.log_every = 500 if TEST_MODE else 2000\n",
        "        self.long_only = bool(long_only)         # <- keep only here\n",
        "\n",
        "        self.current_tick = None\n",
        "        self.position = None\n",
        "        self.portfolio_value = None\n",
        "        self.entry_price = None\n",
        "        self.cooldown_until = -1\n",
        "        self.done_tick = self.end_tick - 1\n",
        "\n",
        "    def _obs(self):\n",
        "        start = self.current_tick - self.window_size\n",
        "        obs = self.features[start:self.current_tick]\n",
        "        return obs.astype(np.float32)\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_tick = max(self.start_tick, self.window_size)\n",
        "        self.position = 0.0\n",
        "        self.portfolio_value = self.initial_capital\n",
        "        self.entry_price = None\n",
        "        self.cooldown_until = -1\n",
        "        obs = self._obs()\n",
        "        info = {\n",
        "            \"portfolio_value\": float(self.portfolio_value),\n",
        "            \"position\": float(self.position),\n",
        "            \"price\": float(self.prices[self.current_tick]),\n",
        "            \"trade_cost\": 0.0,\n",
        "            \"risk_event\": None,\n",
        "            \"cooldown_left\": 0,\n",
        "            \"entry_price\": None,\n",
        "        }\n",
        "        return obs, info\n",
        "\n",
        "    def _pnl_from_entry(self, price_now: float) -> float:\n",
        "        if self.entry_price is None or self.entry_price <= 0 or self.position == 0.0:\n",
        "            return 0.0\n",
        "        side = 1.0 if self.position > 0 else -1.0\n",
        "        return side * (price_now / self.entry_price - 1.0)\n",
        "\n",
        "    def _apply_risk_controls(self, price_t: float, proposed_pos: float):\n",
        "        event = None\n",
        "        cooldown_left = max(0, self.cooldown_until - self.current_tick)\n",
        "\n",
        "        if self.position == 0.0 and self.current_tick < self.cooldown_until:\n",
        "            return 0.0, \"cooldown\", cooldown_left\n",
        "\n",
        "        if (\n",
        "            self.cooldown_steps > 0 and\n",
        "            self.position != 0.0 and\n",
        "            np.sign(proposed_pos) != np.sign(self.position) and\n",
        "            abs(proposed_pos) >= self.min_trade_delta\n",
        "        ):\n",
        "            self.cooldown_until = self.current_tick + self.cooldown_steps\n",
        "            return 0.0, \"flip_block\", self.cooldown_steps\n",
        "\n",
        "        if self.position != 0.0:\n",
        "            pnl_entry = self._pnl_from_entry(price_t)\n",
        "            if (self.stop_loss_pct is not None) and (pnl_entry <= -self.stop_loss_pct):\n",
        "                self.cooldown_until = self.current_tick + self.cooldown_steps\n",
        "                return 0.0, \"stop_loss\", self.cooldown_steps\n",
        "            if (self.take_profit_pct is not None) and (pnl_entry >= self.take_profit_pct):\n",
        "                self.cooldown_until = self.current_tick + self.cooldown_steps\n",
        "                return 0.0, \"take_profit\", self.cooldown_steps\n",
        "\n",
        "        return proposed_pos, event, cooldown_left\n",
        "\n",
        "    def step(self, action):\n",
        "        if SIM_LATENCY_MS and SIM_LATENCY_MS > 0:\n",
        "            try:\n",
        "                time.sleep(float(SIM_LATENCY_MS) / 1000.0)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        a = float(np.array(action).reshape(-1)[0])\n",
        "        if abs(a) < self.dead_band:\n",
        "            a = 0.0\n",
        "\n",
        "        if self.long_only:\n",
        "            a = max(0.0, a)  # disallow shorts\n",
        "            target_pos = float(np.clip(a, 0.0, 1.0)) * self.max_exposure\n",
        "        else:\n",
        "            target_pos = float(np.clip(a, -1.0, 1.0)) * self.max_exposure\n",
        "        target_pos = float(np.clip(target_pos, -self.max_exposure, self.max_exposure))\n",
        "\n",
        "        if abs(target_pos - (0.0 if self.position is None else self.position)) < self.min_trade_delta:\n",
        "            target_pos = self.position\n",
        "\n",
        "        price_t   = float(self.prices[self.current_tick])\n",
        "        old_pos   = float(0.0 if self.position is None else self.position)\n",
        "\n",
        "        target_pos, risk_event, cooldown_left = self._apply_risk_controls(price_t, target_pos)\n",
        "\n",
        "        base_cash = self.portfolio_value if self.portfolio_value else self.initial_capital\n",
        "        trade_notional = abs(target_pos - old_pos) * base_cash\n",
        "        trade_cost = self.bps_total * trade_notional\n",
        "\n",
        "        self.position = float(target_pos)\n",
        "        self.portfolio_value = float(base_cash - trade_cost)\n",
        "        if not np.isfinite(self.portfolio_value) or self.portfolio_value <= 0.0:\n",
        "            self.portfolio_value = max(1e-8, float(self.portfolio_value))\n",
        "\n",
        "        next_tick = self.current_tick + 1\n",
        "        terminated = next_tick >= len(self.prices) or self.current_tick >= self.done_tick\n",
        "        price_tp1 = float(self.prices[min(next_tick, len(self.prices) - 1)])\n",
        "        ret = 0.0 if price_t <= 0 else (price_tp1 / price_t - 1.0)\n",
        "        v_prev = self.portfolio_value\n",
        "        self.portfolio_value = v_prev * (1.0 + self.position * ret)\n",
        "\n",
        "        # --- carry term: reward scales with ret and long size (no ret>0 gate)\n",
        "        reward = (self.portfolio_value - v_prev) / v_prev if v_prev > 0 else 0.0\n",
        "        long_frac = max(self.position, 0.0) / max(self.max_exposure, 1e-9)\n",
        "        baseline  = CARRY_WEIGHT * ret\n",
        "        reward   += baseline * long_frac\n",
        "\n",
        "        penalty = 0.0\n",
        "        penalty += 1e-4 * abs(target_pos - old_pos)\n",
        "        whipsaw_applied = False\n",
        "        cooldown_applied = False\n",
        "        if self.current_tick >= self.cooldown_until:\n",
        "            cash_frac = 1.0 - min(abs(self.position) / max(self.max_exposure, 1e-9), 1.0)\n",
        "            penalty += IDLE_PENALTY * cash_frac\n",
        "        long_frac = max(self.position, 0.0) / max(self.max_exposure, 1e-9)\n",
        "        floor_target, floor_w = 0.20, 0.0  # try target 0.2–0.3, weight 1e-4 → 3e-4\n",
        "        penalty += floor_w * max(0.0, floor_target - long_frac)\n",
        "        if self.whipsaw_penalty > 0.0:\n",
        "            if (old_pos != 0.0 and self.position != 0.0 and np.sign(old_pos) != np.sign(self.position)):\n",
        "                penalty += self.whipsaw_penalty\n",
        "                whipsaw_applied = True\n",
        "            if risk_event == \"flip_block\":\n",
        "                penalty += self.whipsaw_penalty\n",
        "                whipsaw_applied = True\n",
        "        if self.cooldown_step_penalty > 0.0 and self.current_tick < self.cooldown_until:\n",
        "            penalty += self.cooldown_step_penalty\n",
        "            cooldown_applied = True\n",
        "\n",
        "        reward = float(np.clip(reward - penalty, -1.0, 1.0))\n",
        "\n",
        "        if old_pos == 0.0 and self.position != 0.0:\n",
        "            self.entry_price = price_t\n",
        "        if self.position == 0.0:\n",
        "            self.entry_price = None\n",
        "\n",
        "        if self.log_trades and (abs(target_pos - old_pos) > 1e-9 or risk_event):\n",
        "            log.info(\n",
        "                \"t=%d | px=%.4f | pos %.3f→%.3f | notional=%.2f | cost=%.2f bps=%.2f | risk=%s | cd=%d | V=%.2f\",\n",
        "                self.current_tick, price_t, old_pos, self.position,\n",
        "                trade_notional, trade_cost, self.bps_total * 10_000.0,\n",
        "                (risk_event or \"none\"), int(cooldown_left), self.portfolio_value\n",
        "            )\n",
        "\n",
        "        self.current_tick = next_tick\n",
        "        obs = self._obs()\n",
        "        info = {\n",
        "            \"portfolio_value\": float(self.portfolio_value),\n",
        "            \"position\": float(self.position),\n",
        "            \"price\": float(price_tp1),\n",
        "            \"trade_cost\": float(trade_cost),\n",
        "            \"risk_event\": risk_event,\n",
        "            \"cooldown_left\": int(max(0, self.cooldown_until - self.current_tick)) if self.cooldown_until >= 0 else 0,\n",
        "            \"entry_price\": None if self.entry_price is None else float(self.entry_price),\n",
        "            \"whipsaw_penalty_applied\": bool(whipsaw_applied),\n",
        "            \"cooldown_penalty_applied\": bool(cooldown_applied),\n",
        "            \"broker\": str(BROKER),\n",
        "        }\n",
        "        truncated = False\n",
        "        return obs.astype(np.float32), reward, bool(terminated), bool(truncated), info\n",
        "\n",
        "def _risk_kwargs():\n",
        "    return dict(\n",
        "        dead_band=DEAD_BAND,\n",
        "        min_trade_delta=MIN_TRADE_DELTA,\n",
        "        stop_loss_pct=(STOP_LOSS_PCT if ENABLE_SLO else None),\n",
        "        take_profit_pct=(TAKE_PROFIT_PCT if ENABLE_SLO else None),\n",
        "        cooldown_steps=(COOLDOWN_STEPS if ENABLE_SLO else 0),\n",
        "    )\n",
        "\n",
        "def _reward_kwargs():\n",
        "    return dict(\n",
        "        whipsaw_penalty=(WHIPSAW_PENALTY if ENABLE_WHIPSAW_PENALTY else 0.0),\n",
        "        cooldown_step_penalty=(COOLDOWN_STEP_PENALTY if ENABLE_COOLDOWN_PENALTY else 0.0),\n",
        "    )\n",
        "\n",
        "# ====== Helpers ======\n",
        "def _nan_if_none(x):\n",
        "    if x is None:\n",
        "        return float(\"nan\")\n",
        "    try:\n",
        "        xf = float(x)\n",
        "    except (TypeError, ValueError):\n",
        "        return float(\"nan\")\n",
        "    return xf if np.isfinite(xf) else float(\"nan\")\n",
        "\n",
        "def _safe_round(x, ndigits):\n",
        "    x = _nan_if_none(x)\n",
        "    return round(x, ndigits) if np.isfinite(x) else float(\"nan\")\n",
        "\n",
        "# ---- Callback to periodically save VecNormalize stats during training ----\n",
        "class SaveVecNormCallback(BaseCallback):\n",
        "    def __init__(self, vec_env, save_path: str, save_freq: int = 50_000, verbose: int = 0):\n",
        "        super().__init__(verbose)\n",
        "        self.vec_env = vec_env\n",
        "        self.save_path = save_path\n",
        "        self.save_freq = int(save_freq)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.num_timesteps % self.save_freq == 0:\n",
        "            try:\n",
        "                self.vec_env.save(self.save_path)\n",
        "                if self.verbose:\n",
        "                    print(f\"[SaveVecNorm] saved -> {self.save_path}\")\n",
        "            except Exception as e:\n",
        "                if self.verbose:\n",
        "                    print(f\"[SaveVecNorm] failed: {e}\")\n",
        "        return True\n",
        "\n",
        "class HeartbeatCallback(BaseCallback):\n",
        "    def __init__(self, every: int = 10_000, target_steps: int | None = None, py_logger=None):\n",
        "        super().__init__()\n",
        "        self.every = int(every)\n",
        "        self.target_steps = target_steps\n",
        "        self._pylog = py_logger or logging.getLogger(__name__)\n",
        "        self._t0 = None\n",
        "        self._last_t = None\n",
        "        self._last_step = 0\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        self._t0 = time.time()\n",
        "        self._last_t = self._t0\n",
        "        self._last_step = self.num_timesteps\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if (self.num_timesteps - self._last_step) >= self.every:\n",
        "            now = time.time()\n",
        "            dt = max(now - self._last_t, 1e-9)\n",
        "            total_dt = max(now - self._t0, 1e-9)\n",
        "            steps = self.num_timesteps\n",
        "            inst_sps = (steps - self._last_step) / dt\n",
        "            avg_sps = steps / total_dt\n",
        "            eta_str = \"\"\n",
        "            if self.target_steps is not None and inst_sps > 0:\n",
        "                remaining = max(self.target_steps - steps, 0)\n",
        "                eta_sec = remaining / inst_sps\n",
        "                eta_str = f\" | eta={eta_sec/60:.1f}m\"\n",
        "            self._pylog.warning(\n",
        "                f\"[HB] steps={steps:,} | inst={inst_sps:,.0f} sps | avg={avg_sps:,.0f} sps\"\n",
        "                f\" | elapsed={total_dt/60:.1f}m{eta_str}\"\n",
        "            )\n",
        "            self._last_t = now\n",
        "            self._last_step = steps\n",
        "        return True\n",
        "\n",
        "\n",
        "# ========= Train/Eval per symbol =========\n",
        "skipped_all = []\n",
        "global_rows = []\n",
        "\n",
        "for idx, symbol in enumerate(run_symbols, 1):\n",
        "    log.warning(\"▶ [%d/%d] Processing %s | candidate windows=%s\",\n",
        "                idx, len(run_symbols), symbol, CANDIDATE_WS)\n",
        "    log.warning(\"---------------------------------------------------------------\")\n",
        "\n",
        "    sac_cfg = get_sac_cfg(symbol)\n",
        "    sdf = df[df[\"Symbol\"] == symbol].sort_values(\"Datetime\")\n",
        "\n",
        "    # Walk-forward split (1y train, 1y test within available ~2y window)\n",
        "    max_ts = sdf[\"Datetime\"].max()\n",
        "    test_end_dt    = max_ts\n",
        "    train_start_dt = test_end_dt - timedelta(days=729)\n",
        "    train_end_dt   = train_start_dt + timedelta(days=365)\n",
        "    test_start_dt  = train_end_dt\n",
        "\n",
        "    train_start_ts = pd.to_datetime(train_start_dt, utc=True)\n",
        "    train_end_ts   = pd.to_datetime(train_end_dt,   utc=True)\n",
        "    test_start_ts  = pd.to_datetime(test_start_dt,  utc=True)\n",
        "    test_end_ts    = pd.to_datetime(test_end_dt,    utc=True)\n",
        "\n",
        "    train_df = sdf[(sdf[\"Datetime\"] >= train_start_ts) & (sdf[\"Datetime\"] <  train_end_ts)].reset_index(drop=True)\n",
        "    test_df  = sdf[(sdf[\"Datetime\"] >= test_start_ts)  & (sdf[\"Datetime\"] <= test_end_ts)].reset_index(drop=True)\n",
        "    if len(train_df) < max(CANDIDATE_WS) + 10 or len(test_df) < max(CANDIDATE_WS) + 10:\n",
        "        log.warning(\"Symbol %s has too few rows for this split; skipping.\", symbol)\n",
        "        continue\n",
        "    steps_est = max(len(train_df), 1)\n",
        "\n",
        "    existing_ws = set()\n",
        "    try:\n",
        "        for f in os.listdir(SAVE_DIR):\n",
        "            m = re.match(rf\"^sac_{re.escape(symbol)}_ws(\\d+)\\.zip$\", f, re.IGNORECASE)\n",
        "            if m:\n",
        "                existing_ws.add(int(m.group(1)))\n",
        "    except Exception as e:\n",
        "        log.warning(\"Could not scan SAVE_DIR for existing windows (%s). Proceeding as if none exist.\", e)\n",
        "\n",
        "    pending_ws = CANDIDATE_WS if FORCE_RETRAIN else [ws for ws in CANDIDATE_WS if ws not in existing_ws]\n",
        "    if not pending_ws:\n",
        "        log.warning(\"⏭ Ticker %s fully skipped (all %d windows already complete).\", symbol, len(CANDIDATE_WS))\n",
        "        skipped_all.append(symbol)\n",
        "        continue\n",
        "    else:\n",
        "        have_str = \", \".join(map(str, sorted(existing_ws))) if existing_ws else \"none\"\n",
        "        todo_str = \", \".join(map(str, pending_ws))\n",
        "        log.info(\"▶️  %s pending windows: [%s] (already have: %s)\", symbol, todo_str, have_str)\n",
        "\n",
        "    # Write per-symbol features.json (auditing)\n",
        "    try:\n",
        "        sym_dir = os.path.join(RESULTS_DIR, symbol); os.makedirs(sym_dir, exist_ok=True)\n",
        "        _feature_cols = sorted(feature_cols)\n",
        "        with open(os.path.join(sym_dir, \"features.json\"), \"w\") as f:\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"algorithm\": \"SAC\",\n",
        "                    \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
        "                    \"features\": _feature_cols,\n",
        "                    \"note\": \"Active numeric observation columns for this symbol/run.\"\n",
        "                },\n",
        "                f, indent=2\n",
        "            )\n",
        "        log.info(\"Wrote %s/features.json with %d features\", symbol, len(_feature_cols))\n",
        "    except Exception as e:\n",
        "        log.warning(\"Per-symbol features.json write skipped for %s: %s\", symbol, e)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # ==== WS sweep (train+eval per window size, rank by Sharpe) ====\n",
        "    def train_eval_for_ws(ws: int):\n",
        "        log.info(\"  • WS=%d | building envs ...\", ws)\n",
        "        model_path = f\"{SAVE_DIR}/sac_{symbol}_ws{ws}\"\n",
        "        vec_path   = f\"{RESULTS_DIR}/vecnorms/{symbol}_ws{ws}_vecnorm.pkl\"\n",
        "        ckpt_dir   = os.path.join(CKPT_DIR_ROOT, f\"{symbol}_ws{ws}\")\n",
        "        os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "        # --- factories for this ws ---\n",
        "        def make_train_env():\n",
        "            return ContinuousTradingEnv(\n",
        "                train_df, frame_bound=(ws, len(train_df)), window_size=ws,\n",
        "                max_exposure=MAX_EXPOSURE, commission_bps=COMMISSION_BPS,\n",
        "                slippage_bps=SLIPPAGE_BPS, initial_capital=INITIAL_CAPITAL,\n",
        "                log_trades=LOG_TRADES_TRAIN, long_only=LONG_ONLY,\n",
        "                **_risk_kwargs(), **_reward_kwargs()\n",
        "            )\n",
        "\n",
        "        def make_test_env_for_callback():\n",
        "            return ContinuousTradingEnv(\n",
        "                test_df, frame_bound=(ws, len(test_df)), window_size=ws,\n",
        "                max_exposure=MAX_EXPOSURE, commission_bps=COMMISSION_BPS,\n",
        "                slippage_bps=SLIPPAGE_BPS, initial_capital=INITIAL_CAPITAL,\n",
        "                log_trades=LOG_TRADES_EVALCB, long_only=LONG_ONLY,\n",
        "                **_risk_kwargs(), **_reward_kwargs()\n",
        "            )\n",
        "\n",
        "        def make_test_env_for_final():\n",
        "            return ContinuousTradingEnv(\n",
        "                test_df, frame_bound=(ws, len(test_df)), window_size=ws,\n",
        "                max_exposure=MAX_EXPOSURE, commission_bps=COMMISSION_BPS,  # <- fixed typo\n",
        "                slippage_bps=SLIPPAGE_BPS, initial_capital=INITIAL_CAPITAL,\n",
        "                log_trades=LOG_TRADES_FINAL, long_only=LONG_ONLY,\n",
        "                **_risk_kwargs(), **_reward_kwargs()\n",
        "            )\n",
        "\n",
        "        # --- vec wrapper(s) ---\n",
        "        train_venv = DummyVecEnv([make_train_env])\n",
        "\n",
        "        # (re)load or create VecNormalize for training\n",
        "        if os.path.exists(vec_path):\n",
        "            try:\n",
        "                train_env = VecNormalize.load(vec_path, train_venv)\n",
        "                train_env.training = True\n",
        "                log.info(\"  • WS=%d | restored VecNormalize from %s\", ws, vec_path)\n",
        "            except Exception as e:\n",
        "                log.warning(\"  • WS=%d | VecNormalize load failed (%s). Starting fresh.\", ws, e)\n",
        "                train_env = VecNormalize(train_venv, training=True, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
        "        else:\n",
        "            train_env = VecNormalize(train_venv, training=True, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
        "\n",
        "        # Seed + reset after stats are in place\n",
        "        train_env.seed(SEED); _ = train_env.reset()\n",
        "\n",
        "        # --- SAC config (bucketed) ---\n",
        "        loc_cfg = sac_cfg.copy()\n",
        "        # Cap replay buffer for this run\n",
        "        loc_cfg[\"buffer_size\"] = min(int(loc_cfg[\"buffer_size\"]), steps_est * 20)\n",
        "\n",
        "        # --- choose total steps (define BEFORE eval callback) ---\n",
        "        if FAST_TEST:\n",
        "            base_steps  = max(200, min(MIN_TRAIN_STEPS, len(train_df)))\n",
        "            total_steps = max(base_steps, loc_cfg[\"learning_starts\"] + 5_000)\n",
        "        else:\n",
        "            total_steps = (\n",
        "                TRAIN_TOTAL_STEPS if TRAIN_TOTAL_STEPS is not None\n",
        "                else (min(4_000, len(train_df) * 2) if TEST_MODE else min(25_000, len(train_df) * 10))\n",
        "            )\n",
        "        eval_every = max(1_000, total_steps // 50)\n",
        "\n",
        "        # Optional eval env (only if we're using the eval callback)\n",
        "        eval_env = None\n",
        "        if USE_EVAL_CALLBACK:\n",
        "            eval_venv = DummyVecEnv([lambda: Monitor(make_test_env_for_callback())])\n",
        "            eval_env  = VecNormalize(eval_venv, training=False, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
        "            eval_env.obs_rms = train_env.obs_rms\n",
        "            eval_env.seed(SEED); _ = eval_env.reset()\n",
        "\n",
        "        # --- callbacks (create ONCE) ---\n",
        "        ckpt_cb = CheckpointCallback(\n",
        "            save_freq=CKPT_FREQ_STEPS,\n",
        "            save_path=ckpt_dir,\n",
        "            name_prefix=\"sac\",\n",
        "            save_replay_buffer=True\n",
        "        )\n",
        "        vec_cb = SaveVecNormCallback(train_env, vec_path, save_freq=CKPT_FREQ_STEPS, verbose=1)\n",
        "\n",
        "        eval_cb = None\n",
        "        if USE_EVAL_CALLBACK and eval_env is not None:\n",
        "            n_eval_no_improve, min_evals = (1, 2) if FAST_TEST else (2, 4)\n",
        "            eval_cb = EvalCallback(\n",
        "                eval_env,\n",
        "                best_model_save_path=os.path.join(RESULTS_DIR, \"tmp\", f\"best_{symbol}_ws{ws}\"),\n",
        "                eval_freq=eval_every,\n",
        "                n_eval_episodes=1,\n",
        "                callback_after_eval=StopTrainingOnNoModelImprovement(n_eval_no_improve, min_evals, verbose=1),\n",
        "                verbose=1,\n",
        "            )\n",
        "        hb_cb = HeartbeatCallback(\n",
        "          every=max(1_000, total_steps // 100),\n",
        "          target_steps=total_steps,\n",
        "          py_logger=log,\n",
        "        )\n",
        "\n",
        "        callback = CallbackList([cb for cb in (eval_cb, ckpt_cb, vec_cb, hb_cb) if cb is not None])\n",
        "\n",
        "        log.info(\n",
        "            \"EvalCallback: %s | Checkpoint: ENABLED | VecNormSaver: ENABLED | Heartbeat: ENABLED\",\n",
        "            \"ENABLED\" if eval_cb else \"DISABLED\")\n",
        "\n",
        "        # --- create model OR resume from latest checkpoint\n",
        "        resuming = False\n",
        "        latest_ckpt = None\n",
        "        try:\n",
        "            ckpts = sorted(glob.glob(os.path.join(ckpt_dir, \"sac_*_steps.zip\")), key=os.path.getmtime)\n",
        "            if ckpts:\n",
        "                latest_ckpt = ckpts[-1]\n",
        "        except Exception:\n",
        "            latest_ckpt = None\n",
        "\n",
        "        if latest_ckpt:\n",
        "            try:\n",
        "                model = SAC.load(latest_ckpt, env=train_env, device=device)\n",
        "                resuming = True\n",
        "                log.info(\"  • WS=%d | resumed model from %s\", ws, latest_ckpt)\n",
        "            except Exception as e:\n",
        "                log.warning(\"  • WS=%d | failed to load checkpoint (%s). Starting fresh.\", ws, e)\n",
        "                model = SAC(\n",
        "                    \"MlpPolicy\", train_env, device=device, policy_kwargs=policy_kwargs, verbose=1,\n",
        "                    learning_rate=loc_cfg[\"learning_rate\"], batch_size=loc_cfg[\"batch_size\"],\n",
        "                    train_freq=loc_cfg[\"train_freq\"], gradient_steps=loc_cfg[\"gradient_steps\"],\n",
        "                    gamma=loc_cfg[\"gamma\"], tau=loc_cfg[\"tau\"], ent_coef=loc_cfg[\"ent_coef\"],\n",
        "                    target_update_interval=loc_cfg[\"target_update_interval\"],\n",
        "                    buffer_size=loc_cfg[\"buffer_size\"], learning_starts=loc_cfg[\"learning_starts\"], seed=SEED,\n",
        "                )\n",
        "        else:\n",
        "            model = SAC(\n",
        "                \"MlpPolicy\", train_env, device=device, policy_kwargs=policy_kwargs, verbose=1,\n",
        "                learning_rate=loc_cfg[\"learning_rate\"], batch_size=loc_cfg[\"batch_size\"],\n",
        "                train_freq=loc_cfg[\"train_freq\"], gradient_steps=loc_cfg[\"gradient_steps\"],\n",
        "                gamma=loc_cfg[\"gamma\"], tau=loc_cfg[\"tau\"], ent_coef=loc_cfg[\"ent_coef\"],\n",
        "                target_update_interval=loc_cfg[\"target_update_interval\"],\n",
        "                buffer_size=loc_cfg[\"buffer_size\"], learning_starts=loc_cfg[\"learning_starts\"], seed=SEED,\n",
        "            )\n",
        "        if resuming:\n",
        "            buffer_path = latest_ckpt.replace(\".zip\", \"_replay_buffer.pkl\")\n",
        "            if os.path.exists(buffer_path):\n",
        "                try:\n",
        "                    model.load_replay_buffer(buffer_path)\n",
        "                    log.info(\"  • WS=%d | loaded replay buffer %s\", ws, buffer_path)\n",
        "                except Exception as e:\n",
        "                    log.warning(\"  • WS=%d | failed to load replay buffer: %s\", ws, e)\n",
        "\n",
        "        log.info(\"  • WS=%d | learn steps=%s | eval_every=%s | resume=%s\",\n",
        "                ws, f\"{total_steps:,}\", f\"{eval_every:,}\", resuming)\n",
        "\n",
        "        t0 = time.time()\n",
        "        model.learn(\n",
        "            total_timesteps=total_steps,\n",
        "            callback=callback,\n",
        "            reset_num_timesteps=not resuming,\n",
        "            progress_bar=False,\n",
        "        )\n",
        "        dt = time.time() - t0\n",
        "        log.info(\"  • WS=%d | learn done in %.1fs\", ws, dt)\n",
        "\n",
        "        # (these lines already exist in your code – keep them as-is)\n",
        "        model.save(model_path)\n",
        "        train_env.save(vec_path)\n",
        "        log.info(\"  • WS=%d | saved model=%s.zip vecnorm=%s\", ws, model_path, vec_path)\n",
        "        try:\n",
        "            final_rb_path = model_path + \"_replay_buffer.pkl\"\n",
        "            model.save_replay_buffer(final_rb_path)\n",
        "            log.info(\"  • WS=%d | saved final replay buffer -> %s\", ws, final_rb_path)\n",
        "        except Exception as e:\n",
        "            log.warning(\"  • WS=%d | could not save final replay buffer: %s\", ws, e)\n",
        "\n",
        "        # ---- model_info.json (concise) ----\n",
        "        try:\n",
        "            model_info = {\n",
        "                \"algorithm\": \"SAC\",\n",
        "                \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
        "                \"symbol\": symbol,\n",
        "                \"ws\": int(ws),\n",
        "                \"bucket\": sac_cfg.get(\"_bucket\", \"UNKNOWN\"),\n",
        "                \"seed\": int(SEED),\n",
        "                \"features_used\": int(len(feature_cols)),\n",
        "                \"reward_shaping\": {\n",
        "                    \"whipsaw_penalty_enabled\": bool(ENABLE_WHIPSAW_PENALTY),\n",
        "                    \"whipsaw_penalty\": float(WHIPSAW_PENALTY),\n",
        "                    \"cooldown_step_penalty_enabled\": bool(ENABLE_COOLDOWN_PENALTY),\n",
        "                    \"cooldown_step_penalty\": float(COOLDOWN_STEP_PENALTY),\n",
        "                    \"confidence_source\": \"env_penalties\"\n",
        "                },\n",
        "                \"policy\": {\n",
        "                    \"type\": \"MlpPolicy\",\n",
        "                    \"net_arch\": policy_kwargs.get(\"net_arch\", None),\n",
        "                    \"device\": str(device)\n",
        "                },\n",
        "                \"hyperparams\": {\n",
        "                    \"learning_rate\": float(loc_cfg[\"learning_rate\"]),\n",
        "                    \"batch_size\": int(loc_cfg[\"batch_size\"]),\n",
        "                    \"train_freq\": int(loc_cfg[\"train_freq\"]),\n",
        "                    \"gradient_steps\": int(loc_cfg[\"gradient_steps\"]),\n",
        "                    \"gamma\": float(loc_cfg[\"gamma\"]),\n",
        "                    \"tau\": float(loc_cfg[\"tau\"]),\n",
        "                    \"ent_coef\": str(loc_cfg[\"ent_coef\"]),\n",
        "                    \"buffer_size\": int(loc_cfg[\"buffer_size\"]),\n",
        "                    \"learning_starts\": int(loc_cfg[\"learning_starts\"]),\n",
        "                    \"target_update_interval\": int(loc_cfg[\"target_update_interval\"]),\n",
        "                },\n",
        "                \"env_params\": {\n",
        "                    \"window_size\": int(ws),\n",
        "                    \"max_exposure\": float(MAX_EXPOSURE),\n",
        "                    \"commission_bps\": float(COMMISSION_BPS),\n",
        "                    \"slippage_bps\": float(SLIPPAGE_BPS),\n",
        "                    \"dead_band\": float(DEAD_BAND),\n",
        "                    \"min_trade_delta\": float(MIN_TRADE_DELTA),\n",
        "                    \"stop_loss_pct\": float(STOP_LOSS_PCT) if ENABLE_SLO else None,\n",
        "                    \"take_profit_pct\": float(TAKE_PROFIT_PCT) if ENABLE_SLO else None,\n",
        "                    \"cooldown_steps\": int(COOLDOWN_STEPS if ENABLE_SLO else 0),\n",
        "                    \"sim_latency_ms\": int(SIM_LATENCY_MS),\n",
        "                    \"broker\": str(BROKER),\n",
        "                    \"long_only\": bool(LONG_ONLY),\n",
        "                },\n",
        "                \"training\": {\n",
        "                    \"total_steps\": int(total_steps),\n",
        "                    \"eval_every\": int(eval_every),\n",
        "                    \"use_eval_callback\": bool(eval_cb is not None),\n",
        "                    \"test_mode\": bool(TEST_MODE),\n",
        "                    \"fast_test\": bool(FAST_TEST),\n",
        "                    \"deterministic_eval\": True\n",
        "                },\n",
        "                \"artifacts\": {\n",
        "                    \"model_zip\": model_path + \".zip\",\n",
        "                    \"vecnorm_pkl\": vec_path\n",
        "                },\n",
        "            }\n",
        "            mi_dir = os.path.join(RESULTS_DIR, \"model_info\")\n",
        "            os.makedirs(mi_dir, exist_ok=True)\n",
        "            mi_path = os.path.join(mi_dir, f\"sac_{symbol}_ws{ws}_model_info.json\")\n",
        "            with open(mi_path, \"w\") as f:\n",
        "                json.dump(model_info, f, indent=2)\n",
        "            log.info(\"  • WS=%d | wrote model_info.json -> %s\", ws, mi_path)\n",
        "        except Exception as e:\n",
        "            log.warning(\"  • WS=%d | model_info.json write skipped: %s\", ws, e)\n",
        "            mi_path = None\n",
        "\n",
        "        final_eval_raw = DummyVecEnv([make_test_env_for_final])\n",
        "        final_eval_raw.seed(SEED)\n",
        "\n",
        "        # Defensive load of VecNormalize stats\n",
        "        if os.path.exists(vec_path):\n",
        "            final_eval = VecNormalize.load(vec_path, final_eval_raw)\n",
        "        else:\n",
        "            final_eval = VecNormalize(\n",
        "                final_eval_raw, training=False, norm_obs=True, norm_reward=False, clip_obs=10.0\n",
        "            )\n",
        "\n",
        "        final_eval.training = False\n",
        "        final_eval.norm_reward = False\n",
        "        final_eval.obs_rms = train_env.obs_rms\n",
        "        final_eval.seed(SEED)\n",
        "        obs = final_eval.reset()\n",
        "        portfolio, positions = [], []\n",
        "\n",
        "\n",
        "        # Confidence bucket accumulators (per-step stats)\n",
        "        bucket_stats = {\n",
        "            \"clean\":    {\"step_pnls\": [], \"count\": 0},\n",
        "            \"whipsaw\":  {\"step_pnls\": [], \"count\": 0},\n",
        "            \"cooldown\": {\"step_pnls\": [], \"count\": 0},\n",
        "        }\n",
        "        for i in range(len(test_df)):\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, _, dones, infos = final_eval.step(action)\n",
        "            info0 = infos[0]\n",
        "\n",
        "            portfolio.append(float(info0.get(\"portfolio_value\", np.nan)))\n",
        "            positions.append(float(info0.get(\"position\", 0.0)))\n",
        "\n",
        "            if len(portfolio) >= 2:\n",
        "                step_pnl = (portfolio[-1] / max(portfolio[-2], 1e-12)) - 1.0\n",
        "                if info0.get(\"whipsaw_penalty_applied\", False):\n",
        "                    b = \"whipsaw\"\n",
        "                elif info0.get(\"cooldown_penalty_applied\", False):\n",
        "                    b = \"cooldown\"\n",
        "                else:\n",
        "                    b = \"clean\"\n",
        "                bucket_stats[b][\"step_pnls\"].append(float(step_pnl))\n",
        "                bucket_stats[b][\"count\"] += 1\n",
        "\n",
        "            if bool(dones[0]):\n",
        "                break\n",
        "\n",
        "        # save signals for this ws\n",
        "        aligned_len = min(len(test_df), len(portfolio))\n",
        "        signals_path = f\"{RESULTS_DIR}/signals/{symbol}_ws{ws}_sac_signals.csv\"\n",
        "        test_eval_df = test_df.iloc[:aligned_len].copy()\n",
        "        if aligned_len > 0:\n",
        "            buy_hold = float(INITIAL_CAPITAL) * (\n",
        "                test_eval_df[\"Close\"].iloc[-1] / test_eval_df[\"Close\"].iloc[0]\n",
        "            )\n",
        "        else:\n",
        "            buy_hold = float(INITIAL_CAPITAL)\n",
        "\n",
        "        test_eval_df[\"Position\"] = positions[:aligned_len]\n",
        "        test_eval_df[\"Portfolio\"] = portfolio[:aligned_len]\n",
        "        test_eval_df.to_csv(signals_path, index=False)\n",
        "        log.info(\"  • WS=%d | wrote signals CSV → %s (rows=%d)\", ws, signals_path, aligned_len)\n",
        "\n",
        "        # try to attach signals path to model_info\n",
        "        try:\n",
        "            if mi_path and os.path.exists(mi_path):\n",
        "                with open(mi_path, \"r\") as f:\n",
        "                    _mi = json.load(f)\n",
        "                _mi.setdefault(\"artifacts\", {})[\"signals_csv\"] = signals_path\n",
        "                with open(mi_path, \"w\") as f:\n",
        "                    json.dump(_mi, f, indent=2)\n",
        "        except Exception as e:\n",
        "            log.warning(\"  • WS=%d | Could not update model_info with signals_csv: %s\", ws, e)\n",
        "        # ---- metrics (Sharpe + extended) ----\n",
        "        final_value = portfolio[-1] if portfolio else INITIAL_CAPITAL\n",
        "        ret_series  = pd.Series(portfolio, dtype=\"float64\").pct_change().dropna()\n",
        "        if len(ret_series) > 1 and ret_series.std() > 0:\n",
        "            # infer periods/year from median step size of timestamps\n",
        "            step_delta = pd.to_datetime(test_eval_df[\"Datetime\"]).diff().median()\n",
        "            step_sec = float(step_delta.total_seconds()) if pd.notna(step_delta) else 0.0\n",
        "            periods_per_year = int(np.clip(((365*24*3600)/max(step_sec, 1)) if step_sec > 0 else 252, 252, 100_000))\n",
        "            sharpe = (ret_series.mean() / ret_series.std()) * np.sqrt(max(periods_per_year, 1))\n",
        "        else:\n",
        "            sharpe = 0.0\n",
        "\n",
        "        # always compute drawdowns & trade stats\n",
        "        equity = pd.Series(portfolio, dtype=\"float64\")\n",
        "        roll_max = equity.cummax()\n",
        "        dd = (equity / roll_max) - 1.0\n",
        "        max_dd = float(dd.min()) if len(dd) else 0.0\n",
        "        dur_days = (\n",
        "            (test_eval_df[\"Datetime\"].iloc[aligned_len-1] - test_eval_df[\"Datetime\"].iloc[0]).total_seconds() / 86400.0\n",
        "            if aligned_len > 1 else 0.0\n",
        "        )\n",
        "        ann_ret = ((final_value / float(INITIAL_CAPITAL)) ** (365.0 / max(dur_days, 1e-9)) - 1.0) if dur_days > 0 else 0.0\n",
        "        calmar = (ann_ret / abs(max_dd)) if (max_dd < 0.0 and abs(max_dd) > 1e-12) else float(\"nan\")\n",
        "\n",
        "        trades = []\n",
        "        entry_val = None\n",
        "        prev_pos = 0.0\n",
        "        for i in range(aligned_len):\n",
        "            p = float(positions[i])\n",
        "            if prev_pos == 0.0 and abs(p) > 0.0:\n",
        "                entry_val = float(portfolio[i])\n",
        "            if prev_pos != 0.0 and abs(p) > 0.0 and np.sign(prev_pos) != np.sign(p):\n",
        "                if entry_val and entry_val > 0.0:\n",
        "                    trades.append((float(portfolio[i]) - entry_val) / entry_val)\n",
        "                entry_val = float(portfolio[i])\n",
        "            if prev_pos != 0.0 and abs(p) == 0.0 and entry_val and entry_val > 0.0:\n",
        "                trades.append((float(portfolio[i]) - entry_val) / entry_val)\n",
        "                entry_val = None\n",
        "            prev_pos = p\n",
        "\n",
        "        if prev_pos != 0.0 and entry_val and entry_val > 0.0:\n",
        "            trades.append((float(portfolio[aligned_len - 1]) - entry_val) / entry_val)\n",
        "        trade_count = int(len(trades))\n",
        "        hit_ratio = float(np.mean(np.array(trades) > 0.0)) if trade_count > 0 else float(\"nan\")\n",
        "        avg_trade_pnl = float(np.mean(trades)) if trade_count > 0 else float(\"nan\")\n",
        "\n",
        "\n",
        "        # Persist trade ledger\n",
        "        try:\n",
        "            trade_rows = []\n",
        "            entry_val = None; prev_pos = 0.0; entry_idx = None\n",
        "            for i in range(aligned_len):\n",
        "                p = float(positions[i]); nav = float(portfolio[i])\n",
        "                if prev_pos == 0.0 and abs(p) > 0.0:\n",
        "                    entry_val = nav; entry_idx = i\n",
        "                if prev_pos != 0.0 and abs(p) > 0.0 and np.sign(prev_pos) != np.sign(p):\n",
        "                    if entry_val and entry_val > 0.0 and entry_idx is not None:\n",
        "                        trade_rows.append({\n",
        "                            \"entry_idx\": int(entry_idx),\n",
        "                            \"exit_idx\": int(i),\n",
        "                            \"entry_nav\": float(entry_val),\n",
        "                            \"exit_nav\": float(nav),\n",
        "                            \"trade_pnl\": float((nav - entry_val) / entry_val),\n",
        "                            \"flip_close\": True\n",
        "                        })\n",
        "                    entry_val = nav; entry_idx = i\n",
        "                if prev_pos != 0.0 and abs(p) == 0.0 and entry_val and entry_val > 0.0 and entry_idx is not None:\n",
        "                    trade_rows.append({\n",
        "                        \"entry_idx\": int(entry_idx),\n",
        "                        \"exit_idx\": int(i),\n",
        "                        \"entry_nav\": float(entry_val),\n",
        "                        \"exit_nav\": float(nav),\n",
        "                        \"trade_pnl\": float((nav - entry_val) / entry_val),\n",
        "                        \"flip_close\": False\n",
        "                    })\n",
        "                    entry_val = None; entry_idx = None\n",
        "                prev_pos = p\n",
        "            if prev_pos != 0.0 and entry_val and entry_idx is not None:\n",
        "                trade_rows.append({\n",
        "                    \"entry_idx\": int(entry_idx),\n",
        "                    \"exit_idx\": int(aligned_len - 1),\n",
        "                    \"entry_nav\": float(entry_val),\n",
        "                    \"exit_nav\": float(portfolio[aligned_len - 1]),\n",
        "                    \"trade_pnl\": float((portfolio[aligned_len - 1] - entry_val) / entry_val),\n",
        "                    \"flip_close\": False\n",
        "                })\n",
        "            trades_csv = f\"{RESULTS_DIR}/signals/{symbol}_ws{ws}_sac_trades.csv\"\n",
        "            pd.DataFrame(trade_rows).to_csv(trades_csv, index=False)\n",
        "            log.info(\"  • WS=%d | wrote trades CSV → %s (trades=%d)\", ws, trades_csv, len(trade_rows))\n",
        "        except Exception as e:\n",
        "            trades_csv = None\n",
        "            log.warning(\"  • WS=%d | Could not write trades CSV: %s\", ws, e)\n",
        "\n",
        "        pos_arr = np.array(positions[:aligned_len], dtype=\"float64\")\n",
        "        turnover = float(np.sum(np.abs(np.diff(pos_arr)))) if aligned_len > 1 else 0.0\n",
        "        time_in_mkt = float(np.mean(np.abs(pos_arr) > 0.0)) if aligned_len > 0 else 0.0\n",
        "\n",
        "        # Persist WS metrics JSON\n",
        "        try:\n",
        "            def _bucket_summary(bs):\n",
        "                cnt = int(bs.get(\"count\", 0))\n",
        "                arr = np.array(bs.get(\"step_pnls\", []), dtype=\"float64\")\n",
        "                mean_pnl = float(np.mean(arr)) if cnt > 0 else float(\"nan\")\n",
        "                std_pnl  = float(np.std(arr, ddof=1)) if cnt > 1 else float(\"nan\")\n",
        "                hit_ratio_step = float(np.mean(arr > 0)) if cnt > 0 else float(\"nan\")\n",
        "                return {\n",
        "                    \"count\": cnt,\n",
        "                    \"mean_step_pnl\": mean_pnl,\n",
        "                    \"std_step_pnl\": std_pnl,\n",
        "                    \"hit_ratio_step\": hit_ratio_step,\n",
        "                }\n",
        "\n",
        "            for k in (\"clean\", \"whipsaw\", \"cooldown\"):\n",
        "                bucket_stats.setdefault(k, {\"step_pnls\": [], \"count\": 0})\n",
        "\n",
        "            metrics = {\n",
        "                \"symbol\": symbol,\n",
        "                \"ws\": int(ws),\n",
        "                \"final_value\": float(final_value),\n",
        "                \"buy_hold\": float(buy_hold),\n",
        "                \"sharpe\": float(sharpe),\n",
        "                \"max_drawdown\": float(max_dd),\n",
        "                \"calmar\": float(calmar),\n",
        "                \"trade_count\": trade_count,\n",
        "                \"hit_ratio\": hit_ratio,\n",
        "                \"avg_trade_pnl\": avg_trade_pnl,\n",
        "                \"turnover\": turnover,\n",
        "                \"time_in_market\": time_in_mkt,\n",
        "                \"confidence_buckets\": {\n",
        "                    \"clean\":    _bucket_summary(bucket_stats[\"clean\"]),\n",
        "                    \"whipsaw\":  _bucket_summary(bucket_stats[\"whipsaw\"]),\n",
        "                    \"cooldown\": _bucket_summary(bucket_stats[\"cooldown\"]),\n",
        "                }\n",
        "            }\n",
        "            mi_dir = os.path.join(RESULTS_DIR, \"model_info\"); os.makedirs(mi_dir, exist_ok=True)\n",
        "            metrics_path = os.path.join(mi_dir, f\"sac_{symbol}_ws{ws}_metrics.json\")\n",
        "            with open(metrics_path, \"w\") as f:\n",
        "                json.dump(metrics, f, indent=2)\n",
        "            log.info(\"  • WS=%d | wrote metrics JSON → %s\", ws, metrics_path)\n",
        "        except Exception as e:\n",
        "            log.warning(\"  • WS=%d | WS metrics JSON write skipped: %s\", ws, e)\n",
        "\n",
        "        if 'eval_env' in locals() and eval_env is not None:\n",
        "            try:\n",
        "                eval_env.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        for _env in ( 'train_env', 'final_eval', 'final_eval_raw' ):\n",
        "            try:\n",
        "                if _env in locals() and locals()[_env] is not None:\n",
        "                    locals()[_env].close()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Null out first (safe even if some names never existed)\n",
        "        model = train_env = eval_env = final_eval = final_eval_raw = None\n",
        "\n",
        "        # Now delete names if they exist (optional, helps free locals sooner)\n",
        "        try:\n",
        "            del final_eval, final_eval_raw, eval_env, train_env, model\n",
        "        except NameError:\n",
        "            pass\n",
        "\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "        return dict(\n",
        "            ws=ws,\n",
        "            sharpe=float(sharpe),\n",
        "            final_value=float(final_value),\n",
        "            buy_hold=float(buy_hold),\n",
        "            max_dd=float(max_dd),\n",
        "            calmar=(float(calmar) if np.isfinite(calmar) else None),\n",
        "            trade_count=int(trade_count),\n",
        "            hit_ratio=(float(hit_ratio) if trade_count > 0 else None),\n",
        "            avg_trade_pnl=(float(avg_trade_pnl) if trade_count > 0 else None),\n",
        "            turnover=float(turnover),\n",
        "            time_in_mkt=float(time_in_mkt),\n",
        "            model_zip=model_path + \".zip\",\n",
        "            vecnorm_pkl=vec_path,\n",
        "            signals_csv=signals_path\n",
        "        )\n",
        "\n",
        "    # === run sweep ===\n",
        "    results_ws = []\n",
        "    top_heap = []\n",
        "\n",
        "    for j, WS in enumerate(pending_ws, 1):\n",
        "        log.warning(\"   • %s: WS %d/%d = %s\", symbol, j, len(pending_ws), WS)\n",
        "        try:\n",
        "            res = train_eval_for_ws(WS)\n",
        "\n",
        "            results_ws.append({\n",
        "                \"Symbol\": symbol,\n",
        "                \"WS\": WS,\n",
        "                \"Sharpe\": _safe_round(res[\"sharpe\"], 3),\n",
        "                \"SAC_Portfolio\": _safe_round(res[\"final_value\"], 2),\n",
        "                \"BuyHold\": _safe_round(res[\"buy_hold\"], 2),\n",
        "                \"MaxDD\": _safe_round(res.get(\"max_dd\"), 4),\n",
        "                \"Calmar\": _safe_round(res.get(\"calmar\"), 3),\n",
        "                \"TradeCount\": int(res.get(\"trade_count\", 0)),\n",
        "                \"HitRatio\": _safe_round(res.get(\"hit_ratio\"), 3),\n",
        "                \"AvgTradePnL\": _safe_round(res.get(\"avg_trade_pnl\"), 4),\n",
        "                \"Turnover\": _safe_round(res.get(\"turnover\", 0.0), 3),\n",
        "                \"TimeInMkt\": _safe_round(res.get(\"time_in_mkt\", 0.0), 3),\n",
        "                \"ModelZip\": res[\"model_zip\"],\n",
        "                \"VecNorm\": res[\"vecnorm_pkl\"],\n",
        "                \"SignalsCSV\": res[\"signals_csv\"],\n",
        "                \"Edge_vs_BH_%\": _safe_round((res[\"final_value\"] / max(res[\"buy_hold\"], 1e-12) - 1.0) * 100.0, 6),\n",
        "                \"SAC_vs_BH_Ratio\": _safe_round(res[\"final_value\"] / max(res[\"buy_hold\"], 1e-12), 6),\n",
        "\n",
        "            })\n",
        "\n",
        "            s = float(res[\"sharpe\"])\n",
        "            score = s if (np.isfinite(s) and s == s) else float(\"-inf\")\n",
        "            heapq.heappush(top_heap, (score, WS, res))\n",
        "            if len(top_heap) > TOP_N_WINDOWS:\n",
        "                heapq.heappop(top_heap)\n",
        "\n",
        "        except Exception as e:\n",
        "            log.warning(\"WS=%s failed for %s: %s\", WS, symbol, e)\n",
        "\n",
        "    # save per-symbol WS sweep summary\n",
        "    ws_summary_path = os.path.join(RESULTS_DIR, f\"{symbol}_sac_ws_sweep_summary.csv\")\n",
        "\n",
        "    if results_ws:\n",
        "        df_ws = pd.DataFrame(results_ws)\n",
        "        if \"Sharpe\" not in df_ws.columns:\n",
        "            df_ws[\"Sharpe\"] = np.nan\n",
        "        df_ws.sort_values(\"Sharpe\", ascending=False).to_csv(ws_summary_path, index=False)\n",
        "    else:\n",
        "        cols = [\"Symbol\",\"WS\",\"Sharpe\",\"SAC_Portfolio\",\"BuyHold\",\"MaxDD\",\"Calmar\",\n",
        "                \"TradeCount\",\"HitRatio\",\"AvgTradePnL\",\"Turnover\",\"TimeInMkt\",\n",
        "                \"ModelZip\",\"VecNorm\",\"SignalsCSV\",\"Edge_vs_BH_%\",\"SAC_vs_BH_Ratio\"]\n",
        "        pd.DataFrame(columns=cols).to_csv(ws_summary_path, index=False)\n",
        "\n",
        "    try:\n",
        "        ws_json_path = ws_summary_path.replace(\".csv\", \".json\")\n",
        "        with open(ws_json_path, \"w\") as f:\n",
        "            if results_ws:\n",
        "                safe_sorted = sorted(\n",
        "                    results_ws,\n",
        "                    key=lambda r: (r.get(\"Sharpe\") if (isinstance(r.get(\"Sharpe\"), (int, float)) and np.isfinite(r.get(\"Sharpe\"))) else -1e9),\n",
        "                    reverse=True\n",
        "                )\n",
        "                json.dump(safe_sorted, f, indent=2)\n",
        "            else:\n",
        "                json.dump([], f, indent=2)\n",
        "        log.info(\"Wrote WS sweep JSON -> %s\", ws_json_path)\n",
        "    except Exception as e:\n",
        "        ws_json_path = None\n",
        "        log.warning(\"WS sweep JSON write skipped: %s\", e)\n",
        "\n",
        "    global_rows.extend(results_ws)\n",
        "\n",
        "    # --- per-symbol console summary (INSIDE LOOP) ---\n",
        "    def _sharpe_key(row):\n",
        "        try:\n",
        "            s = float(row.get(\"Sharpe\"))\n",
        "            return s if isfinite(s) else float(\"-inf\")\n",
        "        except Exception:\n",
        "            return float(\"-inf\")\n",
        "\n",
        "    def _fmt_num(v, fmt):\n",
        "        try:\n",
        "            x = float(v)\n",
        "            return fmt % x if isfinite(x) else \"nan\"\n",
        "        except Exception:\n",
        "            return \"nan\"\n",
        "\n",
        "    if results_ws:\n",
        "        best_row = max(results_ws, key=_sharpe_key)\n",
        "        log.warning(\n",
        "            \"%s summary → best WS=%s | Sharpe=%s | Final=%s | B&H=%s | MaxDD=%s | Trades=%d | Hit=%s\",\n",
        "            symbol,\n",
        "            best_row.get(\"WS\"),\n",
        "            _fmt_num(best_row.get(\"Sharpe\"),        \"%.3f\"),\n",
        "            _fmt_num(best_row.get(\"SAC_Portfolio\"), \"%.2f\"),\n",
        "            _fmt_num(best_row.get(\"BuyHold\"),       \"%.2f\"),\n",
        "            _fmt_num(best_row.get(\"MaxDD\"),         \"%.4f\"),\n",
        "            int(best_row.get(\"TradeCount\", 0)),\n",
        "            _fmt_num(best_row.get(\"HitRatio\"),      \"%.3f\"),\n",
        "        )\n",
        "        if ws_json_path:\n",
        "            log.warning(\"Artifacts: %s | %s\", ws_summary_path, ws_json_path)\n",
        "        else:\n",
        "            log.warning(\"Artifacts: %s\", ws_summary_path)\n",
        "    else:\n",
        "        log.warning(\"%s summary → no successful window results.\", symbol)\n",
        "\n",
        "    # report winners\n",
        "    top_sorted = sorted(top_heap, key=lambda t: t[0], reverse=True)\n",
        "    log.info(\n",
        "        f\"Top {min(TOP_N_WINDOWS, len(top_sorted))} window sizes for {symbol}: \" +\n",
        "        \", \".join([f\"WS={w} (Sharpe={s:.3f})\" for s, w, _ in top_sorted])\n",
        "    )\n",
        "\n",
        "    # === Persist Top-N winners' artifacts ===\n",
        "    best_root = os.path.join(RESULTS_DIR, \"best\", symbol)\n",
        "    os.makedirs(best_root, exist_ok=True)\n",
        "\n",
        "    top_k = min(TOP_N_WINDOWS, len(top_sorted))\n",
        "    manifest = []\n",
        "    for rank, (s_val, ws_best, res_obj) in enumerate(top_sorted[:top_k], start=1):\n",
        "        ws_dir = os.path.join(best_root, f\"ws{int(ws_best)}\")\n",
        "        os.makedirs(ws_dir, exist_ok=True)\n",
        "\n",
        "        src_model  = res_obj[\"model_zip\"]\n",
        "        src_vec    = res_obj[\"vecnorm_pkl\"]\n",
        "        src_sig    = res_obj[\"signals_csv\"]\n",
        "        src_metrics_json = os.path.join(RESULTS_DIR, \"model_info\", f\"sac_{symbol}_ws{ws_best}_metrics.json\")\n",
        "        src_model_info   = os.path.join(RESULTS_DIR, \"model_info\", f\"sac_{symbol}_ws{ws_best}_model_info.json\")\n",
        "        src_conf_json    = os.path.join(RESULTS_DIR, \"model_info\", f\"sac_{symbol}_ws{ws_best}_confidence_metrics.json\")  # may not exist\n",
        "\n",
        "        def _maybe_copy(src, dst_name):\n",
        "            try:\n",
        "                if src and os.path.exists(src):\n",
        "                    dst = os.path.join(ws_dir, dst_name)\n",
        "                    copyfile(src, dst)\n",
        "                    return dst\n",
        "            except Exception as e:\n",
        "                log.warning(\"Copy failed %s -> %s: %s\", src, dst_name, e)\n",
        "            return None\n",
        "\n",
        "        dsts = {\n",
        "            \"model_zip\": _maybe_copy(src_model,  os.path.basename(src_model)),\n",
        "            \"vecnorm_pkl\": _maybe_copy(src_vec,  os.path.basename(src_vec)),\n",
        "            \"signals_csv\": _maybe_copy(src_sig,  os.path.basename(src_sig)),\n",
        "            \"metrics_json\": _maybe_copy(src_metrics_json, os.path.basename(src_metrics_json)),\n",
        "            \"model_info_json\": _maybe_copy(src_model_info, os.path.basename(src_model_info)),\n",
        "            \"confidence_json\": _maybe_copy(src_conf_json, os.path.basename(src_conf_json)),\n",
        "        }\n",
        "\n",
        "        manifest.append({\n",
        "            \"rank\": rank,\n",
        "            \"ws\": int(ws_best),\n",
        "            \"sharpe\": float(s_val),\n",
        "            \"artifacts\": dsts,\n",
        "        })\n",
        "\n",
        "    try:\n",
        "        top_manifest_path = os.path.join(best_root, \"top_windows.json\")\n",
        "        with open(top_manifest_path, \"w\") as f:\n",
        "            json.dump(manifest, f, indent=2)\n",
        "        log.info(\"Persisted Top-%d winners for %s -> %s\", top_k, symbol, top_manifest_path)\n",
        "    except Exception as e:\n",
        "        log.warning(\"Top-N manifest write failed for %s: %s\", symbol, e)\n",
        "\n",
        "    del results_ws, top_heap, top_sorted\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# --- Global skip summary (optional) ---\n",
        "try:\n",
        "    if skipped_all:\n",
        "        skip_path = os.path.join(RESULTS_DIR, \"skipped_windows_global.csv\")\n",
        "        pd.DataFrame({\"Symbol\": sorted(set(skipped_all))}).to_csv(skip_path, index=False)\n",
        "        log.info(\"Fully skipped tickers (all windows done): %s\", \", \".join(sorted(set(skipped_all))))\n",
        "        log.info(\"Global skip log: %s\", skip_path)\n",
        "    else:\n",
        "        log.info(\"No fully skipped tickers.\")\n",
        "except Exception as e:\n",
        "    log.warning(\"Failed to write global skip log: %s\", e)\n",
        "\n",
        "log.info(\"✅ SAC run complete.\")\n",
        "# === Global summarizer (drop-in) ===\n",
        "try:\n",
        "    # Prefer in-memory rows collected this run; otherwise, sweep the folder.\n",
        "    if \"global_rows\" in globals() and global_rows:\n",
        "        all_rows_df = pd.DataFrame(global_rows)\n",
        "    else:\n",
        "        csvs = sorted(glob.glob(os.path.join(RESULTS_DIR, \"*_sac_ws_sweep_summary.csv\")))\n",
        "        all_rows_df = pd.concat((pd.read_csv(p) for p in csvs), ignore_index=True) if csvs else pd.DataFrame()\n",
        "\n",
        "    if not all_rows_df.empty:\n",
        "        # Ensure derived columns exist (in case older CSVs lacked them)\n",
        "        if \"Edge_vs_BH_%\" not in all_rows_df.columns:\n",
        "            all_rows_df[\"Edge_vs_BH_%\"] = (all_rows_df[\"SAC_Portfolio\"] / all_rows_df[\"BuyHold\"] - 1.0) * 100.0\n",
        "        if \"SAC_vs_BH_Ratio\" not in all_rows_df.columns:\n",
        "            all_rows_df[\"SAC_vs_BH_Ratio\"] = (all_rows_df[\"SAC_Portfolio\"] / all_rows_df[\"BuyHold\"])\n",
        "\n",
        "        # Save all rows\n",
        "        out_all = os.path.join(RESULTS_DIR, \"global_ws_sweep_all_rows.csv\")\n",
        "        all_rows_df.to_csv(out_all, index=False)\n",
        "\n",
        "        # Backfill selector/filter columns if missing in older CSVs\n",
        "        for col, default in [\n",
        "            (\"TimeInMkt\", np.nan),\n",
        "            (\"TradeCount\", 0),\n",
        "            (\"Turnover\", np.nan),\n",
        "            (\"Calmar\", np.nan),\n",
        "            (\"HitRatio\", np.nan),\n",
        "        ]:\n",
        "            if col not in all_rows_df.columns:\n",
        "                all_rows_df[col] = default\n",
        "\n",
        "\n",
        "        # Best WS per symbol by highest Sharpe\n",
        "        best_by_symbol = (\n",
        "            all_rows_df.sort_values([\"Symbol\", \"Sharpe\"], ascending=[True, False])\n",
        "                       .groupby(\"Symbol\", as_index=False).head(1)\n",
        "        )\n",
        "        out_best = os.path.join(RESULTS_DIR, \"global_best_by_symbol.csv\")\n",
        "        best_by_symbol.to_csv(out_best, index=False)\n",
        "\n",
        "        # === Save model selector CSV for later pipeline ===\n",
        "        try:\n",
        "            # Guardrails\n",
        "            flt = (\n",
        "                (best_by_symbol[\"Sharpe\"] > 0.0)\n",
        "                & (best_by_symbol[\"SAC_Portfolio\"] > best_by_symbol[\"BuyHold\"])\n",
        "                & (best_by_symbol[\"TimeInMkt\"].between(0.20, 0.85, inclusive=\"both\"))\n",
        "                & (best_by_symbol[\"TradeCount\"] >= 50)\n",
        "                & (best_by_symbol[\"Turnover\"] <= 600)\n",
        "            )\n",
        "            selector_src = best_by_symbol.loc[flt].copy()\n",
        "            if selector_src.empty:\n",
        "                selector_src = best_by_symbol.copy()  # fallback\n",
        "\n",
        "            # Score = Sharpe + small bonuses/penalties\n",
        "            selector_src[\"Score\"] = (\n",
        "                selector_src[\"Sharpe\"]\n",
        "                + 0.10 * selector_src[\"Calmar\"].fillna(0)\n",
        "                - 0.0005 * selector_src[\"Turnover\"].fillna(0)\n",
        "            )\n",
        "            selector_src = selector_src.sort_values([\"Score\", \"Sharpe\"], ascending=False)\n",
        "\n",
        "            # Final selector schema\n",
        "            selector_df = selector_src[[\"Symbol\", \"Sharpe\", \"HitRatio\", \"MaxDD\", \"SAC_Portfolio\"]].rename(columns={\n",
        "                \"Symbol\": \"Ticker\",\n",
        "                \"HitRatio\": \"Accuracy\",\n",
        "                \"MaxDD\": \"Drawdown\",\n",
        "                \"SAC_Portfolio\": \"Final_Portfolio\"\n",
        "            })\n",
        "            selector_df[\"Model\"] = \"SAC\"\n",
        "\n",
        "            selector_out = os.path.join(RESULTS_DIR, \"sac_model_selector.csv\")\n",
        "            selector_df.to_csv(selector_out, index=False)\n",
        "            print(f\"✅ Saved model selector file → {selector_out} (rows={len(selector_df)})\")\n",
        "        except Exception as e:\n",
        "            print(f\"[selector_writer] Failed to save model selector CSV: {e}\")\n",
        "\n",
        "        # Console summary\n",
        "        syms = best_by_symbol[\"Symbol\"].nunique()\n",
        "        mean_sharpe = float(best_by_symbol[\"Sharpe\"].mean())\n",
        "        median_sharpe = float(best_by_symbol[\"Sharpe\"].median())\n",
        "        n_pos = int((best_by_symbol[\"Sharpe\"] > 0).sum())\n",
        "        n_ge1 = int((best_by_symbol[\"Sharpe\"] >= 1.0).sum())\n",
        "        beat_bh = int((best_by_symbol[\"SAC_Portfolio\"] > best_by_symbol[\"BuyHold\"]).sum())\n",
        "\n",
        "        print(f\"[summarizer] Using RESULTS_DIR = {RESULTS_DIR}\\n\")\n",
        "        print(\"=== Summary ===\")\n",
        "        print(f\"Symbols covered:         {syms}\")\n",
        "        print(f\"Mean Sharpe (best WS):   {mean_sharpe:.3f}\")\n",
        "        print(f\"Median Sharpe (best WS): {median_sharpe:.3f}\")\n",
        "        print(f\"Sharpe > 0 (best WS):    {n_pos} / {syms}\")\n",
        "        print(f\"Sharpe ≥ 1.0 (best WS):  {n_ge1} / {syms}\")\n",
        "        print(f\"Beat Buy&Hold (best WS): {beat_bh} / {syms}\\n\")\n",
        "\n",
        "        # Top 10 windows overall\n",
        "        cols = [\"Symbol\", \"WS\", \"Sharpe\", \"Edge_vs_BH_%\", \"SAC_vs_BH_Ratio\", \"MaxDD\", \"Calmar\"]\n",
        "        top10 = all_rows_df.sort_values(\"Sharpe\", ascending=False).head(10)\n",
        "        top10 = top10[[c for c in cols if c in top10.columns]]\n",
        "        print(\"Top 10 windows overall by Sharpe:\")\n",
        "        print(top10.to_string(index=False))\n",
        "\n",
        "        print(\"\\nSaved:\")\n",
        "        print(\" -\", out_all)\n",
        "        print(\" -\", out_best)\n",
        "    else:\n",
        "        print(\"[summarizer] No rows found to summarize.\")\n",
        "except Exception as e:\n",
        "    print(\"[summarizer] Failed:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfYGc3ncD-EL"
      },
      "outputs": [],
      "source": [
        "# === SAC Model Selector: Full Aggregator + Enhancer ===\n",
        "import os, glob, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Paths (align with your SAC trainer) ---\n",
        "DRIVE_BASE  = os.getenv(\"DRIVE_BASE\", \"/content/drive/MyDrive\")\n",
        "RESULTS_DIR = os.path.join(DRIVE_BASE, \"Results_May_2025\", \"results_sac_walkforward\")\n",
        "\n",
        "# Where SAC models/vecnorms are written by the trainer:\n",
        "FINAL_MODEL_DIR = os.path.join(RESULTS_DIR, \"models_sac\")   # contains sac_{SYM}_ws{WS}.zip\n",
        "VECNORM_DIR     = os.path.join(RESULTS_DIR, \"vecnorms\")     # contains {SYM}_ws{WS}_vecnorm.pkl\n",
        "\n",
        "# Selector outputs\n",
        "SELECTOR_FULL_PATH = os.path.join(RESULTS_DIR, \"sac_model_selector_FULL.csv\")\n",
        "SELECTOR_JSON_PATH = os.path.join(RESULTS_DIR, \"sac_model_selector_final.json\")\n",
        "MODEL_NAME = \"SAC\"\n",
        "\n",
        "# --- 1) Collect all per-symbol sweep summaries ---\n",
        "summary_files = glob.glob(os.path.join(RESULTS_DIR, \"*_sac_ws_sweep_summary.csv\"))\n",
        "if not summary_files:\n",
        "    raise SystemExit(\"❌ No SAC sweep summaries found (expected *_sac_ws_sweep_summary.csv).\")\n",
        "\n",
        "frames = []\n",
        "for p in summary_files:\n",
        "    try:\n",
        "        dfp = pd.read_csv(p)\n",
        "        dfp[\"SourceFile\"] = os.path.basename(p)\n",
        "        frames.append(dfp)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Skipping {p} due to error: {e}\")\n",
        "\n",
        "if not frames:\n",
        "    raise SystemExit(\"❌ No readable SAC summaries.\")\n",
        "\n",
        "raw = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "# Ensure expected columns exist (older runs may miss some)\n",
        "for col in [\"Symbol\",\"WS\",\"Sharpe\",\"SAC_Portfolio\",\"BuyHold\",\"MaxDD\",\"Calmar\",\"HitRatio\",\"Turnover\",\"TimeInMkt\"]:\n",
        "    if col not in raw.columns:\n",
        "        raw[col] = np.nan\n",
        "\n",
        "# Drop dups by (Symbol, WS) keeping the last written row\n",
        "raw = raw.drop_duplicates(subset=[\"Symbol\",\"WS\"], keep=\"last\")\n",
        "\n",
        "# --- 2) Pick best WS per symbol by Sharpe ---\n",
        "# Treat NaN/inf Sharpe as very low\n",
        "def _safe_sharpe(x):\n",
        "    try:\n",
        "        v = float(x)\n",
        "        return v if np.isfinite(v) else -1e9\n",
        "    except Exception:\n",
        "        return -1e9\n",
        "\n",
        "raw[\"_SharpeKey\"] = raw[\"Sharpe\"].apply(_safe_sharpe)\n",
        "best = (raw.sort_values([\"Symbol\",\"_SharpeKey\"], ascending=[True, False])\n",
        "            .groupby(\"Symbol\", as_index=False).head(1).drop(columns=[\"_SharpeKey\"]))\n",
        "\n",
        "# --- 3) Normalize to selector schema ---\n",
        "selector = best.rename(columns={\n",
        "    \"Symbol\": \"Ticker\",\n",
        "    \"WS\": \"Window\",\n",
        "    \"SAC_Portfolio\": \"Final_Portfolio\",\n",
        "    \"MaxDD\": \"Drawdown\",\n",
        "    \"HitRatio\": \"Accuracy\",\n",
        "})\n",
        "selector[\"Model\"] = MODEL_NAME\n",
        "\n",
        "# Backfills for optional columns\n",
        "for col, default in [\n",
        "    (\"Calmar\", np.nan),\n",
        "    (\"Turnover\", np.nan),\n",
        "    (\"TimeInMkt\", np.nan),\n",
        "    (\"BuyHold\", np.nan),\n",
        "]:\n",
        "    if col not in selector.columns:\n",
        "        selector[col] = default\n",
        "\n",
        "# --- 4) Add artifact paths (match your trainer’s save patterns) ---\n",
        "# sac_{TICKER}_ws{Window}.zip and {TICKER}_ws{Window}_vecnorm.pkl\n",
        "selector[\"Window\"] = selector[\"Window\"].astype(int)\n",
        "selector[\"artifact_path\"] = selector.apply(\n",
        "    lambda r: os.path.join(FINAL_MODEL_DIR, f\"sac_{r['Ticker']}_ws{int(r['Window'])}.zip\"), axis=1\n",
        ")\n",
        "selector[\"vecnorm_path\"] = selector.apply(\n",
        "    lambda r: os.path.join(VECNORM_DIR, f\"{r['Ticker']}_ws{int(r['Window'])}_vecnorm.pkl\"), axis=1\n",
        ")\n",
        "selector[\"artifact_exists\"] = selector[\"artifact_path\"].apply(os.path.exists)\n",
        "selector[\"vecnorm_exists\"]  = selector[\"vecnorm_path\"].apply(os.path.exists)\n",
        "\n",
        "# --- 5) Safety gates (light; adjust as you like)\n",
        "# Drawdown is negative (e.g., -0.35). Keep those above -0.6 (i.e., not worse than -60%).\n",
        "gates = (\n",
        "    (selector[\"Sharpe\"] > 0.0) &\n",
        "    (selector[\"Drawdown\"] > -0.60) &\n",
        "    (selector[\"artifact_exists\"]) &\n",
        "    (selector[\"vecnorm_exists\"])\n",
        ")\n",
        "filtered = selector.loc[gates].copy()\n",
        "if filtered.empty:\n",
        "    # Fallback: if filters are too strict, export all best picks so downstream can decide.\n",
        "    print(\"⚠️ All candidates filtered out—exporting unfiltered best picks.\")\n",
        "    filtered = selector.copy()\n",
        "\n",
        "# --- 6) Save aggregated CSV (FULL) ---\n",
        "filtered.to_csv(SELECTOR_FULL_PATH, index=False)\n",
        "print(f\"✅ Aggregated SAC selector saved to → {SELECTOR_FULL_PATH} (rows={len(filtered)})\")\n",
        "\n",
        "# --- 7) Build final JSON with metadata & tie logic (optional ensemble on near-ties) ---\n",
        "# Recompute per-ticker near-tie on Sharpe if multiple rows per ticker survived (rare here since best-per-ticker).\n",
        "EPS = 0.03  # 3% relative Sharpe tie\n",
        "\n",
        "selected_models = {}\n",
        "for ticker, group in filtered.groupby(\"Ticker\"):\n",
        "    # In practice 'filtered' has 1 row per ticker; but support extras if you keep more than best later.\n",
        "    group_sorted = group.sort_values(\"Sharpe\", ascending=False)\n",
        "    top = group_sorted.iloc[0]\n",
        "    second = group_sorted.iloc[1] if len(group_sorted) > 1 else None\n",
        "\n",
        "    if second is not None and abs(top[\"Sharpe\"] - second[\"Sharpe\"]) <= abs(top[\"Sharpe\"]) * EPS:\n",
        "        mode = \"ensemble\"\n",
        "        secondary = {\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"window\": int(second[\"Window\"]),\n",
        "            \"artifact\": {\n",
        "                \"path\": second[\"artifact_path\"],\n",
        "                \"vecnorm\": second[\"vecnorm_path\"],\n",
        "                \"exists\": bool(second[\"artifact_exists\"]),\n",
        "                \"vecnorm_exists\": bool(second[\"vecnorm_exists\"]),\n",
        "            }\n",
        "        }\n",
        "    else:\n",
        "        mode = \"single\"\n",
        "        secondary = None\n",
        "\n",
        "    selected_models[ticker] = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"window\": int(top[\"Window\"]),\n",
        "        \"score\": round(float(top[\"Sharpe\"]), 4),\n",
        "        \"return\": round(float(top[\"Final_Portfolio\"]), 2) if pd.notna(top[\"Final_Portfolio\"]) else None,\n",
        "        \"sharpe\": round(float(top[\"Sharpe\"]), 3) if pd.notna(top[\"Sharpe\"]) else None,\n",
        "        \"drawdown\": round(float(top[\"Drawdown\"]), 4) if pd.notna(top[\"Drawdown\"]) else None,\n",
        "        \"sortino\": None,\n",
        "        \"turnover\": float(top[\"Turnover\"]) if pd.notna(top[\"Turnover\"]) else None,\n",
        "        \"trade_count\": None,\n",
        "        \"stability\": {},\n",
        "        \"regime\": \"unknown\",\n",
        "        \"rl_profile\": \"sac\",\n",
        "        \"artifact\": {\n",
        "            \"path\": top[\"artifact_path\"],\n",
        "            \"vecnorm\": top[\"vecnorm_path\"],\n",
        "            \"features\": None,\n",
        "            \"load_ms\": 180,\n",
        "            \"mem_mb\": 512,\n",
        "            \"exists\": bool(top[\"artifact_exists\"]),\n",
        "            \"vecnorm_exists\": bool(top[\"vecnorm_exists\"]),\n",
        "        },\n",
        "        \"selection\": {\n",
        "            \"mode\": mode,\n",
        "            \"primary\": MODEL_NAME,\n",
        "            \"secondary\": secondary,\n",
        "        }\n",
        "    }\n",
        "\n",
        "with open(SELECTOR_JSON_PATH, \"w\") as f:\n",
        "    json.dump(selected_models, f, indent=2)\n",
        "\n",
        "print(f\"✅ Final enhanced SAC selector JSON saved to → {SELECTOR_JSON_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyqFDWu0EBUp"
      },
      "outputs": [],
      "source": [
        "import os, pandas as pd, matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/Results_May_2025/results_sac_walkforward\"\n",
        "winners = [(\"NVDA\",24), (\"AAPL\",24), (\"MSFT\",16)]  # requested windows\n",
        "\n",
        "def pick_signals(sym, ws):\n",
        "    p = os.path.join(RESULTS_DIR, \"signals\", f\"{sym}_ws{ws}_sac_signals.csv\")\n",
        "    if os.path.exists(p): return p\n",
        "    p2 = os.path.join(RESULTS_DIR, \"best\", sym, f\"ws{ws}\", f\"{sym}_ws{ws}_sac_signals.csv\")\n",
        "    if os.path.exists(p2): return p2\n",
        "    alts = sorted(glob(os.path.join(RESULTS_DIR, \"signals\", f\"{sym}_ws*_sac_signals.csv\")))\n",
        "    if alts:\n",
        "        print(f\"[{sym}] ws{ws} not found; using {os.path.basename(alts[-1])} instead.\")\n",
        "        return alts[-1]\n",
        "    raise FileNotFoundError(f\"No signals CSV found for {sym} under {RESULTS_DIR}.\")\n",
        "\n",
        "for sym, ws in winners:\n",
        "    path = pick_signals(sym, ws)\n",
        "    s = pd.read_csv(path, parse_dates=[\"Datetime\"])\n",
        "    if \"Close\" not in s.columns:\n",
        "        raise RuntimeError(f\"'Close' not in {path}; re-run training that writes Close into signals.\")\n",
        "    base = float(s[\"Portfolio\"].iloc[0])\n",
        "    bh = s[\"Close\"] / s[\"Close\"].iloc[0] * base\n",
        "    plt.figure()\n",
        "    plt.plot(s[\"Datetime\"], s[\"Portfolio\"], label=\"SAC\")\n",
        "    plt.plot(s[\"Datetime\"], bh, label=\"Buy & Hold\")\n",
        "    plt.title(f\"{sym} ({os.path.basename(path)})\")\n",
        "    plt.legend(); plt.xlabel(\"Date\"); plt.ylabel(\"NAV\")\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}