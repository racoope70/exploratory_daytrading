{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/exploratory_daytrading/blob/main/ppo_alpaca_paper_trading_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-DEy5gEqqEi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Clean any partials\n",
        "!pip uninstall -y stable-baselines3 shimmy gymnasium gym autorom AutoROM.accept-rom-license ale-py\n",
        "\n",
        "# Install the compatible trio (no [extra] to avoid Atari deps)\n",
        "!pip install \"gymnasium==0.29.1\" \"shimmy==1.3.0\" \"stable-baselines3==2.3.0\"\n",
        "\n",
        "# Your other libs (safe to keep separate)\n",
        "!pip install alpaca-trade-api ta python-dotenv gym-anytrading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyc0Dr0D86cA"
      },
      "outputs": [],
      "source": [
        "import torch, gymnasium, shimmy, stable_baselines3 as sb3\n",
        "import alpaca_trade_api, websockets, pywt\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"gymnasium:\", gymnasium.__version__)\n",
        "print(\"shimmy:\", shimmy.__version__)\n",
        "print(\"stable-baselines3:\", sb3.__version__)\n",
        "print(\"alpaca-trade-api:\", alpaca_trade_api.__version__)\n",
        "print(\"websockets:\", websockets.__version__)\n",
        "print(\"pywavelets:\", pywt.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM2YQVmcZjEu"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "import os, re, json, csv, shutil, logging, pickle, warnings, time, math, gc\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union, Mapping\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")   # save-to-file only; no inline rendering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import alpaca_trade_api as tradeapi\n",
        "from alpaca_trade_api.rest import TimeFrame, APIError\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "from decimal import Decimal, ROUND_HALF_UP, ROUND_DOWN\n",
        "\n",
        "\n",
        "def round_to_cents(x: float) -> float:\n",
        "    return float(Decimal(str(x)).quantize(Decimal(\"0.01\"), rounding=ROUND_DOWN))\n",
        "\n",
        "# Detect Colab and (optionally) mount Drive\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    from google.colab import drive, files  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Project root (Drive in Colab; cwd locally)\n",
        "if IN_COLAB:\n",
        "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/AlpacaPaper\")\n",
        "else:\n",
        "    PROJECT_ROOT = Path.cwd() / \"AlpacaPaper\"\n",
        "\n",
        "# Ensure project folders exist early\n",
        "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------------------------- Upload / Conversion Helpers -------------------------------\n",
        "def upload_env_and_artifacts_in_colab():\n",
        "    \"\"\"\n",
        "    In Colab this will prompt for:\n",
        "      1) .env (or Alpaca_keys.env.txt)  -> moves to PROJECT_ROOT/.env\n",
        "      2) Any model/feature/vecnorm files -> moves to ARTIFACTS_DIR (or fallback to PROJECT_ROOT/artifacts)\n",
        "    \"\"\"\n",
        "    if not IN_COLAB:\n",
        "        return\n",
        "\n",
        "    target_dir = Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")))\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"Upload your .env (or Alpaca_keys.env.txt). Cancel if already on Drive.\")\n",
        "    up = files.upload()\n",
        "    if up:\n",
        "        if \"Alpaca_keys.env.txt\" in up:\n",
        "            src = Path(\"Alpaca_keys.env.txt\")\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env → {dst}\")\n",
        "        elif \".env\" in up:\n",
        "            src = Path(\".env\")\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env → {dst}\")\n",
        "        else:\n",
        "            any_name = next(iter(up.keys()))\n",
        "            src = Path(any_name)\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env (renamed {any_name}) → {dst}\")\n",
        "\n",
        "    print(\"Upload your artifacts (ppo_*_model.zip, *_vecnorm*.pkl, *_features*.json or .txt).\")\n",
        "    up2 = files.upload()\n",
        "    for name in up2.keys():\n",
        "        shutil.move(name, target_dir / name)\n",
        "    print(\"Artifacts now in:\", sorted(p.name for p in target_dir.iterdir()))\n",
        "\n",
        "def _maybe_convert_features_txt_to_json():\n",
        "    \"\"\"\n",
        "    Convert any 'features_<TICKER>.txt' into 'ppo_<TICKER>_features.json' (simple list).\n",
        "    \"\"\"\n",
        "    art_dir = Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")))\n",
        "    art_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for p in art_dir.glob(\"features_*.txt\"):\n",
        "        ticker = re.sub(r\"^features_|\\.txt$\", \"\", p.name, flags=re.IGNORECASE)\n",
        "        try:\n",
        "            raw = p.read_text().strip()\n",
        "            items = [x.strip() for x in raw.replace(\",\", \"\\n\").splitlines() if x.strip()]\n",
        "            out = {\"features\": items}\n",
        "            out_path = art_dir / f\"ppo_{ticker}_features.json\"\n",
        "            out_path.write_text(json.dumps(out, indent=2))\n",
        "            print(f\"Converted {p.name} → {out_path.name}  ({len(items)} features)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not convert {p.name}: {e}\")\n",
        "\n",
        "def _maybe_rename_vecnorm_scaler():\n",
        "    \"\"\"\n",
        "    Rename any 'scaler_<TICKER>.pkl' to 'ppo_<TICKER>_vecnorm.pkl'.\n",
        "    \"\"\"\n",
        "    art_dir = Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")))\n",
        "    art_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for p in art_dir.glob(\"scaler_*.pkl\"):\n",
        "        ticker = re.sub(r\"^scaler_|\\.pkl$\", \"\", p.name, flags=re.IGNORECASE)\n",
        "        dst = art_dir / f\"ppo_{ticker}_vecnorm.pkl\"\n",
        "        if not dst.exists():\n",
        "            shutil.move(str(p), str(dst))\n",
        "            print(f\"Renamed {p.name} → {dst.name}\")\n",
        "\n",
        "# ---------------------------------- Env & logging --------------------------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load env (supports PROJECT_ROOT/.env)\n",
        "env_candidates = [PROJECT_ROOT / \".env\", Path(\".env\")]\n",
        "for env_path in env_candidates:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(dotenv_path=env_path, override=True)\n",
        "        break\n",
        "else:\n",
        "    load_dotenv(override=True)  # fallback to default search\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "# Centralize Knobs\n",
        "def _to_bool(x: str) -> bool:\n",
        "    return str(x).strip().lower() in (\"1\",\"true\",\"yes\",\"y\",\"on\")\n",
        "\n",
        "def _to_list_csv(x: str) -> list:\n",
        "    return [s.strip().upper() for s in str(x).split(\",\") if s.strip()]\n",
        "\n",
        "@dataclass\n",
        "class Knobs:\n",
        "    # API / mode\n",
        "    APCA_API_BASE_URL: str = \"https://paper-api.alpaca.markets\"\n",
        "    DRY_RUN: bool          = False       # False => place PAPER orders on PAPER endpoint\n",
        "    AUTO_RUN_LIVE: bool    = True\n",
        "    INF_DETERMINISTIC: bool= True\n",
        "\n",
        "    # Universe / files\n",
        "    TICKERS: list          = None\n",
        "    ARTIFACTS_DIR: str     = \"\"\n",
        "    RESULTS_ROOT: str      = \"\"\n",
        "\n",
        "    # Data feed / cadence / staleness\n",
        "    BARS_FEED: str         = \"iex\"          # \"\" lets Alpaca choose; \"iex\" for IEX\n",
        "    COOLDOWN_MIN: int      = 1\n",
        "    STALE_MAX_SEC: int     = 1800\n",
        "\n",
        "    # Sizing & entry/exit sensitivity\n",
        "    SIZING_MODE: str       = \"linear\"    # \"linear\" | \"threshold\"\n",
        "    WEIGHT_CAP: float      = 0.35\n",
        "    CONF_FLOOR: float      = 0.20        # threshold-mode only\n",
        "    ENTER_CONF_MIN: float  = 0.005\n",
        "    ENTER_WEIGHT_MIN: float= 0.010\n",
        "    EXIT_WEIGHT_MAX: float = 0.003\n",
        "    REBALANCE_MIN_NOTIONAL: float = 1.00\n",
        "    USE_FRACTIONALS: bool  = True\n",
        "    SEED_FIRST_SHARE: bool = True\n",
        "    ALLOW_SHORTS: bool = False\n",
        "\n",
        "    # add-ons\n",
        "    DELTA_WEIGHT_MIN: float = 0.005\n",
        "    RAW_POS_MIN: float = 0.00\n",
        "    RAW_NEG_MAX: float = 0.00\n",
        "\n",
        "    # Risk\n",
        "    TAKE_PROFIT_PCT: float = 0.05\n",
        "    STOP_LOSS_PCT: float   = 0.03\n",
        "\n",
        "    # Misc\n",
        "    STALE_BEST_WINDOW: str = \"\"    # e.g. \"3\" (exposed as BEST_WINDOW_ENV)\n",
        "\n",
        "    # Secrets\n",
        "    APCA_API_KEY_ID: str   = \"\"\n",
        "    APCA_API_SECRET_KEY: str = \"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_env(cls, defaults: \"Knobs\", project_root: Path, env: Mapping[str, str], overrides: Mapping[str, object] = None):\n",
        "        kv = {**defaults.__dict__}\n",
        "        kv.update({\n",
        "            \"APCA_API_BASE_URL\": env.get(\"APCA_API_BASE_URL\", kv[\"APCA_API_BASE_URL\"]),\n",
        "            \"AUTO_RUN_LIVE\":     _to_bool(env.get(\"AUTO_RUN_LIVE\", str(kv[\"AUTO_RUN_LIVE\"]))),\n",
        "            \"DRY_RUN\":           _to_bool(env.get(\"DRY_RUN\",       str(kv[\"DRY_RUN\"]))),\n",
        "            \"INF_DETERMINISTIC\": _to_bool(env.get(\"INF_DETERMINISTIC\", str(kv[\"INF_DETERMINISTIC\"]))),\n",
        "\n",
        "            \"TICKERS\":           _to_list_csv(env.get(\"TICKERS\", \",\".join(kv[\"TICKERS\"] or [\"UNH\",\"GE\"]))),\n",
        "            \"ARTIFACTS_DIR\":     env.get(\"ARTIFACTS_DIR\", kv[\"ARTIFACTS_DIR\"] or str(project_root / \"artifacts\")),\n",
        "            \"RESULTS_ROOT\":      env.get(\"RESULTS_ROOT\",  kv[\"RESULTS_ROOT\"]  or str(project_root / \"results\")),\n",
        "\n",
        "            \"BARS_FEED\":         env.get(\"BARS_FEED\", kv[\"BARS_FEED\"]),\n",
        "            \"COOLDOWN_MIN\":      int(env.get(\"COOLDOWN_MIN\", str(kv[\"COOLDOWN_MIN\"])) or kv[\"COOLDOWN_MIN\"]),\n",
        "            \"STALE_MAX_SEC\":     int(env.get(\"STALE_MAX_SEC\", str(kv[\"STALE_MAX_SEC\"])) or kv[\"STALE_MAX_SEC\"]),\n",
        "\n",
        "            \"SIZING_MODE\":       env.get(\"SIZING_MODE\", kv[\"SIZING_MODE\"]),\n",
        "            \"WEIGHT_CAP\":        float(env.get(\"WEIGHT_CAP\",        str(kv[\"WEIGHT_CAP\"]))),\n",
        "            \"CONF_FLOOR\":        float(env.get(\"CONF_FLOOR\",        str(kv[\"CONF_FLOOR\"]))),\n",
        "            \"ENTER_CONF_MIN\":    float(env.get(\"ENTER_CONF_MIN\",    str(kv[\"ENTER_CONF_MIN\"]))),\n",
        "            \"ENTER_WEIGHT_MIN\":  float(env.get(\"ENTER_WEIGHT_MIN\",  str(kv[\"ENTER_WEIGHT_MIN\"]))),\n",
        "            \"EXIT_WEIGHT_MAX\":   float(env.get(\"EXIT_WEIGHT_MAX\",   str(kv[\"EXIT_WEIGHT_MAX\"]))),\n",
        "            \"REBALANCE_MIN_NOTIONAL\": float(env.get(\"REBALANCE_MIN_NOTIONAL\", str(kv[\"REBALANCE_MIN_NOTIONAL\"]))),\n",
        "            \"USE_FRACTIONALS\":   _to_bool(env.get(\"USE_FRACTIONALS\", str(kv[\"USE_FRACTIONALS\"]))),\n",
        "            \"SEED_FIRST_SHARE\":  _to_bool(env.get(\"SEED_FIRST_SHARE\",str(kv[\"SEED_FIRST_SHARE\"]))),\n",
        "\n",
        "            \"TAKE_PROFIT_PCT\":   float(env.get(\"TAKE_PROFIT_PCT\",   str(kv[\"TAKE_PROFIT_PCT\"]))),\n",
        "            \"STOP_LOSS_PCT\":     float(env.get(\"STOP_LOSS_PCT\",     str(kv[\"STOP_LOSS_PCT\"]))),\n",
        "\n",
        "            \"DELTA_WEIGHT_MIN\": float(env.get(\"DELTA_WEIGHT_MIN\", str(kv.get(\"DELTA_WEIGHT_MIN\", 0.02)))),\n",
        "            \"RAW_POS_MIN\":      float(env.get(\"RAW_POS_MIN\",      str(kv.get(\"RAW_POS_MIN\", 0.00)))),\n",
        "            \"RAW_NEG_MAX\":      float(env.get(\"RAW_NEG_MAX\",      str(kv.get(\"RAW_NEG_MAX\", 0.00)))),\n",
        "\n",
        "            \"STALE_BEST_WINDOW\": env.get(\"BEST_WINDOW\", kv[\"STALE_BEST_WINDOW\"]),\n",
        "        })\n",
        "        kv[\"APCA_API_KEY_ID\"]     = env.get(\"APCA_API_KEY_ID\")     or env.get(\"ALPACA_API_KEY_ID\", \"\")     or \"\"\n",
        "        kv[\"APCA_API_SECRET_KEY\"] = env.get(\"APCA_API_SECRET_KEY\") or env.get(\"ALPACA_API_SECRET_KEY\", \"\") or \"\"\n",
        "        if overrides:\n",
        "            for k, v in overrides.items():\n",
        "                if k.upper() == \"TICKERS\" and isinstance(v, str):\n",
        "                    v = _to_list_csv(v)\n",
        "                kv[k] = v\n",
        "        return cls(**kv)\n",
        "\n",
        "    def apply_to_globals(self):\n",
        "        g = globals()\n",
        "        g[\"BASE_URL\"]           = self.APCA_API_BASE_URL\n",
        "        g[\"DRY_RUN\"]            = bool(self.DRY_RUN)\n",
        "        g[\"INF_DETERMINISTIC\"]  = bool(self.INF_DETERMINISTIC)\n",
        "\n",
        "        g[\"TICKERS\"]            = list(self.TICKERS or [\"UNH\",\"GE\"])\n",
        "        g[\"ARTIFACTS_DIR\"]      = Path(self.ARTIFACTS_DIR)\n",
        "        g[\"RESULTS_ROOT\"]       = Path(self.RESULTS_ROOT)\n",
        "        g[\"RESULTS_DIR\"]        = RESULTS_ROOT / datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
        "        g[\"LATEST_DIR\"]         = RESULTS_ROOT / \"latest\"\n",
        "        for p in (ARTIFACTS_DIR, RESULTS_DIR, LATEST_DIR):\n",
        "            p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        g[\"BARS_FEED\"]          = str(self.BARS_FEED).strip()\n",
        "        g[\"COOLDOWN_MIN\"]       = int(self.COOLDOWN_MIN)\n",
        "        g[\"STALE_MAX_SEC\"]      = int(self.STALE_MAX_SEC)\n",
        "\n",
        "        g[\"SIZING_MODE\"]        = self.SIZING_MODE\n",
        "        g[\"WEIGHT_CAP\"]         = float(self.WEIGHT_CAP)\n",
        "        g[\"ENTER_CONF_MIN\"]     = float(self.ENTER_CONF_MIN)\n",
        "        g[\"ENTER_WEIGHT_MIN\"]   = float(self.ENTER_WEIGHT_MIN)\n",
        "        g[\"EXIT_WEIGHT_MAX\"]    = float(self.EXIT_WEIGHT_MAX)\n",
        "        g[\"REBALANCE_MIN_NOTIONAL\"] = float(self.REBALANCE_MIN_NOTIONAL)\n",
        "        g[\"USE_FRACTIONALS\"]    = bool(self.USE_FRACTIONALS)\n",
        "        g[\"SEED_FIRST_SHARE\"]   = bool(self.SEED_FIRST_SHARE)\n",
        "        g[\"ALLOW_SHORTS\"]       = bool(self.ALLOW_SHORTS)\n",
        "        g[\"CONF_FLOOR\"]         = float(self.CONF_FLOOR)\n",
        "        g[\"TAKE_PROFIT_PCT\"]    = float(self.TAKE_PROFIT_PCT)\n",
        "        g[\"STOP_LOSS_PCT\"]      = float(self.STOP_LOSS_PCT)\n",
        "\n",
        "        g[\"BEST_WINDOW_ENV\"]    = (self.STALE_BEST_WINDOW or None)\n",
        "\n",
        "        g[\"API_KEY\"]    = self.APCA_API_KEY_ID or \"\"\n",
        "        g[\"API_SECRET\"] = self.APCA_API_SECRET_KEY or \"\"\n",
        "\n",
        "        g[\"TRADE_LOG_CSV\"]      = RESULTS_DIR / \"trade_log_master.csv\"\n",
        "        g[\"EQUITY_LOG_CSV\"]     = RESULTS_DIR / \"equity_log.csv\"\n",
        "        g[\"PLOT_PATH\"]          = RESULTS_DIR / \"equity_curve.png\"\n",
        "        g[\"PLOT_PATH_LATEST\"]   = LATEST_DIR / \"equity_curve.png\"\n",
        "        g[\"EQUITY_LOG_LATEST\"]  = LATEST_DIR / \"equity_log.csv\"\n",
        "        g[\"TRADE_LOG_LATEST\"]   = LATEST_DIR / \"trade_log_master.csv\"\n",
        "        g[\"DELTA_WEIGHT_MIN\"]   = float(self.DELTA_WEIGHT_MIN)\n",
        "        g[\"RAW_POS_MIN\"]        = float(self.RAW_POS_MIN)\n",
        "        g[\"RAW_NEG_MAX\"]        = float(self.RAW_NEG_MAX)\n",
        "\n",
        "        os.environ[\"APCA_API_BASE_URL\"] = self.APCA_API_BASE_URL\n",
        "        os.environ[\"DRY_RUN\"]           = \"1\" if self.DRY_RUN else \"0\"\n",
        "        os.environ[\"AUTO_RUN_LIVE\"]     = \"1\" if self.AUTO_RUN_LIVE else \"0\"\n",
        "        os.environ[\"BARS_FEED\"]         = self.BARS_FEED\n",
        "\n",
        "\n",
        "def configure_knobs(overrides: Mapping[str, object] = None) -> Knobs:\n",
        "    defaults = Knobs(\n",
        "        TICKERS=_to_list_csv(os.getenv(\"TICKERS\", \"UNH,GE\")),\n",
        "        ARTIFACTS_DIR=os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")),\n",
        "        RESULTS_ROOT=os.getenv(\"RESULTS_ROOT\",  str(PROJECT_ROOT / \"results\")),\n",
        "    )\n",
        "    cfg = Knobs.from_env(defaults, PROJECT_ROOT, os.environ, overrides=overrides)\n",
        "    cfg.apply_to_globals()\n",
        "    return cfg\n",
        "\n",
        "# ---------------------------------- Utility: time ---------------------------------------------\n",
        "def now_utc() -> datetime:\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "def utc_ts(dt_like) -> int:\n",
        "    if isinstance(dt_like, (int, np.integer)):\n",
        "        return int(dt_like)\n",
        "    if isinstance(dt_like, (float, np.floating)):\n",
        "        return int(dt_like)\n",
        "    ts = pd.Timestamp(dt_like)\n",
        "    if ts.tzinfo is None:\n",
        "        ts = ts.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        ts = ts.tz_convert(\"UTC\")\n",
        "    return int(ts.value // 10**9)\n",
        "\n",
        "def utcnow_iso() -> str:\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "def _sleep_to_next_minute_block(n: int):\n",
        "    n = max(1, int(n))\n",
        "    now = now_utc()\n",
        "    base = now.replace(second=0, microsecond=0)\n",
        "    remainder = base.minute % n\n",
        "    add = n if remainder == 0 else (n - remainder)\n",
        "    next_slot = base + timedelta(minutes=add)\n",
        "    time.sleep(max(0, (next_slot - now).total_seconds()))\n",
        "\n",
        "\n",
        "# --------------------------------- CSV logging (master, optional) -----------------------------\n",
        "def ensure_trade_log_header():\n",
        "    if not TRADE_LOG_CSV.exists():\n",
        "        pd.DataFrame([{\n",
        "            \"datetime_utc\": \"\", \"ticker\": \"\", \"signal\": np.nan, \"action\": \"\",\n",
        "            \"price\": np.nan, \"equity\": np.nan, \"qty\": np.nan, \"comment\": \"\"\n",
        "        }]).iloc[0:0].to_csv(TRADE_LOG_CSV, index=False)\n",
        "\n",
        "def log_trade(ticker:str, signal:float, action:str, price:float, equity:float, qty:float=None, comment:str=\"\"):\n",
        "    ensure_trade_log_header()\n",
        "    row = {\n",
        "        \"datetime_utc\": utcnow_iso(),\n",
        "        \"ticker\": ticker,\n",
        "        \"signal\": signal,\n",
        "        \"action\": action,\n",
        "        \"price\": float(price) if price is not None else np.nan,\n",
        "        \"equity\": float(equity) if equity is not None else np.nan,\n",
        "        \"qty\": float(qty) if qty is not None else np.nan,\n",
        "        \"comment\": str(comment) if comment else \"\"\n",
        "    }\n",
        "    df_new = pd.DataFrame([row])\n",
        "    if TRADE_LOG_CSV.exists():\n",
        "        df_old = pd.read_csv(TRADE_LOG_CSV)\n",
        "        pd.concat([df_old, df_new], ignore_index=True).to_csv(TRADE_LOG_CSV, index=False)\n",
        "    else:\n",
        "        df_new.to_csv(TRADE_LOG_CSV, index=False)\n",
        "    try:\n",
        "        shutil.copy2(TRADE_LOG_CSV, TRADE_LOG_LATEST)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# --------------------------------- Alpaca API init --------------------------------------------\n",
        "def init_alpaca() -> \"tradeapi.REST\":\n",
        "    if not (globals().get(\"API_KEY\") and globals().get(\"API_SECRET\")):\n",
        "        raise RuntimeError(\"Missing Alpaca API keys (check your .env).\")\n",
        "    api = tradeapi.REST(API_KEY, API_SECRET, base_url=BASE_URL)\n",
        "    _ = api.get_account()\n",
        "    return api\n",
        "\n",
        "# ------------------------- Portfolio equity logging + metrics ---------------------------------\n",
        "def fetch_portfolio_history(period=\"1M\", timeframe=\"1Hour\", api_in=None):\n",
        "    a = api_in if api_in is not None else globals().get(\"api\", None)\n",
        "    if a is None:\n",
        "        return pd.DataFrame(columns=[\"timestamp_utc\",\"equity\"])\n",
        "    hist = a.get_portfolio_history(period=period, timeframe=timeframe)\n",
        "    return pd.DataFrame({\n",
        "        \"timestamp_utc\": pd.to_datetime(hist.timestamp, unit=\"s\", utc=True),\n",
        "        \"equity\": pd.Series(hist.equity, dtype=\"float64\")\n",
        "    }).dropna()\n",
        "\n",
        "def log_equity_snapshot(api_in=None):\n",
        "    snap = fetch_portfolio_history(period=\"1D\", timeframe=\"5Min\", api_in=api_in)\n",
        "    if snap.empty:\n",
        "        return\n",
        "    latest = snap.iloc[-1:].copy()\n",
        "    latest.rename(columns={\"timestamp_utc\": \"datetime_utc\"}, inplace=True)\n",
        "\n",
        "    if EQUITY_LOG_CSV.exists():\n",
        "        df_old = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "        merged = pd.concat([df_old, latest], ignore_index=True)\n",
        "        merged.drop_duplicates(subset=[\"datetime_utc\"], keep=\"last\").to_csv(EQUITY_LOG_CSV, index=False)\n",
        "    else:\n",
        "        latest.to_csv(EQUITY_LOG_CSV, index=False)\n",
        "\n",
        "    try:\n",
        "        shutil.copy2(EQUITY_LOG_CSV, EQUITY_LOG_LATEST)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def plot_equity_curve(from_equity_csv: bool = True):\n",
        "    with plt.ioff():\n",
        "        if from_equity_csv and EQUITY_LOG_CSV.exists():\n",
        "            df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"]).sort_values(\"datetime_utc\")\n",
        "        else:\n",
        "            df = fetch_portfolio_history(period=\"3M\", timeframe=\"1Hour\").rename(columns={\"timestamp_utc\":\"datetime_utc\"})\n",
        "        if df.empty:\n",
        "            print(\"No equity data to plot yet.\")\n",
        "            return\n",
        "        fig, ax = plt.subplots(figsize=(10, 4))\n",
        "        ax.plot(df[\"datetime_utc\"], df[\"equity\"])\n",
        "        ax.set_title(\"Portfolio Value Over Time (Paper)\")\n",
        "        ax.set_xlabel(\"Time (UTC)\")\n",
        "        ax.set_ylabel(\"Equity ($)\")\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(PLOT_PATH, bbox_inches=\"tight\")\n",
        "        fig.savefig(PLOT_PATH_LATEST, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved equity curve → {PLOT_PATH}\")\n",
        "        print(f\"Updated latest copy → {PLOT_PATH_LATEST}\")\n",
        "\n",
        "def compute_performance_metrics(df_equity: pd.DataFrame):\n",
        "    if df_equity.empty or df_equity[\"equity\"].isna().all():\n",
        "        return {\"cum_return\": np.nan, \"sharpe\": np.nan, \"max_drawdown\": np.nan}\n",
        "\n",
        "    df = df_equity.sort_values(\"datetime_utc\")\n",
        "    e = df[\"equity\"].astype(float)\n",
        "    r = e.pct_change().dropna()\n",
        "    if r.empty:\n",
        "        return {\"cum_return\": 0.0, \"sharpe\": np.nan, \"max_drawdown\": np.nan}\n",
        "\n",
        "    # estimate periods/year from median spacing\n",
        "    dt_sec = df[\"datetime_utc\"].diff().dt.total_seconds().dropna().median()\n",
        "    if not (isinstance(dt_sec, (int, float)) and dt_sec > 0):\n",
        "        periods_per_year = 252 * 78  # ~5-min bars as fallback\n",
        "    else:\n",
        "        periods_per_day = (6.5 * 3600) / dt_sec\n",
        "        periods_per_year = 252 * periods_per_day\n",
        "\n",
        "    sharpe = (r.mean() / (r.std() + 1e-12)) * math.sqrt(periods_per_year)\n",
        "    cum = (1 + r).cumprod()\n",
        "    peak = cum.cummax()\n",
        "    dd = (cum / peak - 1.0).min()\n",
        "    cum_return = e.iloc[-1] / e.iloc[0] - 1.0\n",
        "\n",
        "    return {\"cum_return\": float(cum_return), \"sharpe\": float(sharpe), \"max_drawdown\": float(dd)}\n",
        "\n",
        "\n",
        "# -------------------------------- Hook for strategy loops -------------------------------------\n",
        "def handle_signal_and_trade(ticker:str, signal:float, action:str, price:float, qty:int):\n",
        "    log_equity_snapshot()\n",
        "    eq_df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"]) if EQUITY_LOG_CSV.exists() else pd.DataFrame()\n",
        "    eq_val = float(eq_df.iloc[-1][\"equity\"]) if not eq_df.empty else np.nan\n",
        "    log_trade(ticker=ticker, signal=signal, action=action, price=price, equity=eq_val, qty=qty)\n",
        "\n",
        "# -------------------------------- Per-ticker CSV logging -------------------------------------\n",
        "def _append_csv_row(path: Path, row: dict):\n",
        "    fieldnames = list(row.keys())\n",
        "    if not path.exists():\n",
        "        with path.open(\"w\", newline=\"\") as f:\n",
        "            w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            w.writeheader()\n",
        "            w.writerow(row)\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with path.open(\"r\", newline=\"\") as f:\n",
        "            r = csv.reader(f)\n",
        "            old_header = next(r)\n",
        "    except Exception:\n",
        "        old_header = []\n",
        "\n",
        "    if old_header != fieldnames:\n",
        "        tmp = path.with_suffix(\".tmp\")\n",
        "        with tmp.open(\"w\", newline=\"\") as wf, path.open(\"r\", newline=\"\") as rf:\n",
        "            r = csv.DictReader(rf) if old_header else None\n",
        "            w = csv.DictWriter(wf, fieldnames=fieldnames)\n",
        "            w.writeheader()\n",
        "            if r:\n",
        "                for old_row in r:\n",
        "                    merged = {k: old_row.get(k, \"\") for k in fieldnames}\n",
        "                    w.writerow(merged)\n",
        "        tmp.replace(path)\n",
        "\n",
        "    with path.open(\"a\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        w.writerow(row)\n",
        "\n",
        "\n",
        "def log_trade_symbol(symbol: str,\n",
        "                     bar_time,\n",
        "                     signal: int,\n",
        "                     raw_action: float,\n",
        "                     weight: float,\n",
        "                     confidence: float,\n",
        "                     price: float,\n",
        "                     equity: float,\n",
        "                     dry_run: bool,\n",
        "                     note: str = \"\"):\n",
        "    try:\n",
        "        if bar_time is not None and not pd.isna(bar_time):\n",
        "            ts = pd.to_datetime(bar_time, utc=True)\n",
        "            bt_iso = ts.isoformat()\n",
        "            age_sec = max(0, int((now_utc() - ts).total_seconds()))\n",
        "        else:\n",
        "            bt_iso, age_sec = \"\", \"\"\n",
        "    except Exception:\n",
        "        bt_iso, age_sec = \"\", \"\"\n",
        "\n",
        "    resolved_feed = (os.getenv(\"BARS_FEED\", \"\") or \"\").strip() or \"default\"\n",
        "\n",
        "    # Derive a simple decision label (unless 'note' is explicitly set)\n",
        "    try:\n",
        "        ew = float(weight) if np.isfinite(weight) else 0.0\n",
        "        cf = float(confidence) if np.isfinite(confidence) else 0.0\n",
        "    except Exception:\n",
        "        ew, cf = 0.0, 0.0\n",
        "\n",
        "    decision = note or (\n",
        "        \"rebalance\" if (abs(ew) >= float(globals().get(\"ENTER_WEIGHT_MIN\", 0.0))\n",
        "                        and cf >= float(globals().get(\"ENTER_CONF_MIN\", 0.0)))\n",
        "        else (\"flatten\" if abs(ew) <= float(globals().get(\"EXIT_WEIGHT_MAX\", 0.0)) else \"hold\")\n",
        "    )\n",
        "\n",
        "    row = {\n",
        "        \"log_time\": now_utc().isoformat(),\n",
        "        \"symbol\": symbol,\n",
        "        \"bar_time\": bt_iso,\n",
        "        \"bar_age_sec\": age_sec,\n",
        "        \"feed\": resolved_feed,\n",
        "        \"signal\": \"BUY\" if int(signal) == 1 else \"SELL_OR_HOLD\",\n",
        "        \"raw_action\": float(raw_action) if np.isfinite(raw_action) else \"\",\n",
        "        \"weight\": float(weight) if np.isfinite(weight) else \"\",\n",
        "        \"confidence\": float(confidence) if np.isfinite(confidence) else \"\",\n",
        "        \"price\": float(price) if np.isfinite(price) else \"\",\n",
        "        \"equity\": float(equity) if np.isfinite(equity) else \"\",\n",
        "        \"dry_run\": int(bool(dry_run)),\n",
        "        \"decision\": decision,\n",
        "        \"note\": note,\n",
        "    }\n",
        "\n",
        "    _append_csv_row(RESULTS_DIR / f\"trade_log_{symbol}.csv\", row)\n",
        "\n",
        "# -------------------------------- Artifacts: picker & loaders --------------------------------\n",
        "def _extract_window_idx(path: Path) -> Optional[int]:\n",
        "    m = re.search(r\"_window(\\d+)_\", path.stem, re.IGNORECASE)\n",
        "    if not m:\n",
        "        return None\n",
        "    try:\n",
        "        return int(m.group(1))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def pick_artifacts_for_ticker(\n",
        "    ticker: str,\n",
        "    artifacts_dir: str,\n",
        "    best_window: Optional[str] = None\n",
        ") -> Dict[str, Optional[Path]]:\n",
        "    p = Path(artifacts_dir)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Artifacts directory not found: {p.resolve()}\")\n",
        "\n",
        "    models = sorted(p.glob(f\"ppo_{ticker}_window*_model*.zip\"))\n",
        "    if not models:\n",
        "        models = sorted(p.glob(f\"ppo_{ticker}_model*.zip\")) or sorted(p.glob(f\"*{ticker}*model*.zip\"))\n",
        "    if not models:\n",
        "        raise FileNotFoundError(f\"No PPO model zip found for {ticker} in {p}\")\n",
        "\n",
        "    def _model_sort_key(path: Path):\n",
        "        w = _extract_window_idx(path)\n",
        "        return (w if w is not None else -1, \" (1)\" in path.stem)\n",
        "\n",
        "    models = sorted(models, key=_model_sort_key)\n",
        "\n",
        "    chosen: Optional[Path] = None\n",
        "    if best_window:\n",
        "        chosen = next((m for m in models if f\"_window{best_window}_\" in m.stem), None)\n",
        "        if chosen is None:\n",
        "            logging.warning(\"BEST_WINDOW=%s not found; falling back to best available.\", best_window)\n",
        "\n",
        "    if chosen is None:\n",
        "        with_idx = [(m, _extract_window_idx(m)) for m in models]\n",
        "        with_idx = [(m, w) for (m, w) in with_idx if w is not None]\n",
        "        chosen = max(with_idx, key=lambda t: t[1])[0] if with_idx else models[-1]\n",
        "\n",
        "    base = chosen.stem.replace(\"_model\", \"\")\n",
        "    base_nodup = re.sub(r\"\\s\\(\\d+\\)$\", \"\", base)\n",
        "\n",
        "    vec_candidates = list(p.glob(base + \"_vecnorm*.pkl\")) + \\\n",
        "                     list(p.glob(base_nodup + \"_vecnorm*.pkl\")) + \\\n",
        "                     list(p.glob(f\"ppo_{ticker}_*_vecnorm*.pkl\"))\n",
        "    feat_candidates = list(p.glob(base + \"_features*.json\")) + \\\n",
        "                      list(p.glob(base_nodup + \"_features*.json\")) + \\\n",
        "                      list(p.glob(f\"ppo_{ticker}_*_features*.json\"))\n",
        "\n",
        "    vecnorm = sorted(vec_candidates)[0] if vec_candidates else None\n",
        "    feats   = sorted(feat_candidates)[0] if feat_candidates else None\n",
        "\n",
        "    logging.info(f\"[{ticker}] model={chosen.name} | vecnorm={bool(vecnorm)} | features={bool(feats)}\")\n",
        "    return {\"model\": chosen, \"vecnorm\": vecnorm, \"features\": feats}\n",
        "\n",
        "def load_vecnormalize(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception as e:\n",
        "        logging.warning(\"VecNormalize load failed (%s). Proceeding without it.\", e)\n",
        "        return None\n",
        "\n",
        "def load_features(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_ppo_model(model_path: Path):\n",
        "    return PPO.load(str(model_path))\n",
        "\n",
        "# ---- Cached asset flags (tradable / fractionable / shortable) ----\n",
        "@lru_cache(maxsize=256)\n",
        "def _asset_flags(symbol: str) -> Tuple[bool, bool, bool]:\n",
        "    \"\"\"\n",
        "    Return (tradable, fractionable, shortable) for a symbol.\n",
        "    Cached per-process to reduce repetitive API calls.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        _api = globals().get(\"api\") or init_alpaca()\n",
        "        a = _api.get_asset(symbol)\n",
        "        return (\n",
        "            bool(getattr(a, \"tradable\", True)),\n",
        "            bool(getattr(a, \"fractionable\", False)),\n",
        "            bool(getattr(a, \"shortable\", False)),\n",
        "        )\n",
        "    except Exception:\n",
        "        # conservative fallback\n",
        "        return True, False, False\n",
        "\n",
        "# ---------------------------- Market data + account helpers ----------------------------------\n",
        "def get_recent_bars(api, symbol: str, limit: int = 200, timeframe=TimeFrame.Minute) -> pd.DataFrame:\n",
        "    def _as_df(bars):\n",
        "        if hasattr(bars, \"df\"):\n",
        "            df = bars.df.copy()\n",
        "            if not df.empty:\n",
        "                if isinstance(df.index, pd.MultiIndex):\n",
        "                    try:\n",
        "                        df = df.xs(symbol, level=0)\n",
        "                    except KeyError:\n",
        "                        df = df.reset_index(level=0, drop=True)\n",
        "                df.index = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
        "                df = df.rename(columns={\"open\": \"Open\", \"high\": \"High\", \"low\": \"Low\",\n",
        "                                        \"close\": \"Close\", \"volume\": \"Volume\"})\n",
        "                cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in df.columns]\n",
        "                return df[cols].sort_index()\n",
        "            return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "\n",
        "        rows = []\n",
        "        for b in bars:\n",
        "            ts = getattr(b, \"t\", None)\n",
        "            ts = pd.to_datetime(ts, utc=True) if ts is not None else pd.NaT\n",
        "            rows.append({\n",
        "                \"timestamp\": ts,\n",
        "                \"Open\":   float(getattr(b, \"o\", getattr(b, \"open\",  np.nan))),\n",
        "                \"High\":   float(getattr(b, \"h\", getattr(b, \"high\",  np.nan))),\n",
        "                \"Low\":    float(getattr(b, \"l\", getattr(b, \"low\",   np.nan))),\n",
        "                \"Close\":  float(getattr(b, \"c\", getattr(b, \"close\", np.nan))),\n",
        "                \"Volume\": float(getattr(b, \"v\", getattr(b, \"volume\",np.nan))),\n",
        "            })\n",
        "        df = pd.DataFrame(rows)\n",
        "        if df.empty:\n",
        "            return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "        return df.set_index(pd.to_datetime(df[\"timestamp\"], utc=True)).drop(columns=[\"timestamp\"]).sort_index()\n",
        "\n",
        "    feed = os.getenv(\"BARS_FEED\", \"\").strip()\n",
        "    try:\n",
        "        logging.info(f\"[{symbol}] fetching {limit} {timeframe} bars (feed='{feed or 'default'}')\")\n",
        "        bars = api.get_bars(symbol, timeframe, limit=limit, feed=feed) if feed else api.get_bars(symbol, timeframe, limit=limit)\n",
        "        df = _as_df(bars)\n",
        "        if not df.empty:\n",
        "            return df\n",
        "        if feed:\n",
        "            logging.info(f\"[{symbol}] explicit feed empty; retrying with default feed\")\n",
        "            df2 = _as_df(api.get_bars(symbol, timeframe, limit=limit))\n",
        "            if not df2.empty:\n",
        "                return df2\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars(limit) failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        end_dt = datetime.now(timezone.utc).replace(microsecond=0)\n",
        "        start_dt = end_dt - timedelta(days=5)\n",
        "        end = end_dt.isoformat().replace(\"+00:00\", \"Z\")\n",
        "        start = start_dt.isoformat().replace(\"+00:00\", \"Z\")\n",
        "        logging.info(f\"[{symbol}] retry window start={start} end={end} (feed='{feed or 'default'}')\")\n",
        "        bars = api.get_bars(symbol, timeframe, start=start, end=end, feed=feed) if feed else api.get_bars(symbol, timeframe, start=start, end=end)\n",
        "        return _as_df(bars)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars(start/end) failed: {e}\")\n",
        "        return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "\n",
        "def get_account_equity(api) -> float:\n",
        "    return float(api.get_account().equity)\n",
        "\n",
        "def get_position(api, symbol: str):\n",
        "    try:\n",
        "        return api.get_position(symbol)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_position_qty(api, symbol: str):\n",
        "    pos = get_position(api, symbol)\n",
        "    if not pos:\n",
        "        return 0.0 if USE_FRACTIONALS else 0\n",
        "    try:\n",
        "        q = float(pos.qty)\n",
        "        return q if USE_FRACTIONALS else int(round(q))\n",
        "    except Exception:\n",
        "        return 0.0 if USE_FRACTIONALS else 0\n",
        "\n",
        "def get_last_price(api, symbol: str) -> float:\n",
        "    try:\n",
        "        tr = api.get_latest_trade(symbol)\n",
        "        price = getattr(tr, \"price\", None)\n",
        "        if price is None:\n",
        "            price = getattr(tr, \"p\", None)\n",
        "        if price is not None and np.isfinite(price):\n",
        "            return float(price)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        feed = os.getenv(\"BARS_FEED\", \"\").strip() or None\n",
        "        bars = api.get_bars(symbol, TimeFrame.Minute, limit=1, feed=feed) if feed else api.get_bars(symbol, TimeFrame.Minute, limit=1)\n",
        "        if hasattr(bars, \"df\"):\n",
        "            df = bars.df.copy()\n",
        "            if isinstance(df.index, pd.MultiIndex):\n",
        "                try:\n",
        "                    df = df.xs(symbol, level=0)\n",
        "                except Exception:\n",
        "                    df = df.reset_index(level=0, drop=True)\n",
        "            if not df.empty:\n",
        "                if \"close\" in df.columns: return float(df[\"close\"].iloc[-1])\n",
        "                if \"Close\" in df.columns: return float(df[\"Close\"].iloc[-1])\n",
        "        elif bars:\n",
        "            b = bars[0]\n",
        "            close = getattr(b, \"c\", getattr(b, \"close\", None))\n",
        "            if close is not None:\n",
        "                return float(close)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_last_price via bars failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        qt = api.get_latest_quote(symbol)\n",
        "        ap = getattr(qt, \"ap\", None) or getattr(qt, \"ask_price\", None)\n",
        "        bp = getattr(qt, \"bp\", None) or getattr(qt, \"bid_price\", None)\n",
        "        if ap and bp:\n",
        "            return float((float(ap) + float(bp)) / 2.0)\n",
        "        if ap: return float(ap)\n",
        "        if bp: return float(bp)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        pos = api.get_position(symbol)\n",
        "        return float(pos.avg_entry_price)\n",
        "    except Exception:\n",
        "        return float(\"nan\")\n",
        "\n",
        "def cancel_open_symbol_orders(api, symbol: str):\n",
        "    try:\n",
        "        for o in api.list_orders(status=\"open\"):\n",
        "            if o.symbol == symbol:\n",
        "                api.cancel_order(o.id)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] cancel orders failed: {e}\")\n",
        "\n",
        "def to_2dp_str(x) -> str:\n",
        "    return format(Decimal(str(x)).quantize(Decimal(\"0.01\"), rounding=ROUND_HALF_UP), \"f\")\n",
        "\n",
        "def to_6dp_str(x) -> str:\n",
        "    return format(Decimal(str(x)).quantize(Decimal(\"0.000001\"), rounding=ROUND_DOWN), \"f\")\n",
        "\n",
        "def market_order(api, symbol: str, side: str, qty=None, notional: float=None):\n",
        "    if qty is not None and notional is not None:\n",
        "        logging.warning(f\"[{symbol}] Both qty and notional provided; preferring notional and ignoring qty.\")\n",
        "        qty = None\n",
        "\n",
        "    if qty is None and notional is None:\n",
        "        logging.warning(f\"[{symbol}] No order size provided; skipping.\")\n",
        "        return None\n",
        "    if qty is not None:\n",
        "        try:\n",
        "            if float(qty) <= 0:\n",
        "                logging.warning(f\"[{symbol}] Non-positive qty ({qty}); skipping.\")\n",
        "                return None\n",
        "        except Exception:\n",
        "            pass\n",
        "    if notional is not None and notional <= 0:\n",
        "        logging.warning(f\"[{symbol}] Non-positive notional (${notional}); skipping.\")\n",
        "        return None\n",
        "\n",
        "    if DRY_RUN:\n",
        "        logging.info(\n",
        "            f\"[DRY_RUN] Would submit {side} \"\n",
        "            f\"{('notional=$' + to_2dp_str(notional)) if notional is not None else ('qty=' + str(qty))} \"\n",
        "            f\"{symbol} (market, day)\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        qty_arg = None\n",
        "        if qty is not None:\n",
        "            qty_arg = to_6dp_str(qty) if USE_FRACTIONALS else int(qty)\n",
        "        notional_arg = to_2dp_str(notional) if notional is not None else None\n",
        "\n",
        "        o = api.submit_order(\n",
        "            symbol=symbol,\n",
        "            side=side,\n",
        "            type=\"market\",\n",
        "            time_in_force=\"day\",\n",
        "            qty=qty_arg,\n",
        "            notional=notional_arg,\n",
        "        )\n",
        "\n",
        "        logging.info(\n",
        "            f\"[{symbol}] Submitted {side} \"\n",
        "            f\"{('notional=$' + notional_arg) if notional_arg else ('qty=' + str(qty_arg))}\"\n",
        "        )\n",
        "        return o\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[{symbol}] submit_order failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def market_order_to_qty(api, symbol: str, side: str, qty: Union[int, float, str]):\n",
        "    if USE_FRACTIONALS:\n",
        "        qf = float(qty)\n",
        "        q = int(round(qf)) if abs(qf - round(qf)) < 1e-8 else to_6dp_str(qf)\n",
        "    else:\n",
        "        q = int(qty)\n",
        "    return market_order(api, symbol, side=side, qty=q)\n",
        "\n",
        "\n",
        "# ----------------------------- Sizing / risk + (un)flatten / rebalance ------------------------\n",
        "def action_to_weight(action) -> Tuple[float, float, float]:\n",
        "    a = float(np.array(action).squeeze())\n",
        "    conf = float(abs(np.tanh(a)))\n",
        "    if a == 0:\n",
        "        return 0.0, conf, a\n",
        "    if a < 0:\n",
        "        if not globals().get(\"ALLOW_SHORTS\", False):\n",
        "            return 0.0, conf, a\n",
        "        w = -WEIGHT_CAP * conf if SIZING_MODE == \"linear\" else (\n",
        "            0.0 if conf < CONF_FLOOR else -WEIGHT_CAP * (conf - CONF_FLOOR) / (1.0 - CONF_FLOOR)\n",
        "        )\n",
        "        w = max(-WEIGHT_CAP, min(0.0, float(w)))\n",
        "        return w, conf, a\n",
        "    # a > 0 (long)\n",
        "    if SIZING_MODE == \"linear\":\n",
        "        w = WEIGHT_CAP * conf\n",
        "    else:\n",
        "        w = 0.0 if conf < CONF_FLOOR else WEIGHT_CAP * (conf - CONF_FLOOR) / (1.0 - CONF_FLOOR)\n",
        "    w = max(0.0, min(WEIGHT_CAP, float(w)))\n",
        "    return w, conf, a\n",
        "\n",
        "\n",
        "def compute_target_qty_by_cash(equity: float, price: float, target_weight: float, api=None) -> int:\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        return 0\n",
        "    if api:\n",
        "        acct = api.get_account()\n",
        "        budget = float(getattr(acct, \"buying_power\", getattr(acct, \"cash\", equity)))\n",
        "    else:\n",
        "        budget = equity\n",
        "\n",
        "    target_notional = equity * float(target_weight)           # can be negative\n",
        "    allowed = min(budget, abs(target_notional))\n",
        "    qty = int(allowed // price)\n",
        "\n",
        "    if target_weight > 0:\n",
        "        return max(0, qty)\n",
        "    else:\n",
        "        # negative qty means short\n",
        "        return min(0, -qty) if globals().get(\"ALLOW_SHORTS\", False) else 0\n",
        "\n",
        "\n",
        "def flatten_symbol(api, symbol: str):\n",
        "    qty = get_position_qty(api, symbol)\n",
        "    if (USE_FRACTIONALS and abs(qty) < 1e-8) or (not USE_FRACTIONALS and int(qty) == 0):\n",
        "        return\n",
        "    cancel_open_symbol_orders(api, symbol)\n",
        "    if DRY_RUN:\n",
        "        logging.info(f\"[DRY_RUN] Would close position {symbol}\")\n",
        "        return\n",
        "    try:\n",
        "        api.close_position(symbol)\n",
        "        logging.info(f\"[{symbol}] close_position submitted\")\n",
        "    except Exception:\n",
        "        side = \"sell\" if qty > 0 else \"buy\"\n",
        "        market_order_to_qty(api, symbol, side, abs(qty))\n",
        "\n",
        "def rebalance_to_weight(api, symbol: str, equity: float, target_weight: float):\n",
        "    \"\"\"\n",
        "    Rebalance toward target_weight.\n",
        "    - Uses per-symbol fractionals flag (asset.fractionable).\n",
        "    - Avoids fractional *buys* when covering integer shorts.\n",
        "    - Checks tradable/shortable constraints.\n",
        "    \"\"\"\n",
        "    price = get_last_price(api, symbol)\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        logging.warning(f\"[{symbol}] Price unavailable; skipping rebalance this cycle.\")\n",
        "        return\n",
        "\n",
        "    tradable, fractionable, shortable = _asset_flags(symbol)\n",
        "    if not tradable:\n",
        "        logging.info(f\"[{symbol}] Not tradable; skipping rebalance.\")\n",
        "        return\n",
        "    use_fractionals = bool(USE_FRACTIONALS and fractionable)\n",
        "\n",
        "    have_qty        = get_position_qty(api, symbol)          # signed (negative if short)\n",
        "    have_notional   = have_qty * price                       # current exposure\n",
        "    target_notional = equity * float(target_weight)          # desired exposure\n",
        "    delta_notional  = target_notional - have_notional        # change in exposure\n",
        "\n",
        "    # Skip tiny changes\n",
        "    if abs(delta_notional) < 1e-9:\n",
        "        return\n",
        "    if equity > 0:\n",
        "        delta_weight = abs(delta_notional) / equity\n",
        "        if delta_weight < float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)):\n",
        "            return\n",
        "\n",
        "    if use_fractionals:\n",
        "        dn = round_to_cents(abs(delta_notional))\n",
        "        if dn < float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 0.0)):\n",
        "            return\n",
        "\n",
        "        side = \"buy\" if delta_notional > 0 else \"sell\"\n",
        "        shorting = (target_notional < 0) and (side == \"sell\")  # increasing a short\n",
        "        covering = (have_qty < 0) and (side == \"buy\")         # reducing a short\n",
        "\n",
        "        if shorting:\n",
        "            if not shortable:\n",
        "                logging.info(f\"[{symbol}] Not shortable; skipping rebalance toward short.\")\n",
        "                return\n",
        "            qty = max(1, int(math.floor(dn / price))) if np.isfinite(price) and price > 0 else 1\n",
        "            market_order_to_qty(api, symbol, side=\"sell\", qty=qty)\n",
        "            return\n",
        "\n",
        "        if covering:\n",
        "            # Covering shorts: buy whole shares (avoid fractional buy vs integer short)\n",
        "            qty = max(1, int(math.ceil(dn / price))) if np.isfinite(price) and price > 0 else 1\n",
        "            qty = min(int(abs(have_qty)), qty) if have_qty < 0 else qty\n",
        "            market_order_to_qty(api, symbol, side=\"buy\", qty=qty)\n",
        "            return\n",
        "\n",
        "        # Long exposure changes can safely use notional\n",
        "        market_order(api, symbol, side=side, notional=dn)\n",
        "        return\n",
        "\n",
        "    # ---- Non-fractional mode (whole shares only) ----\n",
        "    want_qty  = compute_target_qty_by_cash(equity, price, target_weight, api)\n",
        "    delta_qty = want_qty - have_qty\n",
        "    if delta_qty == 0:\n",
        "        return\n",
        "\n",
        "    approx_delta_notional = abs(delta_qty) * price\n",
        "    if equity > 0 and approx_delta_notional / equity < float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)):\n",
        "        return\n",
        "    if approx_delta_notional < float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 0.0)):\n",
        "        return\n",
        "\n",
        "    side = \"buy\" if delta_qty > 0 else \"sell\"\n",
        "    shorting = (target_notional < 0) and (side == \"sell\")\n",
        "    if shorting and not shortable:\n",
        "        logging.info(f\"[{symbol}] Not shortable; skipping rebalance toward short.\")\n",
        "        return\n",
        "\n",
        "    market_order_to_qty(api, symbol, side=side, qty=int(abs(delta_qty)))\n",
        "\n",
        "def check_tp_sl_and_maybe_flatten(api, symbol: str) -> bool:\n",
        "    if TAKE_PROFIT_PCT <= 0 and STOP_LOSS_PCT <= 0:\n",
        "        return False\n",
        "    pos = get_position(api, symbol)\n",
        "    if not pos:\n",
        "        return False\n",
        "    try:\n",
        "        plpc = float(pos.unrealized_plpc)\n",
        "    except Exception:\n",
        "        return False\n",
        "    if TAKE_PROFIT_PCT > 0 and plpc >= TAKE_PROFIT_PCT:\n",
        "        logging.info(f\"[{symbol}] TP hit ({plpc:.4f} >= {TAKE_PROFIT_PCT:.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    if STOP_LOSS_PCT > 0 and plpc <= -abs(STOP_LOSS_PCT):\n",
        "        logging.info(f\"[{symbol}] SL hit ({plpc:.4f} <= {-abs(STOP_LOSS_PCT):.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# ----------------------------- Inference helpers / features -----------------------------------\n",
        "def expected_obs_shape(model, vecnorm) -> Optional[tuple]:\n",
        "    for src in (vecnorm, model):\n",
        "        try:\n",
        "            shp = tuple(src.observation_space.shape)\n",
        "            if shp:\n",
        "                return shp\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def compute_art_feat_order(features_hint: Any, df: pd.DataFrame) -> List[str]:\n",
        "    if features_hint is None:\n",
        "        return [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    feats = features_hint.get(\"features\", features_hint) if isinstance(features_hint, dict) else list(features_hint)\n",
        "    drop = {\"datetime\", \"symbol\", \"target\", \"return\"}\n",
        "    return [c for c in feats if c not in drop and (c in df.columns) and pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "def build_obs_from_row(row: pd.Series, order: List[str]) -> np.ndarray:\n",
        "    vals = []\n",
        "    for c in order:\n",
        "        v = row.get(c, np.nan)\n",
        "        vals.append(0.0 if (pd.isna(v) or v is None or v is False) else float(v))\n",
        "    return np.array(vals, dtype=np.float32)\n",
        "\n",
        "def _pick_columns_for_channels(features_hint: Any, df: pd.DataFrame, channels: int) -> List[str]:\n",
        "    numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    cols: List[str] = []\n",
        "    if isinstance(features_hint, dict) and \"features\" in features_hint:\n",
        "        cand = [c for c in features_hint[\"features\"] if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
        "        if len(cand) >= channels:\n",
        "            cols = cand[:channels]\n",
        "    if not cols:\n",
        "        pref = [\"Close\", \"Volume\", \"Adj Close\", \"Open\", \"High\", \"Low\"]\n",
        "        cols = [c for c in pref if c in numeric]\n",
        "        cols += [c for c in numeric if c not in cols]\n",
        "        cols = cols[:channels]\n",
        "    if len(cols) < channels and cols:\n",
        "        while len(cols) < channels:\n",
        "            cols.append(cols[-1])\n",
        "    return cols[:channels]\n",
        "\n",
        "def add_regime(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"Vol20\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Ret20\"] = df[\"Close\"].pct_change(20)\n",
        "    vol_hi   = (df[\"Vol20\"] > df[\"Vol20\"].median()).astype(int)\n",
        "    trend_hi = (df[\"Ret20\"].abs() > df[\"Ret20\"].abs().median()).astype(int)\n",
        "    df[\"Regime4\"] = vol_hi * 2 + trend_hi\n",
        "    return df\n",
        "\n",
        "def denoise_wavelet(series: pd.Series, wavelet: str = \"db1\", level: int = 2) -> pd.Series:\n",
        "    try:\n",
        "        import pywt\n",
        "    except Exception:\n",
        "        return pd.Series(series).astype(float).ewm(span=5, adjust=False).mean()\n",
        "    s = pd.Series(series).astype(float).ffill().bfill()\n",
        "    arr = s.to_numpy()\n",
        "    try:\n",
        "        w = pywt.Wavelet(wavelet)\n",
        "        maxlvl = pywt.dwt_max_level(len(arr), w.dec_len)\n",
        "        lvl = int(max(0, min(level, maxlvl)))\n",
        "        if lvl < 1:\n",
        "            return s\n",
        "        coeffs = pywt.wavedec(arr, w, mode=\"symmetric\", level=lvl)\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = np.zeros_like(coeffs[i])\n",
        "        rec = pywt.waverec(coeffs, w, mode=\"symmetric\")\n",
        "        return pd.Series(rec[:len(arr)], index=s.index)\n",
        "    except Exception:\n",
        "        return s.ewm(span=5, adjust=False).mean()\n",
        "\n",
        "def add_features_live(\n",
        "    df: pd.DataFrame,\n",
        "    use_sentiment: bool = False,\n",
        "    rsi_wilder: bool = True,\n",
        "    atr_wilder: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    df = df.copy().sort_index()\n",
        "    cols_ci = {c.lower(): c for c in df.columns}\n",
        "    rename = {}\n",
        "    for final, alts in {\n",
        "        \"Open\": [\"open\"], \"High\": [\"high\"], \"Low\": [\"low\"],\n",
        "        \"Close\": [\"close\",\"close*\",\"last\"], \"Adj Close\":[\"adj close\",\"adj_close\",\"adjclose\",\"adjusted close\"],\n",
        "        \"Volume\":[\"volume\",\"vol\"]\n",
        "    }.items():\n",
        "        for a in [final.lower()] + alts:\n",
        "            if a in cols_ci:\n",
        "                rename[cols_ci[a]] = final\n",
        "                break\n",
        "    df = df.rename(columns=rename)\n",
        "    if \"Adj Close\" not in df.columns and \"Close\" in df.columns:\n",
        "        df[\"Adj Close\"] = df[\"Close\"]\n",
        "\n",
        "    df[\"SMA_20\"] = df[\"Close\"].rolling(20).mean()\n",
        "    df[\"STD_20\"] = df[\"Close\"].rolling(20).std()\n",
        "    df[\"Upper_Band\"] = df[\"SMA_20\"] + 2 * df[\"STD_20\"]\n",
        "    df[\"Lower_Band\"] = df[\"SMA_20\"] - 2 * df[\"STD_20\"]\n",
        "\n",
        "    df[\"Lowest_Low\"]   = df[\"Low\"].rolling(14).min()\n",
        "    df[\"Highest_High\"] = df[\"High\"].rolling(14).max()\n",
        "    denom = (df[\"Highest_High\"] - df[\"Lowest_Low\"]).replace(0, np.nan)\n",
        "    df[\"Stoch\"] = ((df[\"Close\"] - df[\"Lowest_Low\"]) / denom) * 100\n",
        "\n",
        "    df[\"ROC\"] = df[\"Close\"].pct_change(10)\n",
        "    sign = np.sign(df[\"Close\"].diff().fillna(0))\n",
        "    df[\"OBV\"] = (sign * df[\"Volume\"].fillna(0)).cumsum()\n",
        "\n",
        "    tp = (df[\"High\"] + df[\"Low\"] + df[\"Close\"]) / 3.0\n",
        "    sma_tp = tp.rolling(20).mean()\n",
        "    md = (tp - sma_tp).abs().rolling(20).mean().replace(0, np.nan)\n",
        "    df[\"CCI\"] = (tp - sma_tp) / (0.015 * md)\n",
        "\n",
        "    df[\"EMA_10\"] = df[\"Close\"].ewm(span=10, adjust=False).mean()\n",
        "    df[\"EMA_50\"] = df[\"Close\"].ewm(span=50, adjust=False).mean()\n",
        "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "    df[\"MACD_Line\"]   = ema12 - ema26\n",
        "    df[\"MACD_Signal\"] = df[\"MACD_Line\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    d = df[\"Close\"].diff()\n",
        "    gain = d.clip(lower=0)\n",
        "    loss = (-d.clip(upper=0))\n",
        "    if rsi_wilder:\n",
        "        avg_gain = gain.ewm(alpha=1/14, adjust=False).mean()\n",
        "        avg_loss = loss.ewm(alpha=1/14, adjust=False).mean()\n",
        "    else:\n",
        "        avg_gain = gain.rolling(14).mean()\n",
        "        avg_loss = loss.rolling(14).mean()\n",
        "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
        "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    tr = pd.concat([\n",
        "        (df[\"High\"] - df[\"Low\"]),\n",
        "        (df[\"High\"] - df[\"Close\"].shift()).abs(),\n",
        "        (df[\"Low\"]  - df[\"Close\"].shift()).abs(),\n",
        "    ], axis=1).max(axis=1)\n",
        "    df[\"ATR\"] = tr.ewm(alpha=1/14, adjust=False).mean() if atr_wilder else tr.rolling(14).mean()\n",
        "\n",
        "    df[\"Volatility\"]     = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Denoised_Close\"] = denoise_wavelet(df[\"Close\"])\n",
        "\n",
        "    df = add_regime(df)\n",
        "    df[\"SentimentScore\"] = (df.get(\"SentimentScore\", 0.0) if use_sentiment else 0.0)\n",
        "    df[\"Delta\"] = df[\"Close\"].pct_change(1).fillna(0.0)\n",
        "    df[\"Gamma\"] = df[\"Delta\"].diff().fillna(0.0)\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "def prepare_observation_from_bars(\n",
        "    bars_df: pd.DataFrame,\n",
        "    features_hint: Any = None,\n",
        "    min_required_rows: int = 60,\n",
        "    expected_shape: Optional[tuple] = None,\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    feats_df = add_features_live(bars_df).replace([np.inf, -np.inf], np.nan)\n",
        "    ts = pd.Timestamp.utcnow()\n",
        "    try:\n",
        "        idx_ts = pd.Timestamp(feats_df.index[-1])\n",
        "        ts = idx_ts.tz_convert(\"UTC\") if idx_ts.tzinfo else idx_ts.tz_localize(\"UTC\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if expected_shape is not None:\n",
        "        if len(expected_shape) == 2:\n",
        "            lookback, channels = int(expected_shape[0]), int(expected_shape[1])\n",
        "            cols = _pick_columns_for_channels(features_hint, feats_df, channels)\n",
        "            window_df = feats_df[cols].tail(lookback).fillna(0.0)\n",
        "            arr = window_df.to_numpy(dtype=np.float32)\n",
        "            if arr.shape[0] < lookback:\n",
        "                pad_rows = lookback - arr.shape[0]\n",
        "                arr = np.vstack([np.zeros((pad_rows, channels), dtype=np.float32), arr])\n",
        "            arr = arr[-lookback:, :channels]\n",
        "            return arr.reshape(lookback, channels), int(ts.timestamp())\n",
        "\n",
        "        elif len(expected_shape) == 1:\n",
        "            n = int(expected_shape[0])\n",
        "            cand = compute_art_feat_order(features_hint, feats_df)\n",
        "            if len(feats_df) < max(20, min_required_rows):\n",
        "                raise ValueError(f\"Not enough bars to compute features robustly (have {len(feats_df)}).\")\n",
        "            last = feats_df.iloc[-1]\n",
        "            vals = []\n",
        "            for c in cand[:n]:\n",
        "                v = last.get(c, np.nan)\n",
        "                vals.append(0.0 if (pd.isna(v) or v is None) else float(v))\n",
        "            if len(vals) < n:\n",
        "                vals += [0.0] * (n - len(vals))\n",
        "            return np.asarray(vals, dtype=np.float32), int(ts.timestamp())\n",
        "\n",
        "    order = compute_art_feat_order(features_hint, feats_df)\n",
        "    if not order:\n",
        "        raise ValueError(\"No usable features after resolving artifact order.\")\n",
        "    feats_df = feats_df.dropna(subset=order)\n",
        "    if len(feats_df) < max(20, min_required_rows):\n",
        "        raise ValueError(f\"Not enough bars to compute features robustly (have {len(feats_df)}).\")\n",
        "    last = feats_df.iloc[-1]\n",
        "    obs = build_obs_from_row(last, order)\n",
        "    return obs.astype(np.float32), int(ts.timestamp())\n",
        "\n",
        "# -------------------------------- Live step & loop --------------------------------------------\n",
        "def ensure_market_open(api) -> bool:\n",
        "    try:\n",
        "        return bool(api.get_clock().is_open)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _sleep_until_open(api):\n",
        "    try:\n",
        "        clock = api.get_clock()\n",
        "        if getattr(clock, \"is_open\", False):\n",
        "            return\n",
        "        nxt = pd.to_datetime(getattr(clock, \"next_open\"), utc=True, errors=\"coerce\")\n",
        "        if pd.isna(nxt):\n",
        "            time.sleep(60)\n",
        "            return\n",
        "        wait = max(1, int((nxt - now_utc()).total_seconds()))\n",
        "        logging.info(\"Market closed. Sleeping %ds until next open.\", wait)\n",
        "        time.sleep(wait)\n",
        "    except Exception:\n",
        "        time.sleep(60)\n",
        "\n",
        "def infer_target_weight(model: PPO, vecnorm: Optional[VecNormalize], obs: np.ndarray) -> Tuple[float, float, float]:\n",
        "    x = obs\n",
        "    if vecnorm is not None and hasattr(vecnorm, \"normalize_obs\"):\n",
        "        try:\n",
        "            x = vecnorm.normalize_obs(x)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"VecNormalize.normalize_obs failed; using raw obs. Err: {e}\")\n",
        "    action, _ = model.predict(x, deterministic=INF_DETERMINISTIC)\n",
        "    return action_to_weight(action)\n",
        "\n",
        "def maybe_patch_stale_with_latest_trade(api, symbol: str, bars_df: pd.DataFrame, max_age_sec: int = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    If the last minute bar is older than STALE_MAX_SEC (or max_age_sec) but a fresher latest trade exists,\n",
        "    append a synthetic bar using the trade price (O=H=L=C=last trade, V=0).\n",
        "    \"\"\"\n",
        "    if bars_df.empty:\n",
        "        return bars_df\n",
        "    max_age_sec = max_age_sec or int(globals().get(\"STALE_MAX_SEC\", 600))\n",
        "    try:\n",
        "        last_ts = pd.Timestamp(bars_df.index[-1])\n",
        "        last_ts = last_ts.tz_convert(\"UTC\") if last_ts.tzinfo else last_ts.tz_localize(\"UTC\")\n",
        "        age_sec = int((now_utc() - last_ts).total_seconds())\n",
        "        if age_sec <= max_age_sec:\n",
        "            return bars_df\n",
        "\n",
        "        lt = api.get_latest_trade(symbol)\n",
        "        price = float(getattr(lt, \"price\", getattr(lt, \"p\", float(\"nan\"))))\n",
        "        ts = pd.to_datetime(getattr(lt, \"timestamp\", getattr(lt, \"t\", None)), utc=True)\n",
        "        if not (pd.notna(ts) and np.isfinite(price)):\n",
        "            return bars_df\n",
        "\n",
        "        lt_age = int((now_utc() - ts).total_seconds())\n",
        "        if lt_age > max_age_sec:\n",
        "            return bars_df\n",
        "\n",
        "        synth_time = max(last_ts + pd.Timedelta(minutes=1), ts.floor(\"min\"))\n",
        "        row = pd.DataFrame(\n",
        "            {\"Open\":[price], \"High\":[price], \"Low\":[price], \"Close\":[price], \"Volume\":[0.0]},\n",
        "            index=pd.DatetimeIndex([synth_time], tz=\"UTC\")\n",
        "        )\n",
        "        patched = pd.concat([bars_df, row]).sort_index()\n",
        "        patched = patched[~patched.index.duplicated(keep=\"last\")]\n",
        "        logging.info(f\"[{symbol}] Patched stale bars with synthetic trade bar @ {synth_time.isoformat()} px={price:.2f}\")\n",
        "        return patched\n",
        "    except Exception as e:\n",
        "        logging.debug(f\"[{symbol}] maybe_patch_stale_with_latest_trade failed: {e}\")\n",
        "        return bars_df\n",
        "\n",
        "def run_live_once_for_symbol(\n",
        "    api,\n",
        "    symbol: str,\n",
        "    model: PPO,\n",
        "    vecnorm: Optional[VecNormalize],\n",
        "    features_hint: Optional[dict] = None,\n",
        "):\n",
        "    shape = expected_obs_shape(model, vecnorm)\n",
        "\n",
        "    bars_df = get_recent_bars(api, symbol, limit=200, timeframe=TimeFrame.Minute)\n",
        "    if bars_df.empty:\n",
        "        logging.warning(f\"[{symbol}] No recent bars; skipping.\")\n",
        "        return\n",
        "\n",
        "    # Freshness + context\n",
        "    last_ts = pd.Timestamp(bars_df.index[-1])\n",
        "    if last_ts.tzinfo is None:\n",
        "        last_ts = last_ts.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        last_ts = last_ts.tz_convert(\"UTC\")\n",
        "    age = int((now_utc() - last_ts).total_seconds())\n",
        "    logging.info(f\"[{symbol}] last bar: {last_ts} | age={age}s | feed='{BARS_FEED or 'default'}'\")\n",
        "\n",
        "    bars_df = maybe_patch_stale_with_latest_trade(api, symbol, bars_df)\n",
        "    # Build observation (robust to early-session / data hiccups)\n",
        "    try:\n",
        "        obs, obs_ts = prepare_observation_from_bars(\n",
        "            bars_df,\n",
        "            features_hint=features_hint,\n",
        "            min_required_rows=60,\n",
        "            expected_shape=shape,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.info(f\"[{symbol}] Could not build observation this cycle: {e}\")\n",
        "        try:\n",
        "            eq = get_account_equity(api)\n",
        "            px = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api, symbol)\n",
        "        except Exception:\n",
        "            eq, px = float(\"nan\"), float(\"nan\")\n",
        "        log_trade_symbol(\n",
        "            symbol,\n",
        "            bars_df.index[-1] if not bars_df.empty else pd.NaT,\n",
        "            0, 0.0, 0.0, 0.0, px, eq, DRY_RUN,\n",
        "            note=\"obs_build_error\"\n",
        "        )\n",
        "        return\n",
        "    # Stale observation guard\n",
        "    if utc_ts(now_utc()) - obs_ts > STALE_MAX_SEC:\n",
        "        logging.info(f\"[{symbol}] Stale obs ...; skip.\")\n",
        "        try:\n",
        "            eq = get_account_equity(api)\n",
        "            px = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api, symbol)\n",
        "        except Exception:\n",
        "            eq, px = float(\"nan\"), float(\"nan\")\n",
        "        log_trade_symbol(\n",
        "            symbol,\n",
        "            bars_df.index[-1] if not bars_df.empty else pd.NaT,\n",
        "            0, 0.0, 0.0, 0.0, px, eq, DRY_RUN,\n",
        "            note=\"skip_stale\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    # TP/SL guard (may flatten and exit this cycle)\n",
        "    if check_tp_sl_and_maybe_flatten(api, symbol):\n",
        "        return\n",
        "\n",
        "    # --- inference ---\n",
        "    target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "    eq   = get_account_equity(api)\n",
        "    px   = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api, symbol)\n",
        "    have = get_position_qty(api, symbol)\n",
        "\n",
        "    logging.info(\n",
        "        f\"[{symbol}] raw={raw:.4f} conf={conf:.3f} → target_w={target_w:.4f} \"\n",
        "        f\"px=${px:.2f} eq=${eq:,.2f} have={have}\"\n",
        "    )\n",
        "    logging.debug(\n",
        "        f\"[{symbol}] Gates: conf≥ENTER_CONF_MIN? {conf>=ENTER_CONF_MIN} | \"\n",
        "        f\"|target_w|≥ENTER_WEIGHT_MIN? {abs(target_w)>=ENTER_WEIGHT_MIN} | \"\n",
        "        f\"|target_w|≤EXIT_WEIGHT_MAX? {abs(target_w)<=EXIT_WEIGHT_MAX} | \"\n",
        "        f\"Δw floor (DELTA_WEIGHT_MIN): {float(globals().get('DELTA_WEIGHT_MIN',0.0))}\"\n",
        "    )\n",
        "\n",
        "    # Raw gates\n",
        "    RAW_POS_MIN = float(globals().get(\"RAW_POS_MIN\", 0.0))\n",
        "    if target_w > 0 and raw < RAW_POS_MIN:\n",
        "        logging.info(f\"[{symbol}] Raw {raw:.4f} < RAW_POS_MIN {RAW_POS_MIN:.4f}; no action.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"raw_gate_long\")\n",
        "        return\n",
        "\n",
        "    RAW_NEG_GATE = float(globals().get(\"RAW_NEG_MAX\", 0.0))\n",
        "    if target_w < 0 and abs(raw) < RAW_NEG_GATE:\n",
        "        logging.info(f\"[{symbol}] |raw| {abs(raw):.4f} < RAW_NEG_GATE {RAW_NEG_GATE:.4f}; no action.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"raw_gate_short\")\n",
        "        return\n",
        "\n",
        "    # Flatten FIRST if near-flat and we have a position\n",
        "    pos = get_position(api, symbol)\n",
        "    if abs(target_w) <= EXIT_WEIGHT_MAX and pos:\n",
        "        logging.info(f\"[{symbol}] Model near-flat (≤{EXIT_WEIGHT_MAX:.3f}); flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"flatten\")\n",
        "        return\n",
        "\n",
        "    # Low confidence AND near-flat → do nothing\n",
        "    if conf < ENTER_CONF_MIN and abs(target_w) <= EXIT_WEIGHT_MAX:\n",
        "        logging.info(f\"[{symbol}] Below conf/near-flat gates; no action.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"no_action\")\n",
        "        return\n",
        "\n",
        "    # Entry / rebalance\n",
        "    if abs(target_w) >= ENTER_WEIGHT_MIN and conf >= ENTER_CONF_MIN:\n",
        "        # Seed a brand-new position (long OR short)\n",
        "        if SEED_FIRST_SHARE and have == 0:\n",
        "            seed_notional = max(REBALANCE_MIN_NOTIONAL, round_to_cents(px if np.isfinite(px) else 1.00))\n",
        "            side = \"buy\" if target_w > 0 else \"sell\"\n",
        "\n",
        "            # If seeding a short, verify shortable\n",
        "            if side == \"sell\":\n",
        "                try:\n",
        "                    a = api.get_asset(symbol)\n",
        "                    if not getattr(a, \"shortable\", False):\n",
        "                        logging.info(f\"[{symbol}] Not shortable; skipping seed short.\")\n",
        "                        log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"not_shortable_seed\")\n",
        "                        return\n",
        "                except Exception as e:\n",
        "                    logging.debug(f\"[{symbol}] get_asset shortable check failed: {e}\")\n",
        "\n",
        "            if target_w > 0 and USE_FRACTIONALS:\n",
        "                market_order(api, symbol, side=side, notional=seed_notional)\n",
        "            else:\n",
        "                market_order_to_qty(api, symbol, side=side, qty=1)\n",
        "\n",
        "            log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"seed_open\")\n",
        "            return\n",
        "\n",
        "        # Normal rebalance toward target weight\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"rebalance\")\n",
        "        rebalance_to_weight(api, symbol, eq, target_w)\n",
        "    else:\n",
        "        # Did not meet entry thresholds → hold\n",
        "        logging.info(f\"[{symbol}] target_w ({target_w:.4f}) or conf ({conf:.3f}) below entry gates; hold.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"hold\")\n",
        "\n",
        "def run_live(tickers: List[str]):\n",
        "    api_local = init_alpaca()\n",
        "\n",
        "    per_ticker: Dict[str, Tuple[PPO, Optional[VecNormalize], Optional[dict]]] = {}\n",
        "    best = (globals().get(\"BEST_WINDOW_ENV\") or None)\n",
        "\n",
        "    for t in tickers:\n",
        "        try:\n",
        "            picks   = pick_artifacts_for_ticker(t, os.getenv(\"ARTIFACTS_DIR\", str(ARTIFACTS_DIR)), best_window=best)\n",
        "            model   = load_ppo_model(picks[\"model\"])\n",
        "            vecnorm = load_vecnormalize(picks.get(\"vecnorm\"))\n",
        "            if vecnorm and hasattr(vecnorm, \"training\"): vecnorm.training = False\n",
        "            if vecnorm and hasattr(vecnorm, \"norm_reward\"): vecnorm.norm_reward = False\n",
        "            feats   = load_features(picks.get(\"features\"))\n",
        "            per_ticker[t] = (model, vecnorm, feats)\n",
        "            logging.info(\"[%s] Artifacts loaded and ready.\", t)\n",
        "        except Exception as e:\n",
        "            logging.exception(\"[%s] Failed to load artifacts: %s\", t, e)\n",
        "\n",
        "    if not per_ticker:\n",
        "        raise RuntimeError(\"No models loaded for any ticker. Check artifacts directory and names.\")\n",
        "\n",
        "    loaded_syms = list(per_ticker.keys())\n",
        "    logging.info(\"Starting live execution for (loaded): %s\", loaded_syms)\n",
        "\n",
        "    last_exec_at = now_utc() - timedelta(minutes=COOLDOWN_MIN)\n",
        "    cycle = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            if not ensure_market_open(api_local):\n",
        "                _sleep_until_open(api_local)\n",
        "                continue\n",
        "\n",
        "            t_cycle_start = time.perf_counter()\n",
        "\n",
        "            for t, (model, vecnorm, feat_hint) in per_ticker.items():\n",
        "                t_sym_start = time.perf_counter()\n",
        "                run_live_once_for_symbol(api_local, t, model, vecnorm, features_hint=feat_hint)\n",
        "                logging.info(\"[TIMER] %s symbol work: %.3fs\", t, time.perf_counter() - t_sym_start)\n",
        "\n",
        "            log_equity_snapshot(api_in=api_local)\n",
        "            cycle += 1\n",
        "\n",
        "            if (cycle % 6) == 0:\n",
        "                try:\n",
        "                    plot_equity_curve(from_equity_csv=True)\n",
        "                    df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])  # <-- parse dates here\n",
        "                    m = compute_performance_metrics(df)\n",
        "                    logging.info(\n",
        "                        \"Perf: cum_return=%.2f%% | sharpe=%.2f | maxDD=%.2f%%\",   # label + key\n",
        "                        100*m[\"cum_return\"], m[\"sharpe\"], 100*m[\"max_drawdown\"]\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    logging.warning(\"Plot/metrics failed: %s\", e)\n",
        "\n",
        "            last_exec_at = now_utc()\n",
        "            logging.info(\"[TIMER] full-cycle active time: %.3fs (cooldown=%d min)\",\n",
        "                         time.perf_counter() - t_cycle_start, COOLDOWN_MIN)\n",
        "\n",
        "            if (cycle % 12) == 0:\n",
        "                gc.collect()\n",
        "\n",
        "            _sleep_to_next_minute_block(COOLDOWN_MIN)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logging.info(\"KeyboardInterrupt: stopping live loop.\")\n",
        "        try:\n",
        "            log_equity_snapshot(api_in=api_local)\n",
        "            plot_equity_curve(from_equity_csv=True)\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Finalization failed: %s\", e)\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Live loop exception: %s\", e)\n",
        "        try:\n",
        "            log_equity_snapshot(api_in=api_local)\n",
        "        except Exception:\n",
        "            pass\n",
        "        time.sleep(5)\n",
        "\n",
        "# --------------------------------- Diagnostic runner ------------------------------------------\n",
        "def ticker_diagnostic(ticker: str,\n",
        "                      dry_run: bool = None,\n",
        "                      timeframe: TimeFrame = TimeFrame.Minute,\n",
        "                      limit: int = 300):\n",
        "    if dry_run is None:\n",
        "        dry_run = bool(globals().get(\"DRY_RUN\", True))\n",
        "\n",
        "    print(f\"\\nRunning strategy for {ticker}...\")\n",
        "\n",
        "    try:\n",
        "        api_local = init_alpaca()\n",
        "        positions_start = len(api_local.list_positions())\n",
        "        orders_start    = len(api_local.list_orders(status=\"open\"))\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Alpaca: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        best   = (globals().get(\"BEST_WINDOW_ENV\") or None)\n",
        "        picks  = pick_artifacts_for_ticker(\n",
        "            ticker,\n",
        "            os.getenv(\"ARTIFACTS_DIR\", str(globals().get(\"ARTIFACTS_DIR\", PROJECT_ROOT / \"artifacts\"))),\n",
        "            best_window=best\n",
        "        )\n",
        "        model   = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm = load_vecnormalize(picks.get(\"vecnorm\")) if picks.get(\"vecnorm\") else None\n",
        "        if vecnorm and hasattr(vecnorm, \"training\"): vecnorm.training = False\n",
        "        if vecnorm and hasattr(vecnorm, \"norm_reward\"): vecnorm.norm_reward = False\n",
        "        feats   = load_features(picks.get(\"features\"))\n",
        "        print(f\"Model artifacts loaded for {ticker}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load model for {ticker}: {e}\")\n",
        "        return\n",
        "\n",
        "    min_rows_needed = 60\n",
        "    try:\n",
        "        shape     = expected_obs_shape(model, vecnorm)\n",
        "        lookback  = int(shape[0]) if (shape is not None and len(shape) == 2) else None\n",
        "        bars_need = max(200, (lookback or 0) * 3)\n",
        "        bars_df   = get_recent_bars(api_local, ticker, limit=max(limit, bars_need), timeframe=timeframe)\n",
        "\n",
        "        min_rows_needed = lookback if lookback is not None else 60\n",
        "        if len(bars_df) < min_rows_needed:\n",
        "            print(f\"Not enough data for {ticker}: {len(bars_df)} rows (need ≥ {min_rows_needed})\")\n",
        "            bars_df = pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching bars for {ticker}: {e}\")\n",
        "        bars_df = pd.DataFrame()\n",
        "\n",
        "    obs, obs_ts = None, None\n",
        "    if not bars_df.empty:\n",
        "        try:\n",
        "            obs, obs_ts = prepare_observation_from_bars(\n",
        "                bars_df,\n",
        "                features_hint=feats,\n",
        "                min_required_rows=min_rows_needed,\n",
        "                expected_shape=shape,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing observation for {ticker}: {e}\")\n",
        "\n",
        "    signal = None\n",
        "    target_w = conf = raw = float(\"nan\")\n",
        "    predictions_made = 0\n",
        "    bar_time = pd.NaT\n",
        "    price = float(\"nan\")\n",
        "    equity = float(\"nan\")\n",
        "\n",
        "    orders_submitted = 0\n",
        "    market_closed = 0\n",
        "\n",
        "    if obs is not None:\n",
        "        # OUTER TRY — wraps the whole prediction/trade logic\n",
        "        try:\n",
        "            target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "            signal = int(target_w > 0.0)  # (diagnostic display only)\n",
        "            predictions_made = 1\n",
        "            print(f\"Prediction for {ticker}: {signal} (1 = Buy, 0 = Sell)\")\n",
        "\n",
        "            bar_time = bars_df.index[-1] if not bars_df.empty else pd.NaT\n",
        "            price    = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api_local, ticker)\n",
        "            equity   = get_account_equity(api_local)\n",
        "            print(f\"raw={raw:.4f} conf={conf:.3f} target_w={target_w:.3f} price=${price:.2f} equity=${equity:,.2f}\")\n",
        "\n",
        "            # Log to per-ticker CSV\n",
        "            log_trade_symbol(ticker, bar_time, signal, raw, target_w, conf, price, equity,\n",
        "                             dry_run=dry_run, note=\"diagnostic\")\n",
        "\n",
        "            # INNER TRY — clock/orders section\n",
        "            try:\n",
        "                clock = api_local.get_clock()\n",
        "                if not clock.is_open:\n",
        "                    print(\"Market is closed.\")\n",
        "                    market_closed = 1\n",
        "                else:\n",
        "                    # Mirror live loop: skip if obs got stale\n",
        "                    if utc_ts(now_utc()) - obs_ts > STALE_MAX_SEC:\n",
        "                        print(\"Stale observation; skipping order submission.\")\n",
        "                        log_trade_symbol(\n",
        "                            ticker, bar_time, 0, raw, target_w, conf, price, equity,\n",
        "                            dry_run, note=\"skip_stale_diag\"\n",
        "                        )\n",
        "                    elif signal is not None and not dry_run:\n",
        "                        FORCE_FIRST_BUY = os.getenv(\"FORCE_FIRST_BUY\",\"0\").lower() in (\"1\",\"true\",\"yes\")\n",
        "\n",
        "                        # Do we already hold ticker?\n",
        "                        try:\n",
        "                            pos = api_local.get_position(ticker)\n",
        "                            has_position = float(pos.qty) != 0.0\n",
        "                        except APIError:\n",
        "                            has_position = False\n",
        "\n",
        "                        have = get_position_qty(api_local, ticker)\n",
        "\n",
        "                        if abs(target_w) <= EXIT_WEIGHT_MAX and has_position:\n",
        "                            flatten_symbol(api_local, ticker)\n",
        "                            print(f\"FLATTEN submitted for {ticker}\")\n",
        "                            orders_submitted += 1\n",
        "\n",
        "\n",
        "                        elif (\n",
        "                            SEED_FIRST_SHARE\n",
        "                            and have == 0\n",
        "                            and abs(target_w) >= ENTER_WEIGHT_MIN\n",
        "                            and conf >= ENTER_CONF_MIN\n",
        "                        ):\n",
        "\n",
        "                            seed_notional = max(REBALANCE_MIN_NOTIONAL, round_to_cents(price if np.isfinite(price) else 1.00))\n",
        "                            side = \"buy\" if target_w > 0 else \"sell\"\n",
        "\n",
        "                            skip_seed = False\n",
        "                            if side == \"sell\":\n",
        "                                try:\n",
        "                                    a = api_local.get_asset(ticker)\n",
        "                                    if not getattr(a, \"shortable\", False):\n",
        "                                        print(f\"[{ticker}] Not shortable; skipping seed short.\")\n",
        "                                        log_trade_symbol(ticker, bar_time, 0, raw, target_w, conf, price, equity, dry_run, note=\"not_shortable_seed\")\n",
        "                                        skip_seed = True\n",
        "                                except Exception as e:\n",
        "                                    print(f\"[{ticker}] get_asset shortable check failed: {e}\")\n",
        "\n",
        "                            if not skip_seed:\n",
        "                                if USE_FRACTIONALS and target_w > 0:\n",
        "                                    market_order(api_local, ticker, side=side, notional=seed_notional)\n",
        "                                else:\n",
        "                                    market_order_to_qty(api_local, ticker, side=side, qty=1)\n",
        "\n",
        "                                log_trade_symbol(ticker, bar_time, int(target_w > 0), raw, target_w, conf, price, equity, dry_run, note=\"seed_open\")\n",
        "                                orders_submitted += 1\n",
        "\n",
        "                        elif (FORCE_FIRST_BUY and not has_position) or (signal == 1 and not has_position):\n",
        "                            market_order(\n",
        "                                api_local, symbol=ticker, side=\"buy\",\n",
        "                                qty=(1 if not USE_FRACTIONALS else None),\n",
        "                                notional=(price if USE_FRACTIONALS else None),\n",
        "                            )\n",
        "                            print(f\"BUY order submitted for {ticker}\")\n",
        "                            orders_submitted += 1\n",
        "\n",
        "                        elif signal == 0 and has_position and have > 0:\n",
        "                            market_order(\n",
        "                                api_local, symbol=ticker, side=\"sell\",\n",
        "                                qty=(1 if not USE_FRACTIONALS else None),\n",
        "                                notional=(price if USE_FRACTIONALS else None),\n",
        "                            )\n",
        "                            print(f\"SELL order submitted for {ticker}\")\n",
        "                            orders_submitted += 1\n",
        "\n",
        "                        else:\n",
        "                            print(f\"No action taken for {ticker}\")\n",
        "                    else:\n",
        "                        print(f\"(dry-run) No order submitted for {ticker} — signal={signal}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Trade/clock error for {ticker}: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during prediction/trading for {ticker}: {e}\")\n",
        "\n",
        "    try:\n",
        "        positions_end = len(api_local.list_positions())\n",
        "        orders_end    = len(api_local.list_orders(status=\"open\"))\n",
        "        print(\"\\n========== SUMMARY ==========\")\n",
        "        print(f\"Processed:         1\")\n",
        "        print(f\"Models loaded:     1\")\n",
        "        print(f\"Predictions made:  {predictions_made}\")\n",
        "        print(f\"Market closed:     {market_closed}\")\n",
        "        print(f\"Orders submitted:  {orders_submitted} (dry_run={dry_run})\")\n",
        "        print(f\"Existing positions (start -> end): {positions_start} -> {positions_end}\")\n",
        "        print(f\"Open orders        (start -> end): {orders_start} -> {orders_end}\")\n",
        "        print(\"=============================\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return {\n",
        "        \"signal\": signal,\n",
        "        \"target_w\": target_w,\n",
        "        \"conf\": conf,\n",
        "        \"raw\": raw,\n",
        "        \"bar_time\": bar_time,\n",
        "        \"price\": price,\n",
        "        \"equity\": equity,\n",
        "        \"dry_run\": dry_run,\n",
        "    }\n",
        "\n",
        "def log_config_banner():\n",
        "    try:\n",
        "        artifacts_list = sorted(p.name for p in ARTIFACTS_DIR.iterdir()) if ARTIFACTS_DIR.exists() else []\n",
        "    except Exception:\n",
        "        artifacts_list = []\n",
        "\n",
        "    logging.info(\"=== CONFIG ===\")\n",
        "    logging.info(\"Project root        : %s\", PROJECT_ROOT)\n",
        "    logging.info(\"ARTIFACTS_DIR       : %s\", ARTIFACTS_DIR)\n",
        "    logging.info(\"RESULTS_DIR         : %s\", RESULTS_DIR)\n",
        "    logging.info(\"Tickers             : %s\", TICKERS)\n",
        "    logging.info(\"API base            : %s\", BASE_URL)\n",
        "    logging.info(\"AUTO_RUN_LIVE       : %s\", os.getenv(\"AUTO_RUN_LIVE\", \"\"))\n",
        "    logging.info(\"INF_DETERMINISTIC   : %s\", INF_DETERMINISTIC)\n",
        "    logging.info(\n",
        "        \"DRY_RUN: %s | BARS_FEED: %s | USE_FRACTIONALS: %s | COOLDOWN_MIN: %s | STALE_MAX_SEC: %s\",\n",
        "        DRY_RUN, BARS_FEED, USE_FRACTIONALS, COOLDOWN_MIN, STALE_MAX_SEC,\n",
        "    )\n",
        "    logging.info(\n",
        "        \"WEIGHT_CAP: %.3f | SIZING_MODE: %s | ENTER_CONF_MIN: %.3f | ENTER_WEIGHT_MIN: %.3f | \"\n",
        "        \"EXIT_WEIGHT_MAX: %.3f | REBALANCE_MIN_NOTIONAL: %.2f\",\n",
        "        WEIGHT_CAP, SIZING_MODE, ENTER_CONF_MIN, ENTER_WEIGHT_MIN, EXIT_WEIGHT_MAX, REBALANCE_MIN_NOTIONAL,\n",
        "    )\n",
        "    logging.info(\n",
        "        \"TAKE_PROFIT_PCT: %.3f | STOP_LOSS_PCT: %.3f | BEST_WINDOW_ENV: %s\",\n",
        "        TAKE_PROFIT_PCT, STOP_LOSS_PCT, (BEST_WINDOW_ENV or \"\"),\n",
        "    )\n",
        "    logging.info(\n",
        "        \"DELTA_WEIGHT_MIN: %.3f | RAW_POS_MIN: %.3f | RAW_NEG_MAX: %.3f\",\n",
        "        float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)),\n",
        "        float(globals().get(\"RAW_POS_MIN\", 0.0)),\n",
        "        float(globals().get(\"RAW_NEG_MAX\", 0.0)),\n",
        "    )\n",
        "    if artifacts_list:\n",
        "        logging.info(\"Artifacts present (%d): %s\", len(artifacts_list), \", \".join(artifacts_list))\n",
        "\n",
        "# ===================================== MAIN ===================================================\n",
        "if __name__ == \"__main__\":\n",
        "    if IN_COLAB:\n",
        "        upload_env_and_artifacts_in_colab()\n",
        "        _maybe_convert_features_txt_to_json()\n",
        "        _maybe_rename_vecnorm_scaler()\n",
        "        load_dotenv(dotenv_path=PROJECT_ROOT / \".env\", override=True)\n",
        "\n",
        "    cfg = configure_knobs(overrides={\n",
        "    # cadence / freshness\n",
        "    \"BARS_FEED\": \"iex\",\n",
        "    \"STALE_MAX_SEC\": 600,\n",
        "    \"COOLDOWN_MIN\": 3,\n",
        "\n",
        "    # entry/exit gates\n",
        "    \"ENTER_CONF_MIN\": 0.12,\n",
        "    \"ENTER_WEIGHT_MIN\": 0.02,\n",
        "    \"EXIT_WEIGHT_MAX\": 0.008,\n",
        "\n",
        "    # rebalance tolerance\n",
        "    \"DELTA_WEIGHT_MIN\": 0.012,     # ~1.2% of equity shift required\n",
        "    \"REBALANCE_MIN_NOTIONAL\": 25.0, # avoid $1 trickles (was 1.00)\n",
        "\n",
        "    # raw-action sanity gates\n",
        "    \"RAW_POS_MIN\": 0.10,\n",
        "    \"RAW_NEG_MAX\": 0.10,           # add this for symmetry on shorts\n",
        "\n",
        "    # sizing & risk\n",
        "    \"WEIGHT_CAP\": 0.35,\n",
        "    \"TAKE_PROFIT_PCT\": 0.02,\n",
        "    \"STOP_LOSS_PCT\": 0.01,\n",
        "\n",
        "    # shorts\n",
        "    \"ALLOW_SHORTS\": True,\n",
        "})\n",
        "\n",
        "    log_config_banner()\n",
        "\n",
        "    api = init_alpaca()\n",
        "    acct = api.get_account()\n",
        "    logging.info(\"Account status: %s | equity=%s | cash=%s\", acct.status, acct.equity, acct.cash)\n",
        "\n",
        "    for _sym in TICKERS:\n",
        "        try:\n",
        "            ticker_diagnostic(_sym, dry_run=DRY_RUN)\n",
        "        except Exception as e:\n",
        "            print(f\"[DIAG] {_sym} failed: {e}\")\n",
        "\n",
        "    if os.getenv(\"AUTO_RUN_LIVE\", \"1\").lower() in (\"1\",\"true\",\"yes\",\"y\",\"on\"):\n",
        "        run_live(TICKERS)\n",
        "    else:\n",
        "        logging.info(\"AUTO_RUN_LIVE disabled; live loop not started.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Safe summary + diagnostics (no path clobbering) ----------\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Resolve dirs once, preferring globals set by the main script\n",
        "RESULTS_DIR = Path(globals().get(\"RESULTS_DIR\", os.getenv(\"RESULTS_DIR\", \".\")))\n",
        "LATEST_DIR  = Path(globals().get(\"LATEST_DIR\",  os.getenv(\"LATEST_DIR\",  str(RESULTS_DIR))))\n",
        "\n",
        "# Prefer explicit global equity paths if present; else pick newest equity_log*.csv\n",
        "eq_candidates = [\n",
        "    globals().get(\"EQUITY_LOG_CSV\"),\n",
        "    globals().get(\"EQUITY_LOG_LATEST\"),\n",
        "    RESULTS_DIR / \"equity_log.csv\",\n",
        "    LATEST_DIR / \"equity_log.csv\",\n",
        "]\n",
        "\n",
        "def _first_existing(paths):\n",
        "    for p in paths:\n",
        "        if p:\n",
        "            p = Path(p)\n",
        "            if p.exists() and p.is_file():\n",
        "                return p\n",
        "    return None\n",
        "\n",
        "eq_path = _first_existing(eq_candidates)\n",
        "if eq_path is None:\n",
        "    # try any \"equity_log*.csv\" and pick the most recent\n",
        "    all_eq = list(RESULTS_DIR.glob(\"equity_log*.csv\")) + list(LATEST_DIR.glob(\"equity_log*.csv\"))\n",
        "    eq_path = max(all_eq, key=lambda p: p.stat().st_mtime, default=None)\n",
        "\n",
        "if eq_path and eq_path.exists():\n",
        "    try:\n",
        "        eq = pd.read_csv(eq_path, parse_dates=[\"datetime_utc\"]).sort_values(\"datetime_utc\")\n",
        "        if not eq.empty:\n",
        "            r = eq[\"equity\"].pct_change().dropna()\n",
        "            sharpe_h = (r.mean() / (r.std() + 1e-12)) * np.sqrt(252 * 6.5) if len(r) else float(\"nan\")\n",
        "            print(f\"\\nEquity summary — last: ${eq['equity'].iloc[-1]:,.2f} | \"\n",
        "                  f\"n={len(eq)} pts | Sharpe(h): {sharpe_h:.2f} | src={eq_path}\")\n",
        "        else:\n",
        "            print(f\"No rows in equity log: {eq_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not summarize equity ({eq_path}): {e}\")\n",
        "else:\n",
        "    print(\"No equity_log*.csv found in RESULTS_DIR/LATEST_DIR.\")\n",
        "\n",
        "# Report only what's in your env (default UNH)\n",
        "tickers_to_report = [t.strip().upper() for t in os.getenv(\"TICKERS\", \"UNH\").split(\",\") if t.strip()]\n",
        "\n",
        "print(\"\\nTrade Summary:\")\n",
        "for ticker in tickers_to_report:\n",
        "    # check both locations\n",
        "    trade_candidates = [\n",
        "        RESULTS_DIR / f\"trade_log_{ticker}.csv\",\n",
        "        LATEST_DIR / f\"trade_log_{ticker}.csv\",\n",
        "    ]\n",
        "    log_path = _first_existing(trade_candidates)\n",
        "    if not log_path:\n",
        "        # tolerate Drive duplicates like \"trade_log_XYZ (1).csv\"\n",
        "        any_logs = list(RESULTS_DIR.glob(f\"trade_log_{ticker}*.csv\")) + \\\n",
        "                   list(LATEST_DIR.glob(f\"trade_log_{ticker}*.csv\"))\n",
        "        log_path = max(any_logs, key=lambda p: p.stat().st_mtime, default=None)\n",
        "\n",
        "    if not log_path or not log_path.exists():\n",
        "        print(f\"{ticker}: no trades logged yet.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(log_path, on_bad_lines=\"skip\",\n",
        "                         parse_dates=[\"log_time\",\"bar_time\"])\n",
        "        key = \"signal\" if \"signal\" in df.columns else (\"action\" if \"action\" in df.columns else None)\n",
        "        if key:\n",
        "            counts = df[key].value_counts(dropna=False).to_dict()\n",
        "            print(f\"{ticker}: {counts} | src={log_path.name}\")\n",
        "        else:\n",
        "            print(f\"{ticker}: log present but missing 'signal'/'action' columns. src={log_path.name}\")\n",
        "\n",
        "        if \"confidence\" in df.columns and df[\"confidence\"].notna().any():\n",
        "            plt.figure(figsize=(8, 3.5))\n",
        "            df[\"confidence\"].dropna().plot(kind=\"hist\", bins=10, edgecolor=\"black\")\n",
        "            plt.title(f\"{ticker} - Confidence Distribution\")\n",
        "            plt.xlabel(\"confidence\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        for col in [\"weight\", \"raw_action\"]:\n",
        "            if col in df.columns and df[col].notna().any():\n",
        "                s = df[col].dropna()\n",
        "                print(f\"{ticker} {col}: mean={s.mean():.3f}, std={s.std():.3f}, \"\n",
        "                      f\"min={s.min():.3f}, max={s.max():.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{ticker}: could not summarize trades ({log_path}): {e}\")\n",
        "\n",
        "# --- Position Summary (unchanged) ---\n",
        "try:\n",
        "    if 'api' not in globals():\n",
        "        api = init_alpaca()\n",
        "    positions = api.list_positions()\n",
        "    total_market_value = 0.0\n",
        "    print(\"\\nPosition Summary:\")\n",
        "    for p in positions:\n",
        "        mv = float(p.market_value)\n",
        "        total_market_value += mv\n",
        "        print(f\"  {p.symbol}: {p.qty} shares @ ${float(p.current_price):.2f} | Value: ${mv:,.2f}\")\n",
        "    print(f\"\\nTotal Market Value: ${total_market_value:,.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not summarize positions: {e}\")\n",
        "\n",
        "# --- Filled order counts (last 14 days) ---\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "def count_filled_orders_since(api, symbol: str, days: int = 14) -> int:\n",
        "    after = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
        "    orders = api.list_orders(status=\"all\", after=after, nested=True)\n",
        "    return sum(1 for o in orders if o.symbol == symbol and o.status in (\"filled\",\"partially_filled\"))\n",
        "\n",
        "try:\n",
        "    api_chk = api if 'api' in globals() else init_alpaca()\n",
        "    for sym in tickers_to_report:  # or use TICKERS\n",
        "        n = count_filled_orders_since(api_chk, sym, days=14)\n",
        "        print(f\"{sym}: {n} filled trades in last 14 days\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not fetch filled orders: {e}\")\n"
      ],
      "metadata": {
        "id": "h5X68r85om00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Export locally & download to your computer (Colab) ---\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from google.colab import files   # <-- NEW: for browser download\n",
        "import shutil, time, pandas as pd\n",
        "\n",
        "# Drive root (same as before, to read your results)\n",
        "ROOT = Path(\"/content/drive/MyDrive/AlpacaPaper\")\n",
        "TODAY = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Original sources in Drive (unchanged)\n",
        "SRC_RESULTS = ROOT / \"results\" / TODAY         # e.g., /.../results/2025-10-13\n",
        "SRC_EXPORT  = ROOT / \"results_export\" / TODAY  # rescue export folder (if used)\n",
        "\n",
        "# === CHANGE: write/export to LOCAL staging (in Colab VM), not Drive ===\n",
        "DEST = Path(\"/content\") / \"exports\" / f\"{TODAY}_export\"\n",
        "DEST.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def copy_all(src_dir, dest_dir):\n",
        "    if src_dir.exists():\n",
        "        for p in src_dir.glob(\"*\"):\n",
        "            if p.is_file():\n",
        "                shutil.copy2(p, dest_dir / p.name)\n",
        "                print(\"Copied:\", p.name, \"from\", src_dir.name)\n",
        "    else:\n",
        "        print(\"Missing source:\", src_dir)\n",
        "\n",
        "# Copy from both possible sources into local /content/exports/<today>_export\n",
        "copy_all(SRC_RESULTS, DEST)\n",
        "copy_all(SRC_EXPORT, DEST)\n",
        "\n",
        "# Build/refresh trade_log_master.csv from per-symbol logs (in LOCAL DEST)\n",
        "sym_logs = list(DEST.glob(\"trade_log_*.csv\"))\n",
        "if sym_logs:\n",
        "    frames = []\n",
        "    for p in sym_logs:\n",
        "        try:\n",
        "            df = pd.read_csv(p)\n",
        "            df[\"symbol_file\"] = p.stem.replace(\"trade_log_\", \"\")\n",
        "            frames.append(df)\n",
        "        except Exception as e:\n",
        "            print(\"Skip\", p.name, \"->\", e)\n",
        "    if frames:\n",
        "        master = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        master_path = DEST / \"trade_log_master.csv\"\n",
        "        master.to_csv(master_path, index=False)\n",
        "        print(\"Wrote:\", master_path)\n",
        "\n",
        "# Zip LOCALLY under /content and trigger a browser download\n",
        "zip_base = Path(\"/content\") / f\"results_{TODAY}_{int(time.time())}\"\n",
        "archive_path = shutil.make_archive(str(zip_base), \"zip\", DEST)\n",
        "archive_path = str(Path(archive_path))  # ensure string for files.download\n",
        "\n",
        "print(\"ZIP ->\", archive_path)\n",
        "\n",
        "# OPTIONAL: also keep a copy in Drive (uncomment if wanted)\n",
        "# shutil.copy2(archive_path, ROOT / \"results\" / Path(archive_path).name)\n",
        "\n",
        "# Prompt download to your computer\n",
        "files.download(archive_path)\n",
        "\n",
        "# Show what's in the LOCAL export folder\n",
        "print(\"\\nLocal export now contains:\")\n",
        "for p in sorted(DEST.iterdir()):\n",
        "    print(\" -\", p.name)\n"
      ],
      "metadata": {
        "id": "RaKYNmKdOwOt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}