{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrhliCML2j4azmSUByO0C0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/exploratory_daytrading/blob/main/ppo_alpaca_paper_trading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-DEy5gEqqEi",
        "outputId": "3b0ec379-be0f-41e4-ce49-4233d26b5b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alpaca-trade-api\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.32.4)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.8.0)\n",
            "Collecting websockets<11,>=9.0 (from alpaca-trade-api)\n",
            "  Downloading websockets-10.4.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting msgpack==1.0.3 (from alpaca-trade-api)\n",
            "  Downloading msgpack-1.0.3.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (3.12.15)\n",
            "Collecting PyYAML==6.0.1 (from alpaca-trade-api)\n",
            "  Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting deprecation==2.1.0 (from alpaca-trade-api)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.20.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp<4,>=3.8.3->alpaca-trade-api) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.18.1->alpaca-trade-api) (1.17.0)\n",
            "Downloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (724 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: msgpack, websockets\n",
            "  Building wheel for msgpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-1.0.3-cp312-cp312-linux_x86_64.whl size=15688 sha256=06a607fd1df93540f9f7b59fbe1c78819b5d452e76e82f55f28b6d8ad1e374bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/bd/3f/f043e8f634db9c90ae128d631f43ae9990eef01274a63291f9\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-linux_x86_64.whl size=107332 sha256=9f63dd2a4bd8647f982da714a1af84f36c8a8d8b1dc663ff138e61d9f9b398fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/cf/6d/5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
            "Successfully built msgpack websockets\n",
            "Installing collected packages: msgpack, websockets, urllib3, PyYAML, deprecation, alpaca-trade-api\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.1\n",
            "    Uninstalling msgpack-1.1.1:\n",
            "      Successfully uninstalled msgpack-1.1.1\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.13.0 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\n",
            "google-adk 1.13.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 10.4 which is incompatible.\n",
            "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "google-genai 1.33.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.1 alpaca-trade-api-3.2.0 deprecation-2.1.0 msgpack-1.0.3 urllib3-1.26.20 websockets-10.4\n",
            "Requirement already satisfied: alpaca-trade-api in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.32.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.8.0)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (10.4)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.0.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (3.12.15)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.20.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp<4,>=3.8.3->alpaca-trade-api) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.18.1->alpaca-trade-api) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=edc0df7ddae7bc86d316a2793196c735d29ea30b9bcdfc0f87d90a8f9c5d597c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install alpaca-trade-api\n",
        "!pip install alpaca-trade-api python-dotenv ta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean any partials\n",
        "!pip uninstall -y stable-baselines3 shimmy gymnasium gym autorom AutoROM.accept-rom-license ale-py\n",
        "\n",
        "# Install the compatible trio (no [extra] to avoid Atari deps)\n",
        "!pip install \"gymnasium==0.29.1\" \"shimmy==1.3.0\" \"stable-baselines3==2.3.0\"\n",
        "\n",
        "# Your other libs (safe to keep separate)\n",
        "!pip install alpaca-trade-api ta python-dotenv gym-anytrading\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HgUjRXJs8ez",
        "outputId": "8282b3e0-7c16-4784-e362-7bdbab4295c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping stable-baselines3 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping shimmy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: gymnasium 1.2.0\n",
            "Uninstalling gymnasium-1.2.0:\n",
            "  Successfully uninstalled gymnasium-1.2.0\n",
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "\u001b[33mWARNING: Skipping autorom as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping AutoROM.accept-rom-license as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: ale-py 0.11.2\n",
            "Uninstalling ale-py-0.11.2:\n",
            "  Successfully uninstalled ale-py-0.11.2\n",
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting shimmy==1.3.0\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting stable-baselines3==2.3.0\n",
            "  Downloading stable_baselines3-2.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.3.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->stable-baselines3==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13->stable-baselines3==2.3.0) (3.0.2)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'stable-baselines3' candidate (version 2.3.0 at https://files.pythonhosted.org/packages/51/0b/6539076ed58343f1404dea0462167b079b5264508b8e5bbed01cea9f66b8/stable_baselines3-2.3.0-py3-none-any.whl (from https://pypi.org/simple/stable-baselines3/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Loading broken with PyTorch 1.13\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium, shimmy, stable-baselines3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 shimmy-1.3.0 stable-baselines3-2.3.0\n",
            "Requirement already satisfied: alpaca-trade-api in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: ta in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Collecting gym-anytrading\n",
            "  Downloading gym_anytrading-2.0.0-py3-none-any.whl.metadata (292 bytes)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.32.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.8.0)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (10.4)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.0.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (3.12.15)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (25.0)\n",
            "Requirement already satisfied: gymnasium>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from gym-anytrading) (0.29.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from gym-anytrading) (3.10.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.20.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.1->gym-anytrading) (1.17.0)\n",
            "Downloading gym_anytrading-2.0.0-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gym-anytrading\n",
            "Successfully installed gym-anytrading-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install if not already\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8LJ--nts-e9",
        "outputId": "0ffba8fb-cb2b-4de0-e94e-6640991fb7cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, math, time, typing, numpy as np\n",
        "import warnings\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import ta\n",
        "\n",
        "from alpaca_trade_api.rest import REST, TimeFrame, APIError\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.vec_env.vec_normalize import VecNormalize\n",
        "\n",
        "# Colab Drive (optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Use a PPO-specific output dir (rename from LightGBM)\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/Alpaca_Results/PPO\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS4-sGMItAIM",
        "outputId": "e0683bef-428b-4c03-cfef-d50384ea9203"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Prompts you to upload the .env file for the key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "hWMP7LcYtCOg",
        "outputId": "522c4d06-acc4-41f8-ea51-0fe2993e28f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7a3d30d3-65cf-4a55-8ad6-7cba41eedbff\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7a3d30d3-65cf-4a55-8ad6-7cba41eedbff\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Alpaca_keys.env.txt to Alpaca_keys.env.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Rename to .env so dotenv can recognize it\n",
        "os.rename(\"Alpaca_keys.env.txt\", \".env\")"
      ],
      "metadata": {
        "id": "emNRU9dEtIHo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload: model_ABT.txt, features_ABT.txt, scaler_ABT.pkl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "R8oUOgIYtJxk",
        "outputId": "9934ac57-9718-4437-956d-f2bda40b6f0f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-edd40064-d7a5-4227-a451-ec2f9c0c4275\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-edd40064-d7a5-4227-a451-ec2f9c0c4275\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ppo_UNH_window3_features.json to ppo_UNH_window3_features.json\n",
            "Saving ppo_UNH_window3_model_info.json to ppo_UNH_window3_model_info.json\n",
            "Saving ppo_UNH_window3_model.zip to ppo_UNH_window3_model.zip\n",
            "Saving ppo_UNH_window3_probability_config.json to ppo_UNH_window3_probability_config.json\n",
            "Saving ppo_UNH_window3_vecnorm.pkl to ppo_UNH_window3_vecnorm.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- One-time imports & logging ---\n",
        "# Imports + basic logging + Alpaca init\n",
        "import os, re, json, csv, shutil, logging, pickle, warnings, time, math\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import Dict, Optional, Tuple, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import alpaca_trade_api as tradeapi\n",
        "from alpaca_trade_api.rest import TimeFrame, APIError\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "\n",
        "load_dotenv(override=True)\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "API_KEY    = os.getenv(\"APCA_API_KEY_ID\")     or os.getenv(\"ALPACA_API_KEY_ID\")     or \"\"\n",
        "API_SECRET = os.getenv(\"APCA_API_SECRET_KEY\") or os.getenv(\"ALPACA_API_SECRET_KEY\") or \"\"\n",
        "BASE_URL   = os.getenv(\"APCA_API_BASE_URL\")   or os.getenv(\"ALPACA_API_BASE_URL\")   or \"https://paper-api.alpaca.markets\"\n",
        "\n",
        "def init_alpaca() -> \"tradeapi.REST\":\n",
        "    if not API_KEY or not API_SECRET:\n",
        "        raise RuntimeError(\"Missing API keys.\")\n",
        "    api = tradeapi.REST(API_KEY, API_SECRET, base_url=BASE_URL)\n",
        "    _ = api.get_account()  # sanity\n",
        "    return api\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Optional quick sanity check (kept out of import-time side effects)\n",
        "    logging.info(\"Keys loaded: %s %s | BASE_URL=%s\", bool(API_KEY), bool(API_SECRET), BASE_URL)\n",
        "    api = init_alpaca()\n",
        "    logging.info(\"Account status: %s\", api.get_account().status)\n",
        "\n",
        "    # run_live(TICKERS)  # uncomment when ready\n",
        "\n",
        "def sanity_check():\n",
        "    print(\"Keys loaded (bools):\", bool(API_KEY), bool(API_SECRET), \"| BASE_URL:\", BASE_URL)\n",
        "    api = init_alpaca()\n",
        "    acct = api.get_account()\n",
        "    print(\"Account status:\", acct.status)\n",
        "\n",
        "sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJmb00cRaHyH",
        "outputId": "f1a8bdb5-8d48-4aa4-cf77-874d23e5679b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys loaded (bools): True True | BASE_URL: https://paper-api.alpaca.markets\n",
            "Account status: ACTIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "for root in [\"/content\", \"/content/drive/MyDrive\"]:\n",
        "    hits = list(Path(root).rglob(\"ppo_*_model.zip\"))\n",
        "    if hits:\n",
        "        print(\"Found in:\", root)\n",
        "        for h in hits: print(\"  \", h)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkfPWqz7yxrR",
        "outputId": "7e864bbf-50f6-4763-bdbd-0f5882dced78"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found in: /content\n",
            "   /content/ppo_UNH_window3_model.zip\n",
            "   /content/drive/MyDrive/Results_May_2025/ppo_models_master/ppo_CVX_window1_model.zip\n",
            "   /content/drive/MyDrive/Results_May_2025/ppo_models_master/ppo_GE_window1_model.zip\n",
            "Found in: /content/drive/MyDrive\n",
            "   /content/drive/MyDrive/Results_May_2025/ppo_models_master/ppo_CVX_window1_model.zip\n",
            "   /content/drive/MyDrive/Results_May_2025/ppo_models_master/ppo_GE_window1_model.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UNH artifacts + paper-trading runtime config (run BEFORE sanity_check/run_live) ---\n",
        "import os, shutil, csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from alpaca_trade_api.rest import TimeFrame, APIError\n",
        "\n",
        "# --- Env for UNH paper trading (set FIRST) ---\n",
        "os.environ[\"ARTIFACTS_DIR\"]     = \"/content\"\n",
        "os.environ[\"TICKERS\"]           = \"UNH\"\n",
        "os.environ[\"BEST_WINDOW\"]       = \"3\"\n",
        "os.environ[\"INF_DETERMINISTIC\"] = \"1\"\n",
        "os.environ[\"DRY_RUN\"]           = \"0\"     # \"1\"=log only (no orders); \"0\"=place PAPER orders\n",
        "os.environ[\"BARS_FEED\"]         = \"iex\"   # paper feed is \"iex\"; live (if entitled) would be \"sip\"\n",
        "# Derived knobs used by other functions\n",
        "TICKERS          = [t.strip() for t in os.getenv(\"TICKERS\",\"UNH\").split(\",\") if t.strip()]\n",
        "BEST_WINDOW_ENV  = (os.getenv(\"BEST_WINDOW\",\"\").strip() or None)\n",
        "\n",
        "# local knobs (read once from env)\n",
        "DRY_RUN   = os.getenv(\"DRY_RUN\", \"1\").lower() in (\"1\",\"true\",\"yes\")\n",
        "BARS_FEED = os.getenv(\"BARS_FEED\", \"iex\").strip()\n",
        "\n",
        "# --- CSV logging setup (after ARTIFACTS_DIR is set) ---\n",
        "RESULTS_DIR = Path(os.getenv(\"RESULTS_DIR\", os.getenv(\"ARTIFACTS_DIR\", \"/content\")))\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _append_csv_row(path: Path, row: dict):\n",
        "    write_header = not path.exists()\n",
        "    with path.open(\"a\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=row.keys())\n",
        "        if write_header:\n",
        "            w.writeheader()\n",
        "        w.writerow(row)\n",
        "\n",
        "def log_trade(symbol: str,\n",
        "              bar_time,            # pandas/np datetime or ISO\n",
        "              signal: int,         # 1=Buy, 0=Sell/Hold\n",
        "              raw_action: float,   # PPO raw action\n",
        "              weight: float,       # target portfolio weight\n",
        "              confidence: float,   # |tanh(raw_action)|\n",
        "              price: float,        # last price\n",
        "              equity: float,       # account equity\n",
        "              dry_run: bool,\n",
        "              note: str = \"\"):\n",
        "    # normalize times → ISO UTC\n",
        "    try:\n",
        "        bt_iso = pd.to_datetime(bar_time, utc=True).isoformat()\n",
        "    except Exception:\n",
        "        bt_iso = \"\"\n",
        "    row = {\n",
        "        \"log_time\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"symbol\": symbol,\n",
        "        \"bar_time\": bt_iso,\n",
        "        \"signal\": \"BUY\" if int(signal) == 1 else \"SELL_OR_HOLD\",\n",
        "        \"raw_action\": float(raw_action) if np.isfinite(raw_action) else \"\",\n",
        "        \"weight\": float(weight) if np.isfinite(weight) else \"\",\n",
        "        \"confidence\": float(confidence) if np.isfinite(confidence) else \"\",\n",
        "        \"price\": float(price) if np.isfinite(price) else \"\",\n",
        "        \"equity\": float(equity) if np.isfinite(equity) else \"\",\n",
        "        \"dry_run\": int(bool(dry_run)),\n",
        "        \"note\": note,\n",
        "    }\n",
        "    _append_csv_row(RESULTS_DIR / f\"trade_log_{symbol}.csv\", row)\n",
        "\n",
        "# --- Ensure the artifact trio sits together in ARTIFACTS_DIR ---\n",
        "ART_DIR = Path(os.environ[\"ARTIFACTS_DIR\"])\n",
        "CANDIDATE_DRIVE_ROOTS = [Path(\"/content/drive/MyDrive\"), Path(\"/content/drive\")]\n",
        "\n",
        "need = [\n",
        "    \"ppo_UNH_window3_model.zip\",\n",
        "    \"ppo_UNH_window3_vecnorm.pkl\",\n",
        "    \"ppo_UNH_window3_features.json\",\n",
        "]\n",
        "\n",
        "for name in need:\n",
        "    dst = ART_DIR / name\n",
        "    if dst.exists():\n",
        "        continue\n",
        "    src = None\n",
        "    for root in CANDIDATE_DRIVE_ROOTS:\n",
        "        try:\n",
        "            src = next(root.rglob(name))\n",
        "            break\n",
        "        except StopIteration:\n",
        "            continue\n",
        "    if src:\n",
        "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "# --- Quick verification ---\n",
        "print(\"ARTIFACTS_DIR =\", ART_DIR)\n",
        "print(\"Exists:\", ART_DIR.exists())\n",
        "print(\"Artifacts found:\", sorted(p.name for p in ART_DIR.glob(\"ppo_UNH_window3_*\")))\n",
        "print(\"DRY_RUN   =\", os.getenv(\"DRY_RUN\"))\n",
        "print(\"BARS_FEED =\", os.getenv(\"BARS_FEED\"))\n",
        "\n",
        "missing = [n for n in need if not (ART_DIR / n).exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing artifacts in {ART_DIR}:\\n  \" + \"\\n  \".join(missing)\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66_yCEpJxNcI",
        "outputId": "a315cd9a-6d23-4923-f8a1-39841689a026"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARTIFACTS_DIR = /content\n",
            "Exists: True\n",
            "Artifacts found: ['ppo_UNH_window3_features.json', 'ppo_UNH_window3_model.zip', 'ppo_UNH_window3_model_info.json', 'ppo_UNH_window3_probability_config.json', 'ppo_UNH_window3_vecnorm.pkl']\n",
            "DRY_RUN   = 0\n",
            "BARS_FEED = iex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === RUN UNH DIAGNOSTIC (paper if DRY_RUN=='0', dry-run if '1') ===\n",
        "# --- define unh_diagnostic (drop-in) ---\n",
        "def unh_diagnostic(dry_run=False, timeframe=TimeFrame.Minute, limit=300)\n",
        "\n",
        "    ticker = \"UNH\"\n",
        "    print(f\"\\nRunning strategy for {ticker}...\")\n",
        "\n",
        "    # snapshot\n",
        "    try:\n",
        "        api = init_alpaca()\n",
        "        positions_start = len(api.list_positions())\n",
        "        orders_start    = len(api.list_orders(status=\"open\"))\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Alpaca: {e}\")\n",
        "        return\n",
        "\n",
        "    # load artifacts\n",
        "    try:\n",
        "        best = (os.getenv(\"BEST_WINDOW\",\"\").strip() or None)\n",
        "        picks   = pick_artifacts_for_ticker(ticker, os.getenv(\"ARTIFACTS_DIR\",\"/content\"), best_window=best)\n",
        "        model   = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm = load_vecnormalize(picks[\"vecnorm\"]) if picks[\"vecnorm\"] else None\n",
        "        feats   = load_features(picks[\"features\"])\n",
        "        print(f\"Model artifacts loaded for {ticker}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load model for {ticker}: {e}\")\n",
        "        return\n",
        "\n",
        "    # figure out how many bars we need (shape-aware)\n",
        "    try:\n",
        "        shape = expected_obs_shape(model, vecnorm)\n",
        "        lookback = int(shape[0]) if (shape is not None and len(shape) == 2) else None\n",
        "        bars_limit = max(200, (lookback or 0) * 3)\n",
        "\n",
        "        bars_df = get_recent_bars(api, ticker, limit=bars_limit, timeframe=timeframe)\n",
        "        min_rows_needed = lookback if lookback is not None else 60\n",
        "        if len(bars_df) < min_rows_needed:\n",
        "            print(f\"Not enough data for {ticker}: {len(bars_df)} rows (need ≥ {min_rows_needed})\")\n",
        "            bars_df = None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching bars for {ticker}: {e}\")\n",
        "        bars_df = None\n",
        "\n",
        "    # build obs\n",
        "    obs = None\n",
        "    if bars_df is not None and not bars_df.empty:\n",
        "        try:\n",
        "            obs, obs_ts = prepare_observation_from_bars(\n",
        "                bars_df,\n",
        "                features_hint=feats,\n",
        "                min_required_rows=min_rows_needed,\n",
        "                expected_shape=shape,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing observation for {ticker}: {e}\")\n",
        "\n",
        "    # predict\n",
        "    signal = None\n",
        "    if obs is not None:\n",
        "        try:\n",
        "            target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "            signal = int(target_w > 0.0)  # 1 = Buy, 0 = Sell/Hold\n",
        "            print(f\"Prediction for {ticker}: {signal} (1 = Buy, 0 = Sell)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction error for {ticker}: {e}\")\n",
        "\n",
        "    # optional paper order if market open\n",
        "    try:\n",
        "        clock = api.get_clock()\n",
        "        if not clock.is_open:\n",
        "            print(\"Market is closed.\")\n",
        "        else:\n",
        "            if signal is not None:\n",
        "                if not dry_run:\n",
        "                    has_position = False\n",
        "                    try:\n",
        "                        pos = api.get_position(ticker)\n",
        "                        has_position = float(pos.qty) > 0\n",
        "                    except APIError:\n",
        "                        has_position = False\n",
        "\n",
        "                    if signal == 1 and not has_position:\n",
        "                        api.submit_order(symbol=ticker, qty=1, side='buy', type='market', time_in_force='day')\n",
        "                        print(f\"BUY order submitted for {ticker}\")\n",
        "                    elif signal == 0 and has_position:\n",
        "                        api.submit_order(symbol=ticker, qty=1, side='sell', type='market', time_in_force='day')\n",
        "                        print(f\"SELL order submitted for {ticker}\")\n",
        "                    else:\n",
        "                        print(f\"No action taken for {ticker}\")\n",
        "                else:\n",
        "                    print(f\"(dry-run) No order submitted for {ticker} — signal={signal}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Trade/clock error for {ticker}: {e}\")\n",
        "\n",
        "    # tiny summary\n",
        "    try:\n",
        "        positions_end = len(api.list_positions())\n",
        "        orders_end    = len(api.list_orders(status=\"open\"))\n",
        "        print(\"\\n========== SUMMARY ==========\")\n",
        "        print(f\"Processed:         1\")\n",
        "        print(f\"Models loaded:     1\")\n",
        "        print(f\"Predictions made:  {1 if signal is not None else 0}\")\n",
        "        print(f\"Orders submitted:  {'(paper)' if not dry_run else 0}\")\n",
        "        print(f\"Existing positions (start -> end): {positions_start} -> {positions_end}\")\n",
        "        print(f\"Open orders        (start -> end): {orders_start} -> {orders_end}\")\n",
        "        print(\"=============================\")\n",
        "    except Exception:\n",
        "        pass\n",
        "# --- define unh_diagnostic (drop-in) ---\n",
        "def unh_diagnostic(dry_run=True, timeframe=TimeFrame.Minute, limit=300):\n",
        "    ticker = \"UNH\"\n",
        "    print(f\"\\nRunning strategy for {ticker}...\")\n",
        "\n",
        "    # snapshot\n",
        "    try:\n",
        "        api = init_alpaca()\n",
        "        positions_start = len(api.list_positions())\n",
        "        orders_start    = len(api.list_orders(status=\"open\"))\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Alpaca: {e}\")\n",
        "        return\n",
        "\n",
        "    # load artifacts\n",
        "    try:\n",
        "        best = (os.getenv(\"BEST_WINDOW\",\"\").strip() or None)\n",
        "        picks   = pick_artifacts_for_ticker(ticker, os.getenv(\"ARTIFACTS_DIR\",\"/content\"), best_window=best)\n",
        "        model   = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm = load_vecnormalize(picks[\"vecnorm\"]) if picks[\"vecnorm\"] else None\n",
        "        feats   = load_features(picks[\"features\"])\n",
        "        print(f\"Model artifacts loaded for {ticker}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load model for {ticker}: {e}\")\n",
        "        return\n",
        "\n",
        "    # figure out how many bars we need (shape-aware)\n",
        "    try:\n",
        "        shape = expected_obs_shape(model, vecnorm)\n",
        "        lookback = int(shape[0]) if (shape is not None and len(shape) == 2) else None\n",
        "        bars_limit = max(200, (lookback or 0) * 3)\n",
        "\n",
        "        bars_df = get_recent_bars(api, ticker, limit=bars_limit, timeframe=timeframe)\n",
        "        min_rows_needed = lookback if lookback is not None else 60\n",
        "        if len(bars_df) < min_rows_needed:\n",
        "            print(f\"Not enough data for {ticker}: {len(bars_df)} rows (need ≥ {min_rows_needed})\")\n",
        "            bars_df = None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching bars for {ticker}: {e}\")\n",
        "        bars_df = None\n",
        "\n",
        "    # build obs\n",
        "    obs = None\n",
        "    if bars_df is not None and not bars_df.empty:\n",
        "        try:\n",
        "            obs, obs_ts = prepare_observation_from_bars(\n",
        "                bars_df,\n",
        "                features_hint=feats,\n",
        "                min_required_rows=min_rows_needed,\n",
        "                expected_shape=shape,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing observation for {ticker}: {e}\")\n",
        "\n",
        "    # predict\n",
        "    signal = None\n",
        "    if obs is not None:\n",
        "        try:\n",
        "            target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "            signal = int(target_w > 0.0)  # 1 = Buy, 0 = Sell/Hold\n",
        "            print(f\"Prediction for {ticker}: {signal} (1 = Buy, 0 = Sell)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction error for {ticker}: {e}\")\n",
        "\n",
        "    # optional paper order if market open\n",
        "    try:\n",
        "        clock = api.get_clock()\n",
        "        if not clock.is_open:\n",
        "            print(\"Market is closed.\")\n",
        "        else:\n",
        "            if signal is not None:\n",
        "                if not dry_run:\n",
        "                    has_position = False\n",
        "                    try:\n",
        "                        pos = api.get_position(ticker)\n",
        "                        has_position = float(pos.qty) > 0\n",
        "                    except APIError:\n",
        "                        has_position = False\n",
        "\n",
        "                    if signal == 1 and not has_position:\n",
        "                        api.submit_order(symbol=ticker, qty=1, side='buy', type='market', time_in_force='day')\n",
        "                        print(f\"BUY order submitted for {ticker}\")\n",
        "                    elif signal == 0 and has_position:\n",
        "                        api.submit_order(symbol=ticker, qty=1, side='sell', type='market', time_in_force='day')\n",
        "                        print(f\"SELL order submitted for {ticker}\")\n",
        "                    else:\n",
        "                        print(f\"No action taken for {ticker}\")\n",
        "                else:\n",
        "                    print(f\"(dry-run) No order submitted for {ticker} — signal={signal}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Trade/clock error for {ticker}: {e}\")\n",
        "\n",
        "    # tiny summary\n",
        "    try:\n",
        "        positions_end = len(api.list_positions())\n",
        "        orders_end    = len(api.list_orders(status=\"open\"))\n",
        "        print(\"\\n========== SUMMARY ==========\")\n",
        "        print(f\"Processed:         1\")\n",
        "        print(f\"Models loaded:     1\")\n",
        "        print(f\"Predictions made:  {1 if signal is not None else 0}\")\n",
        "        print(f\"Orders submitted:  {'(paper)' if not dry_run else 0}\")\n",
        "        print(f\"Existing positions (start -> end): {positions_start} -> {positions_end}\")\n",
        "        print(f\"Open orders        (start -> end): {orders_start} -> {orders_end}\")\n",
        "        print(\"=============================\")\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "Ccts6cRv5a9B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Tuple, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---- Column normalization ---------------------------------------------------------------\n",
        "def normalize_ohlcv_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Case-insensitive rename to: Open, High, Low, Close, Adj Close, Volume.\n",
        "    If Adj Close missing, copy Close.\n",
        "    \"\"\"\n",
        "    cols_ci = {c.lower(): c for c in df.columns}\n",
        "    want = {\n",
        "        \"Open\":      [\"open\"],\n",
        "        \"High\":      [\"high\"],\n",
        "        \"Low\":       [\"low\"],\n",
        "        \"Close\":     [\"close\", \"close*\", \"last\"],\n",
        "        \"Adj Close\": [\"adj close\", \"adj_close\", \"adjclose\", \"adjusted close\"],\n",
        "        \"Volume\":    [\"volume\", \"vol\"],\n",
        "    }\n",
        "    rename = {}\n",
        "    for final, alts in want.items():\n",
        "        for a in [final.lower()] + alts:\n",
        "            if a in cols_ci:\n",
        "                rename[cols_ci[a]] = final\n",
        "                break\n",
        "    out = df.rename(columns=rename).copy()\n",
        "    if \"Adj Close\" not in out.columns and \"Close\" in out.columns:\n",
        "        out[\"Adj Close\"] = out[\"Close\"]\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---- Light wavelet denoiser (safe fallbacks) -------------------------------------------\n",
        "def denoise_wavelet(series: pd.Series, wavelet: str = \"db1\", level: int = 2) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Low-pass via wavelets; if pywt missing or params invalid, fallback to a short EMA.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import pywt\n",
        "    except Exception:\n",
        "        return pd.Series(series).astype(float).ewm(span=5, adjust=False).mean()\n",
        "\n",
        "    s = pd.Series(series).astype(float).ffill().bfill()\n",
        "    arr = s.to_numpy()\n",
        "    try:\n",
        "        w = pywt.Wavelet(wavelet)\n",
        "        maxlvl = pywt.dwt_max_level(len(arr), w.dec_len)\n",
        "        lvl = int(max(0, min(level, maxlvl)))\n",
        "        if lvl < 1:\n",
        "            return s\n",
        "        coeffs = pywt.wavedec(arr, w, mode=\"symmetric\", level=lvl)\n",
        "        for i in range(1, len(coeffs)):  # zero details, keep approximation\n",
        "            coeffs[i] = np.zeros_like(coeffs[i])\n",
        "        rec = pywt.waverec(coeffs, w, mode=\"symmetric\")\n",
        "        return pd.Series(rec[:len(arr)], index=s.index)\n",
        "    except Exception:\n",
        "        return s.ewm(span=5, adjust=False).mean()\n",
        "\n",
        "\n",
        "# ---- Simple regime labels ---------------------------------------------------------------\n",
        "def add_regime(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"Vol20\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Ret20\"] = df[\"Close\"].pct_change(20)\n",
        "    vol_hi   = (df[\"Vol20\"] > df[\"Vol20\"].median()).astype(int)\n",
        "    trend_hi = (df[\"Ret20\"].abs() > df[\"Ret20\"].abs().median()).astype(int)\n",
        "    df[\"Regime4\"] = vol_hi * 2 + trend_hi  # values in {0,1,2,3}\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---- Indicators used at live time (match training set) ---------------------------------\n",
        "def add_features_live(\n",
        "    df: pd.DataFrame,\n",
        "    use_sentiment: bool = False,\n",
        "    rsi_wilder: bool = True,\n",
        "    atr_wilder: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds a compact set of indicators (Bollinger, Stoch, ROC, OBV, CCI, EMAs, MACD, RSI, ATR, Volatility).\n",
        "    Returns a numeric DataFrame with NaN where warmups are needed (to be dropped later).\n",
        "    \"\"\"\n",
        "    df = normalize_ohlcv_cols(df).copy().sort_index()\n",
        "\n",
        "    # Bollinger\n",
        "    df[\"SMA_20\"] = df[\"Close\"].rolling(20).mean()\n",
        "    df[\"STD_20\"] = df[\"Close\"].rolling(20).std()\n",
        "    df[\"Upper_Band\"] = df[\"SMA_20\"] + 2 * df[\"STD_20\"]\n",
        "    df[\"Lower_Band\"] = df[\"SMA_20\"] - 2 * df[\"STD_20\"]\n",
        "\n",
        "    # Stochastic %K (unsmoothed)\n",
        "    df[\"Lowest_Low\"]   = df[\"Low\"].rolling(14).min()\n",
        "    df[\"Highest_High\"] = df[\"High\"].rolling(14).max()\n",
        "    denom = (df[\"Highest_High\"] - df[\"Lowest_Low\"]).replace(0, np.nan)\n",
        "    df[\"Stoch\"] = ((df[\"Close\"] - df[\"Lowest_Low\"]) / denom) * 100\n",
        "\n",
        "    # Momentum / volume / CCI\n",
        "    df[\"ROC\"] = df[\"Close\"].pct_change(10)\n",
        "    sign = np.sign(df[\"Close\"].diff().fillna(0))\n",
        "    df[\"OBV\"] = (sign * df[\"Volume\"].fillna(0)).cumsum()\n",
        "\n",
        "    tp = (df[\"High\"] + df[\"Low\"] + df[\"Close\"]) / 3.0\n",
        "    sma_tp = tp.rolling(20).mean()\n",
        "    md = (tp - sma_tp).abs().rolling(20).mean().replace(0, np.nan)\n",
        "    df[\"CCI\"] = (tp - sma_tp) / (0.015 * md)\n",
        "\n",
        "    # EMAs + MACD\n",
        "    df[\"EMA_10\"] = df[\"Close\"].ewm(span=10, adjust=False).mean()\n",
        "    df[\"EMA_50\"] = df[\"Close\"].ewm(span=50, adjust=False).mean()\n",
        "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "    df[\"MACD_Line\"]   = ema12 - ema26\n",
        "    df[\"MACD_Signal\"] = df[\"MACD_Line\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    # RSI (Wilder default)\n",
        "    d = df[\"Close\"].diff()\n",
        "    gain = d.clip(lower=0)\n",
        "    loss = (-d.clip(upper=0))\n",
        "    if rsi_wilder:\n",
        "        avg_gain = gain.ewm(alpha=1/14, adjust=False).mean()\n",
        "        avg_loss = loss.ewm(alpha=1/14, adjust=False).mean()\n",
        "    else:\n",
        "        avg_gain = gain.rolling(14).mean()\n",
        "        avg_loss = loss.rolling(14).mean()\n",
        "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
        "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # ATR (Wilder default)\n",
        "    tr = pd.concat([\n",
        "        (df[\"High\"] - df[\"Low\"]),\n",
        "        (df[\"High\"] - df[\"Close\"].shift()).abs(),\n",
        "        (df[\"Low\"]  - df[\"Close\"].shift()).abs(),\n",
        "    ], axis=1).max(axis=1)\n",
        "    df[\"ATR\"] = tr.ewm(alpha=1/14, adjust=False).mean() if atr_wilder else tr.rolling(14).mean()\n",
        "\n",
        "    # Volatility + denoised close\n",
        "    df[\"Volatility\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Denoised_Close\"] = denoise_wavelet(df[\"Close\"])\n",
        "\n",
        "    # Regime + optional sentiment + simple “Greeks” proxies\n",
        "    df = add_regime(df)\n",
        "    df[\"SentimentScore\"] = (df.get(\"SentimentScore\", 0.0) if use_sentiment else 0.0)\n",
        "    df[\"Delta\"] = df[\"Close\"].pct_change(1).fillna(0.0)\n",
        "    df[\"Gamma\"] = df[\"Delta\"].diff().fillna(0.0)\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---- Artifact feature order (for 1D vector models) -------------------------------------\n",
        "def compute_art_feat_order(features_hint: Any, df: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"\n",
        "    Resolve the exact feature order from your artifact, keeping only numeric columns present.\n",
        "    Accepts either a list or a dict with key 'features'.\n",
        "    \"\"\"\n",
        "    if features_hint is None:\n",
        "        return [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "    feats = features_hint.get(\"features\", features_hint) if isinstance(features_hint, dict) else list(features_hint)\n",
        "    drop = {\"datetime\", \"symbol\", \"target\", \"return\"}\n",
        "    return [c for c in feats if c not in drop and (c in df.columns) and pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "\n",
        "def build_obs_from_row(row: pd.Series, order: List[str]) -> np.ndarray:\n",
        "    \"\"\"Row -> 1D float32 vector in the artifact's column order (NaNs -> 0).\"\"\"\n",
        "    vals = []\n",
        "    for c in order:\n",
        "        v = row.get(c, np.nan)\n",
        "        vals.append(0.0 if (pd.isna(v) or v is None or v is False) else float(v))\n",
        "    return np.array(vals, dtype=np.float32)\n",
        "\n",
        "\n",
        "# ---- Shape discovery + live columns picking (for 2D models) -----------------------------\n",
        "def expected_obs_shape(model, vecnorm) -> Optional[tuple]:\n",
        "    \"\"\"\n",
        "    Returns the tuple shape the model expects for a *single* observation,\n",
        "    e.g. (10, 2). Falls back to None if not available.\n",
        "    \"\"\"\n",
        "    for src in (vecnorm, model):\n",
        "        try:\n",
        "            shp = tuple(src.observation_space.shape)  # SB3 exposes this\n",
        "            if shp:\n",
        "                return shp\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None  # unknown; caller will use 1D fallback\n",
        "\n",
        "\n",
        "def _pick_columns_for_channels(features_hint: Any, df: pd.DataFrame, channels: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Pick which DataFrame columns to use as 'channels' when building a 2D observation.\n",
        "    Priority: artifact feature list (if present) → common OHLCV columns → other numeric.\n",
        "    \"\"\"\n",
        "    numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    cols: List[str] = []\n",
        "\n",
        "    if isinstance(features_hint, dict) and \"features\" in features_hint:\n",
        "        cand = [c for c in features_hint[\"features\"] if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
        "        if len(cand) >= channels:\n",
        "            cols = cand[:channels]\n",
        "\n",
        "    if not cols:\n",
        "        pref = [\"Close\", \"Volume\", \"Adj Close\", \"Open\", \"High\", \"Low\"]\n",
        "        cols = [c for c in pref if c in numeric]\n",
        "        cols += [c for c in numeric if c not in cols]\n",
        "        cols = cols[:channels]\n",
        "\n",
        "    if len(cols) < channels and cols:\n",
        "        while len(cols) < channels:\n",
        "            cols.append(cols[-1])  # duplicate last to fill\n",
        "    return cols[:channels]\n",
        "\n",
        "\n",
        "# ---- Observation builder (2D-aware with 1D fallback) ------------------------------------\n",
        "def prepare_observation_from_bars(\n",
        "    bars_df: pd.DataFrame,\n",
        "    features_hint: Any = None,\n",
        "    min_required_rows: int = 60,\n",
        "    expected_shape: Optional[tuple] = None,\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Convert recent OHLCV bars to a single observation.\n",
        "    If the model expects 2D (lookback, channels), we build a window using raw columns.\n",
        "    Otherwise we build a 1D feature vector in the artifact's column order.\n",
        "\n",
        "    Returns: (obs: np.ndarray, obs_timestamp_utc: int)\n",
        "    \"\"\"\n",
        "    feats_df = add_features_live(bars_df).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Timestamp for staleness guard\n",
        "    ts = pd.Timestamp.utcnow()\n",
        "    try:\n",
        "        idx_ts = pd.Timestamp(feats_df.index[-1])\n",
        "        ts = idx_ts.tz_convert(\"UTC\") if idx_ts.tzinfo else idx_ts.tz_localize(\"UTC\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # ---- 2D path (e.g., expected_shape == (10, 2)) --------------------------------------\n",
        "    if expected_shape is not None and len(expected_shape) == 2:\n",
        "        lookback, channels = int(expected_shape[0]), int(expected_shape[1])\n",
        "        cols = _pick_columns_for_channels(features_hint, feats_df, channels)\n",
        "\n",
        "        # Use the raw (or close-to-raw) columns; take tail(lookback) and pad if short\n",
        "        window_df = feats_df[cols].tail(lookback)\n",
        "        arr = window_df.to_numpy(dtype=np.float32)\n",
        "\n",
        "        if arr.shape[0] < lookback:\n",
        "            pad_rows = lookback - arr.shape[0]\n",
        "            arr = np.vstack([np.zeros((pad_rows, channels), dtype=np.float32), arr])\n",
        "\n",
        "        # Guard final shape exactly\n",
        "        arr = arr[-lookback:, :channels]\n",
        "        return arr.reshape(lookback, channels), int(ts.timestamp())\n",
        "\n",
        "    # ---- 1D fallback (artifact feature vector) ------------------------------------------\n",
        "    order = compute_art_feat_order(features_hint, feats_df)\n",
        "    if not order:\n",
        "        raise ValueError(\"No usable features after resolving artifact order.\")\n",
        "    feats_df = feats_df.dropna(subset=order)\n",
        "\n",
        "    if len(feats_df) < max(20, min_required_rows):\n",
        "        raise ValueError(f\"Not enough bars to compute features robustly (have {len(feats_df)}).\")\n",
        "\n",
        "    last = feats_df.iloc[-1]\n",
        "    obs = build_obs_from_row(last, order)\n",
        "    return obs.astype(np.float32), int(ts.timestamp())\n"
      ],
      "metadata": {
        "id": "88meE9-BgkON"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PPO paper-trading setup (Colab) ---\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Env toggles\n",
        "os.environ.update({\n",
        "    \"ARTIFACTS_DIR\": \"/content\",\n",
        "    \"TICKERS\": \"UNH\",\n",
        "    \"INF_DETERMINISTIC\": \"1\",  # PPO deterministic inference\n",
        "    \"DRY_RUN\": \"1\",            # no real orders\n",
        "    \"BARS_FEED\": \"iex\",        # paper feed\n",
        "    # \"BEST_WINDOW\": \"3\",      # optional; leave unset if you only have one model\n",
        "})\n",
        "\n",
        "# 2) Make sure artifacts are colocated\n",
        "ART = Path(os.environ[\"ARTIFACTS_DIR\"])\n",
        "NEED = [\n",
        "    \"ppo_UNH_window3_model.zip\",       # or your single model name\n",
        "    \"ppo_UNH_window3_vecnorm.pkl\",     # if you have it\n",
        "    \"ppo_UNH_window3_features.json\",   # if you saved feature order\n",
        "]\n",
        "DRIVE = Path(\"/content/drive/MyDrive\")\n",
        "for name in NEED:\n",
        "    dst = ART / name\n",
        "    if not dst.exists():\n",
        "        src = next(DRIVE.rglob(name), None)\n",
        "        if src:\n",
        "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "print(\"ARTIFACTS:\", sorted(p.name for p in ART.glob(\"ppo_UNH*\")))\n",
        "\n",
        "# 3) (Re)define INF_DETERMINISTIC in case main reads after this cell\n",
        "INF_DETERMINISTIC = os.getenv(\"INF_DETERMINISTIC\", \"1\").lower() in (\"1\",\"true\",\"yes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7iah8xGffTP",
        "outputId": "906dc68d-dbb6-4980-e31c-a2117d4cb803"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARTIFACTS: ['ppo_UNH_window3_features.json', 'ppo_UNH_window3_model.zip', 'ppo_UNH_window3_model_info.json', 'ppo_UNH_window3_probability_config.json', 'ppo_UNH_window3_vecnorm.pkl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Put these runtime knobs near the top of your script (before the picker) ---\n",
        "ARTIFACTS_DIR   = os.getenv(\"ARTIFACTS_DIR\", \"/content\")\n",
        "BEST_WINDOW_ENV = (os.getenv(\"BEST_WINDOW\", \"\").strip() or None)  # OPTIONAL\n",
        "\n",
        "# --- Helper stays the same (optional: used when multiple windows are present) ---\n",
        "def _extract_window_idx(path: Path) -> Optional[int]:\n",
        "    m = re.search(r\"_window(\\d+)_\", path.stem, re.IGNORECASE)\n",
        "    if not m: return None\n",
        "    try:\n",
        "        return int(m.group(1))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# --- REPLACE your pick_artifacts_for_ticker with this version ---\n",
        "def pick_artifacts_for_ticker(\n",
        "    ticker: str,\n",
        "    artifacts_dir: str,\n",
        "    best_window: Optional[str] = None  # <- no global in the default\n",
        ") -> Dict[str, Optional[Path]]:\n",
        "    p = Path(artifacts_dir)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Artifacts directory not found: {p.resolve()}\")\n",
        "\n",
        "    # Look for common model filenames\n",
        "    models = sorted(p.glob(f\"ppo_{ticker}_window*_model.zip\"))\n",
        "    if not models:\n",
        "        # fallbacks if you didn't include the window in the filename\n",
        "        models = sorted(p.glob(f\"ppo_{ticker}_model.zip\")) or sorted(p.glob(f\"*{ticker}*model.zip\"))\n",
        "        if not models:\n",
        "            found = \"\\n\".join(f\" - {x}\" for x in p.rglob(\"*model.zip\"))\n",
        "            raise FileNotFoundError(f\"No PPO model for {ticker} in {p.resolve()}.\\nSeen:\\n{found or ' (none)'}\")\n",
        "\n",
        "    # If an env-provided BEST_WINDOW is set, try to honor it; otherwise pick sensibly\n",
        "    chosen: Optional[Path] = None\n",
        "    if best_window:\n",
        "        chosen = next((m for m in models if f\"_window{best_window}_\" in m.stem), None)\n",
        "        if chosen is None:\n",
        "            logging.warning(\"BEST_WINDOW=%s not found; falling back to best available.\", best_window)\n",
        "\n",
        "    if chosen is None:\n",
        "        # If multiple windows exist, pick the one with the largest window index; else last sorted\n",
        "        with_idx = [(m, _extract_window_idx(m)) for m in models]\n",
        "        with_idx = [(m, w) for (m, w) in with_idx if w is not None]\n",
        "        chosen = max(with_idx, key=lambda t: t[1])[0] if with_idx else models[-1]\n",
        "\n",
        "    base   = chosen.stem.replace(\"_model\", \"\")\n",
        "    vecnorm = next(iter(sorted(p.glob(base + \"_vecnorm.pkl\"))), None)\n",
        "    feats   = next(iter(sorted(p.glob(base + \"_features.json\"))), None)\n",
        "\n",
        "    logging.info(f\"[{ticker}] model={chosen.name} | vecnorm={bool(vecnorm)} | features={bool(feats)}\")\n",
        "    return {\"model\": chosen, \"vecnorm\": vecnorm, \"features\": feats}\n",
        "\n",
        "def load_vecnormalize(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def load_features(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_ppo_model(model_path: Path):\n",
        "    return PPO.load(str(model_path))\n",
        "\n",
        "# ===== Alpaca helpers ====================================================================\n",
        "\n",
        "def get_recent_bars(api, symbol: str, limit: int = 200, timeframe=TimeFrame.Minute) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch recent bars and return a DataFrame with index=timestamp and\n",
        "    columns: Open, High, Low, Close, Volume. Reads BARS_FEED at call time.\n",
        "    If empty, retries with a 5-day start/end window.\n",
        "    \"\"\"\n",
        "    def _as_df(bars):\n",
        "        if hasattr(bars, \"df\"):\n",
        "            df = bars.df.copy()\n",
        "            if not df.empty:\n",
        "                if isinstance(df.index, pd.MultiIndex):\n",
        "                    df = df.xs(symbol, level=0)\n",
        "                df.index = pd.to_datetime(df.index, utc=True)\n",
        "                df = df.rename(columns={\"open\":\"Open\",\"high\":\"High\",\"low\":\"Low\",\"close\":\"Close\",\"volume\":\"Volume\"})\n",
        "                return df[[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]].sort_index()\n",
        "            return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "\n",
        "        # list-like fallback\n",
        "        rows = []\n",
        "        for b in bars:\n",
        "            ts = pd.to_datetime(getattr(b, \"t\", None), utc=True)\n",
        "            rows.append({\n",
        "                \"timestamp\": ts,\n",
        "                \"Open\":   float(getattr(b, \"o\", getattr(b, \"open\",  np.nan))),\n",
        "                \"High\":   float(getattr(b, \"h\", getattr(b, \"high\",  np.nan))),\n",
        "                \"Low\":    float(getattr(b, \"l\", getattr(b, \"low\",   np.nan))),\n",
        "                \"Close\":  float(getattr(b, \"c\", getattr(b, \"close\", np.nan))),\n",
        "                \"Volume\": float(getattr(b, \"v\", getattr(b, \"volume\",np.nan))),\n",
        "            })\n",
        "        return pd.DataFrame(rows).set_index(\"timestamp\").sort_index()\n",
        "\n",
        "    feed = os.getenv(\"BARS_FEED\", \"\").strip()  # read at call time\n",
        "    try:\n",
        "        logging.info(f\"[{symbol}] fetching {limit} {timeframe} bars (feed='{feed or 'default'}')\")\n",
        "        if feed:\n",
        "            bars = api.get_bars(symbol, timeframe, limit=limit, feed=feed)\n",
        "        else:\n",
        "            bars = api.get_bars(symbol, timeframe, limit=limit)\n",
        "        df = _as_df(bars)\n",
        "        if not df.empty:\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars(limit) failed: {e}\")\n",
        "\n",
        "    # Fallback: explicit 5-day window\n",
        "    try:\n",
        "        end   = datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
        "        start = (datetime.utcnow() - timedelta(days=5)).replace(microsecond=0).isoformat() + \"Z\"\n",
        "        logging.info(f\"[{symbol}] retry with window start={start} end={end} (feed='{feed or 'default'}')\")\n",
        "        if feed:\n",
        "            bars = api.get_bars(symbol, timeframe, start=start, end=end, feed=feed)\n",
        "        else:\n",
        "            bars = api.get_bars(symbol, timeframe, start=start, end=end)\n",
        "        return _as_df(bars)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars(start/end) failed: {e}\")\n",
        "        return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "\n",
        "def get_account_equity(api) -> float:\n",
        "    return float(api.get_account().equity)\n",
        "\n",
        "def get_position(api, symbol: str):\n",
        "    try:\n",
        "        return api.get_position(symbol)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_position_qty(api, symbol: str) -> int:\n",
        "    pos = get_position(api, symbol)\n",
        "    if not pos:\n",
        "        return 0\n",
        "    try:\n",
        "        return int(float(pos.qty))\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def get_last_price(api, symbol: str) -> float:\n",
        "    try:\n",
        "        bars = api.get_bars(symbol, TimeFrame.Minute, limit=1)\n",
        "        if len(bars) > 0:\n",
        "            b = bars[0]\n",
        "            close = getattr(b, \"c\", getattr(b, \"close\", None))\n",
        "            if close is not None:\n",
        "                return float(close)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_last_price via bars failed: {e}\")\n",
        "    pos = get_position(api, symbol)\n",
        "    if pos:\n",
        "        try:\n",
        "            return float(pos.avg_entry_price)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(\"nan\")\n",
        "\n",
        "def cancel_open_symbol_orders(api, symbol: str):\n",
        "    try:\n",
        "        for o in api.list_orders(status=\"open\"):\n",
        "            if o.symbol == symbol:\n",
        "                api.cancel_order(o.id)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] cancel orders failed: {e}\")\n",
        "\n",
        "def market_order_to_qty(api, symbol: str, side: str, qty: int):\n",
        "    if qty <= 0:\n",
        "        return None\n",
        "    if DRY_RUN:\n",
        "        logging.info(f\"[DRY_RUN] Would submit {side} {qty} {symbol} (market, day)\")\n",
        "        return None\n",
        "    try:\n",
        "        o = api.submit_order(\n",
        "            symbol=symbol,\n",
        "            side=side,\n",
        "            type=\"market\",\n",
        "            qty=qty,\n",
        "            time_in_force=\"day\"\n",
        "        )\n",
        "        logging.info(f\"[{symbol}] Submitted {side} {qty} (market) id={o.id}\")\n",
        "        return o\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[{symbol}] submit_order failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ===== Sizing / Risk =====================================================================\n",
        "def action_to_weight(action) -> Tuple[float, float, float]:\n",
        "    a = float(np.array(action).squeeze())\n",
        "    if a <= 0:\n",
        "        return 0.0, abs(np.tanh(a)), a\n",
        "    conf = abs(np.tanh(a))\n",
        "    if SIZING_MODE == \"linear\":\n",
        "        w = WEIGHT_CAP * conf\n",
        "    else:\n",
        "        w = 0.0 if conf < CONF_FLOOR else WEIGHT_CAP * (conf - CONF_FLOOR) / (1.0 - CONF_FLOOR)\n",
        "    return max(0.0, min(WEIGHT_CAP, w)), conf, a\n",
        "\n",
        "# Optional: avoid 0-share buys for small positive weights\n",
        "MIN_POS_QTY = 1\n",
        "def compute_target_qty(equity: float, price: float, target_weight: float) -> int:\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        return 0\n",
        "    notional = equity * target_weight\n",
        "    qty = int(notional // price)  # floor\n",
        "    if target_weight > 0 and qty == 0:\n",
        "        return MIN_POS_QTY\n",
        "    return max(0, qty)\n",
        "def compute_target_qty_by_cash(equity: float, price: float, target_weight: float, api=None) -> int:\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        return 0\n",
        "    cash_cap = float(api.get_account().cash) if api else equity\n",
        "    notional = min(cash_cap, equity * target_weight)\n",
        "    qty = int(notional // price)\n",
        "    if target_weight > 0 and qty == 0 and notional >= price:\n",
        "        return 1\n",
        "    return max(0, qty)\n",
        "\n",
        "\n",
        "def flatten_symbol(api, symbol: str):\n",
        "    qty = get_position_qty(api, symbol)\n",
        "    if qty == 0:\n",
        "        return\n",
        "    cancel_open_symbol_orders(api, symbol)\n",
        "    side = \"sell\" if qty > 0 else \"buy\"\n",
        "    market_order_to_qty(api, symbol, side, abs(qty))\n",
        "\n",
        "def rebalance_to_weight(api, symbol: str, equity: float, target_weight: float):\n",
        "    if target_weight <= 0:\n",
        "        flatten_symbol(api, symbol)\n",
        "        return\n",
        "    price    = get_last_price(api, symbol)\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        logging.warning(f\"[{symbol}] Price unavailable; skipping rebalance this cycle.\")\n",
        "        return\n",
        "    have_qty = get_position_qty(api, symbol)\n",
        "    want_qty = compute_target_qty_by_cash(equity, price, target_weight, api)\n",
        "    delta    = want_qty - have_qty\n",
        "\n",
        "    delta    = want_qty - have_qty\n",
        "    if delta == 0:\n",
        "        return\n",
        "    side = \"buy\" if delta > 0 else \"sell\"\n",
        "    market_order_to_qty(api, symbol, side, abs(delta))\n",
        "\n",
        "# ===== Simple TP/SL (optional) ============================================================\n",
        "def check_tp_sl_and_maybe_flatten(api, symbol: str) -> bool:\n",
        "    if TAKE_PROFIT_PCT <= 0 and STOP_LOSS_PCT <= 0:\n",
        "        return False\n",
        "    pos = get_position(api, symbol)\n",
        "    if not pos:\n",
        "        return False\n",
        "    try:\n",
        "        plpc = float(pos.unrealized_plpc)  # +0.031 = +3.1%\n",
        "    except Exception:\n",
        "        return False\n",
        "    if TAKE_PROFIT_PCT > 0 and plpc >= TAKE_PROFIT_PCT:\n",
        "        logging.info(f\"[{symbol}] TP hit ({plpc:.4f} >= {TAKE_PROFIT_PCT:.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    if STOP_LOSS_PCT > 0 and plpc <= -abs(STOP_LOSS_PCT):\n",
        "        logging.info(f\"[{symbol}] SL hit ({plpc:.4f} <= {-abs(STOP_LOSS_PCT):.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# ===== Inference ==========================================================================\n",
        "def infer_target_weight(model: PPO, vecnorm: Optional[VecNormalize], obs: np.ndarray) -> Tuple[float, float, float]:\n",
        "    x = obs\n",
        "    if vecnorm is not None and hasattr(vecnorm, \"normalize_obs\"):\n",
        "        try:\n",
        "            x = vecnorm.normalize_obs(x)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"VecNormalize.normalize_obs failed; using raw obs. Err: {e}\")\n",
        "    action, _ = model.predict(x, deterministic=INF_DETERMINISTIC)\n",
        "    return action_to_weight(action)\n",
        "\n",
        "# ===== Single-symbol step ================================================================\n",
        "def run_live_once_for_symbol(api, symbol: str, model: PPO, vecnorm: Optional[VecNormalize], features_hint: Optional[dict] = None):\n",
        "    # Discover the expected shape (e.g., (lookback, channels))\n",
        "    shape = expected_obs_shape(model, vecnorm)\n",
        "\n",
        "    # 1) Get fresh bars → features → obs\n",
        "    bars_df = get_recent_bars(api, symbol, limit=200, timeframe=TimeFrame.Minute)\n",
        "    if bars_df.empty:\n",
        "        logging.warning(f\"[{symbol}] No recent bars; skipping.\")\n",
        "        return\n",
        "\n",
        "    obs, obs_ts = prepare_observation_from_bars(\n",
        "        bars_df,\n",
        "        features_hint=features_hint,\n",
        "        min_required_rows=60,\n",
        "        expected_shape=shape,   # <-- IMPORTANT\n",
        "    )\n",
        "\n",
        "    # 2) Staleness guard\n",
        "    if utc_ts(now_utc()) - obs_ts > STALE_MAX_SEC:\n",
        "        logging.warning(f\"[{symbol}] Observation stale (> {STALE_MAX_SEC}s). Skipping.\")\n",
        "        return\n",
        "\n",
        "    # 3) Optional TP/SL pre-check\n",
        "    if check_tp_sl_and_maybe_flatten(api, symbol):\n",
        "        return\n",
        "\n",
        "    # 4) Inference → target weight\n",
        "    target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "    eq = get_account_equity(api)\n",
        "    logging.info(f\"[{symbol}] action={raw:.4f} conf={conf:.3f} → target_w={target_w:.3f} | equity=${eq:,.2f}\")\n",
        "    bar_time = bars_df.index[-1] if not bars_df.empty else pd.NaT\n",
        "    price    = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api, symbol)\n",
        "    log_trade(symbol, bar_time, int(target_w > 0.0), raw, target_w, conf, price, eq, DRY_RUN, note=\"live\")\n",
        "\n",
        "    # 5) Rebalance\n",
        "    rebalance_to_weight(api, symbol, eq, target_w)\n",
        "\n",
        "\n",
        "# ===== Live loop =========================================================================\n",
        "def run_live(tickers: List[str]):\n",
        "    api = init_alpaca()\n",
        "\n",
        "    # Load artifacts per ticker\n",
        "    per_ticker: Dict[str, Tuple[PPO, Optional[VecNormalize], Optional[dict]]] = {}\n",
        "    for t in tickers:\n",
        "        picks = pick_artifacts_for_ticker(\"UNH\", ARTIFACTS_DIR, best_window=BEST_WINDOW_ENV)\n",
        "        model      = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm    = load_vecnormalize(picks[\"vecnorm\"]) if picks[\"vecnorm\"] else None\n",
        "        feat_order = load_features(picks[\"features\"])\n",
        "        per_ticker[t] = (model, vecnorm, feat_order)\n",
        "\n",
        "    logging.info(f\"Starting live execution for: {tickers}\")\n",
        "    last_exec_at = now_utc() - timedelta(minutes=COOLDOWN_MIN)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            if not ensure_market_open(api):\n",
        "                logging.info(\"Market closed. Sleeping 30s.\")\n",
        "                time.sleep(30)\n",
        "                continue\n",
        "\n",
        "            if (now_utc() - last_exec_at) < timedelta(minutes=COOLDOWN_MIN):\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "\n",
        "            for t in tickers:\n",
        "                model, vecnorm, feat_hint = per_ticker[t]\n",
        "                run_live_once_for_symbol(api, t, model, vecnorm, features_hint=feat_hint)\n",
        "\n",
        "            last_exec_at = now_utc()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logging.info(\"KeyboardInterrupt: stopping live loop.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            logging.exception(f\"Live loop exception: {e}\")\n",
        "            time.sleep(5)\n",
        "\n",
        "# ===== UNH Diagnostic (friendly prints, works even if market is closed) ===================\n",
        "def unh_diagnostic(\n",
        "    dry_run: bool = True,\n",
        "    timeframe: TimeFrame = TimeFrame.Minute,\n",
        "    limit: int = 300\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs a one-off UNH evaluation, printing messages like your example:\n",
        "      Running strategy for UNH...\n",
        "      Model artifacts loaded for UNH\n",
        "      Prediction for UNH: 0 (1 = Buy, 0 = Sell)\n",
        "      Market is closed.\n",
        "    \"\"\"\n",
        "    ticker = \"UNH\"\n",
        "    print(f\"\\nRunning strategy for {ticker}...\")\n",
        "\n",
        "    # Stats for the summary\n",
        "    processed = 1\n",
        "    loaded    = 0\n",
        "    predicted = 0\n",
        "    closed    = 0\n",
        "    data_err  = 0\n",
        "    model_err = 0\n",
        "    orders    = 0\n",
        "\n",
        "    # Connect & snapshot counts\n",
        "    try:\n",
        "        api = init_alpaca()\n",
        "        positions_start = len(api.list_positions())\n",
        "        orders_start    = len(api.list_orders(status=\"open\"))\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Alpaca: {e}\")\n",
        "        return\n",
        "\n",
        "    # Load artifacts\n",
        "    try:\n",
        "        picks   = pick_artifacts_for_ticker(ticker, ARTIFACTS_DIR, best_window=BEST_WINDOW_ENV)\n",
        "        model   = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm = load_vecnormalize(picks[\"vecnorm\"]) if picks[\"vecnorm\"] else None\n",
        "        feats   = load_features(picks[\"features\"])\n",
        "        print(f\"Model artifacts loaded for {ticker}\")\n",
        "        loaded += 1\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load model for {ticker}: {e}\")\n",
        "        model_err += 1\n",
        "        # Summary\n",
        "        positions_end = len(api.list_positions())\n",
        "        orders_end    = len(api.list_orders(status=\"open\"))\n",
        "        print(\"\\n========== SUMMARY ==========\")\n",
        "        print(f\"Processed:         {processed}\")\n",
        "        print(f\"Models loaded:     {loaded}\")\n",
        "        print(f\"Predictions made:  {predicted}\")\n",
        "        print(f\"Market closed:     {closed}\")\n",
        "        print(f\"Data errors:       {data_err}\")\n",
        "        print(f\"Model errors:      {model_err}\")\n",
        "        print(f\"Orders submitted:  {orders} (dry_run={dry_run})\")\n",
        "        print(f\"Existing positions (start -> end): {positions_start} -> {positions_end}\")\n",
        "        print(f\"Open orders        (start -> end): {orders_start} -> {orders_end}\")\n",
        "        print(\"=============================\")\n",
        "        return\n",
        "\n",
        "    # Fetch bars (recent)\n",
        "    # ===== Fetch bars (recent) — shape-aware minimum =====\n",
        "    try:\n",
        "        # Discover model's expected obs shape (e.g., (lookback, channels))\n",
        "        shape = expected_obs_shape(model, vecnorm)\n",
        "        lookback = int(shape[0]) if (shape is not None and len(shape) == 2) else None\n",
        "\n",
        "        # Pull enough bars (3× lookback if known; otherwise 200)\n",
        "        bars_limit = max(200, (lookback or 0) * 3)\n",
        "        bars_df = get_recent_bars(api, ticker, limit=bars_limit, timeframe=timeframe)\n",
        "\n",
        "        # Dynamic minimum rows:\n",
        "        #  - If model is 2D (lookback, channels) -> need at least `lookback` rows\n",
        "        #  - If model is 1D vector -> keep your normal 60-row requirement (tune if desired)\n",
        "        min_rows_needed = lookback if lookback is not None else 60\n",
        "        if len(bars_df) < min_rows_needed:\n",
        "            print(f\"Not enough data for {ticker}: {len(bars_df)} rows (need ≥ {min_rows_needed})\")\n",
        "            data_err += 1\n",
        "            raise ValueError(\"insufficient bars\")\n",
        "\n",
        "        # Build an observation (pass the shape so we use the 2D path when available)\n",
        "        obs, obs_ts = prepare_observation_from_bars(\n",
        "            bars_df,\n",
        "            features_hint=feats,\n",
        "            min_required_rows=min_rows_needed,\n",
        "            expected_shape=shape,   # <-- critical for (lookback, channels) models\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching/processing bars for {ticker}: {e}\")\n",
        "        data_err += 1\n",
        "        bars_df = None\n",
        "        obs = None\n",
        "\n",
        "    # Predict (binary-like, derived from PPO weight)\n",
        "    signal = None\n",
        "    if obs is not None:\n",
        "        try:\n",
        "            target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "            signal = int(target_w > 0.0)  # 1 = Buy, 0 = Sell/Hold\n",
        "            print(f\"Prediction for {ticker}: {signal} (1 = Buy, 0 = Sell)\")\n",
        "            bar_time = bars_df.index[-1] if (bars_df is not None and not bars_df.empty) else pd.NaT\n",
        "            price    = float(bars_df[\"Close\"].iloc[-1]) if (bars_df is not None and not bars_df.empty) else get_last_price(api, ticker)\n",
        "            equity   = get_account_equity(api)\n",
        "            log_trade(ticker, bar_time, signal, raw, target_w, conf, price, equity, dry_run=True, note=\"diagnostic\")\n",
        "            predicted += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction error for {ticker}: {e}\")\n",
        "            model_err += 1\n",
        "\n",
        "    # Trade execution (dry-run friendly) + market-open info\n",
        "    try:\n",
        "        clock = api.get_clock()\n",
        "        if not clock.is_open:\n",
        "            print(\"Market is closed.\")\n",
        "            closed += 1\n",
        "        else:\n",
        "            # Only attempt order submission if not dry_run and we had a valid signal\n",
        "            FORCE_FIRST_BUY = os.getenv(\"FORCE_FIRST_BUY\",\"0\").lower() in (\"1\",\"true\",\"yes\")\n",
        "\n",
        "            # Only attempt order submission if not dry_run and we had a valid signal\n",
        "            if not dry_run and signal is not None:\n",
        "                FORCE_FIRST_BUY = os.getenv(\"FORCE_FIRST_BUY\",\"0\").lower() in (\"1\",\"true\",\"yes\")\n",
        "                has_position = False\n",
        "                try:\n",
        "                    pos = api.get_position(ticker)\n",
        "                    has_position = float(pos.qty) > 0\n",
        "                except APIError:\n",
        "                    has_position = False\n",
        "\n",
        "                if signal == 1 and (FORCE_FIRST_BUY or not has_position):\n",
        "                    api.submit_order(symbol=ticker, qty=1, side='buy', type='market', time_in_force='day')\n",
        "                    orders += 1\n",
        "                    print(f\"BUY order submitted for {ticker}\")\n",
        "\n",
        "                if signal == 1 and not has_position:\n",
        "                    api.submit_order(symbol=ticker, qty=1, side='buy', type='market', time_in_force='day')\n",
        "                    orders += 1\n",
        "                    print(f\"BUY order submitted for {ticker}\")\n",
        "                elif signal == 0 and has_position:\n",
        "                    api.submit_order(symbol=ticker, qty=1, side='sell', type='market', time_in_force='day')\n",
        "                    orders += 1\n",
        "                    print(f\"SELL order submitted for {ticker}\")\n",
        "                else:\n",
        "                    print(f\"No action taken for {ticker}\")\n",
        "            else:\n",
        "                if signal is not None:\n",
        "                    print(f\"(dry-run) No order submitted for {ticker} — signal={signal}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Trade/clock error for {ticker}: {e}\")\n",
        "\n",
        "    # Final snapshot + summary\n",
        "    positions_end = len(api.list_positions())\n",
        "    orders_end    = len(api.list_orders(status=\"open\"))\n",
        "    print(\"\\n========== SUMMARY ==========\")\n",
        "    print(f\"Processed:         {processed}\")\n",
        "    print(f\"Models loaded:     {loaded}\")\n",
        "    print(f\"Predictions made:  {predicted}\")\n",
        "    print(f\"Market closed:     {closed}\")\n",
        "    print(f\"Data errors:       {data_err}\")\n",
        "    print(f\"Model errors:      {model_err}\")\n",
        "    print(f\"Orders submitted:  {orders} (dry_run={dry_run})\")\n",
        "    print(f\"Existing positions (start -> end): {positions_start} -> {positions_end}\")\n",
        "    print(f\"Open orders        (start -> end): {orders_start} -> {orders_end}\")\n",
        "    print(\"=============================\")\n",
        "\n",
        "# ===== Utilities: quick sanity ============================================================\n",
        "def sanity_check(run_single_step: bool = True):\n",
        "    \"\"\"Connectivity + artifacts + optional one live step for first ticker.\"\"\"\n",
        "    logging.info(\"Sanity: keys=%s/%s base=%s\", bool(API_KEY), bool(API_SECRET), BASE_URL)\n",
        "    api = init_alpaca()\n",
        "    acct = api.get_account()\n",
        "    logging.info(\"Account status: %s | equity=%s | cash=%s\", acct.status, acct.equity, acct.cash)\n",
        "\n",
        "    if not TICKERS:\n",
        "        raise RuntimeError(\"No TICKERS configured.\")\n",
        "    for t in tickers:\n",
        "      picks = pick_artifacts_for_ticker(t0, ARTIFACTS_DIR, best_window=BEST_WINDOW_ENV)  # not 't'\n",
        "      logging.info(\"Artifacts for %s -> %s\", t0, {k: (str(v) if v else None) for k,v in picks.items()})\n",
        "\n",
        "    if run_single_step:\n",
        "        model   = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm = load_vecnormalize(picks[\"vecnorm\"]) if picks[\"vecnorm\"] else None\n",
        "        feat    = load_features(picks[\"features\"])\n",
        "        run_live_once_for_symbol(api, t0, model, vecnorm, features_hint=feat)\n",
        "        print(\"One-step OK for\", t0)\n",
        ""
      ],
      "metadata": {
        "id": "bACO23vYtPcR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Live-trade-only plotting add-on\n",
        "#   • plots only when fills exist\n",
        "#   • quiet (no chart) if no fills\n",
        "# ================================\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta, timezone\n",
        "\n",
        "# mark when your session started (so we only look for *new* fills)\n",
        "SESSION_START = datetime.now(timezone.utc)\n",
        "\n",
        "def _rfc3339(dt_utc: datetime) -> str:\n",
        "    \"\"\"RFC3339 string for Alpaca 'after' param (must be tz-aware UTC).\"\"\"\n",
        "    if dt_utc.tzinfo is None:\n",
        "        dt_utc = dt_utc.replace(tzinfo=timezone.utc)\n",
        "    else:\n",
        "        dt_utc = dt_utc.astimezone(timezone.utc)\n",
        "    # Alpaca accepts ISO8601/RFC3339 with 'Z'\n",
        "    return dt_utc.isoformat().replace(\"+00:00\", \"Z\")\n",
        "\n",
        "def get_fills_since(api, symbol: str, since_utc: datetime):\n",
        "    \"\"\"\n",
        "    Return a list of fills for `symbol` since `since_utc`.\n",
        "    Each item: dict(symbol, side, qty, price, ts[UTC]).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        acts = api.get_activities(\n",
        "            activity_types=\"FILL\",\n",
        "            after=_rfc3339(since_utc)\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.warning(\"get_activities(FILL) failed: %s\", e)\n",
        "        return []\n",
        "\n",
        "    fills = []\n",
        "    for a in acts or []:\n",
        "        sym = getattr(a, \"symbol\", None) or getattr(a, \"order_symbol\", None)\n",
        "        if str(sym or \"\") != symbol:\n",
        "            continue\n",
        "        t_raw = (\n",
        "            getattr(a, \"transaction_time\", None)\n",
        "            or getattr(a, \"filled_at\", None)\n",
        "            or getattr(a, \"activity_time\", None)\n",
        "        )\n",
        "        try:\n",
        "            ts = pd.to_datetime(t_raw, utc=True)\n",
        "        except Exception:\n",
        "            continue\n",
        "        fills.append({\n",
        "            \"symbol\": symbol,\n",
        "            \"side\":   (getattr(a, \"side\", \"\") or \"\").lower(),  # 'buy'/'sell'\n",
        "            \"qty\":    float(getattr(a, \"qty\", 0) or 0),\n",
        "            \"price\":  float(getattr(a, \"price\", np.nan)) if hasattr(a, \"price\") else np.nan,\n",
        "            \"ts\":     ts\n",
        "        })\n",
        "\n",
        "    fills.sort(key=lambda x: x[\"ts\"])\n",
        "    return fills\n",
        "\n",
        "def _bars_between(api, symbol: str, start_utc: pd.Timestamp, end_utc: pd.Timestamp) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch minute bars in [start, end] and return a normalized OHLCV DF.\n",
        "    Uses your helper get_recent_bars if available; else falls back to direct API.\n",
        "    \"\"\"\n",
        "    # Try your helper first (if present)\n",
        "    try:\n",
        "        # Heuristic: fetch enough minutes to cover the window\n",
        "        mins = max(30, int((end_utc - start_utc).total_seconds() // 60) + 5)\n",
        "        df = get_recent_bars(api, symbol, limit=mins, timeframe=TimeFrame.Minute)\n",
        "        # Filter to the exact window just in case\n",
        "        df = df.loc[(df.index >= start_utc) & (df.index <= end_utc)]\n",
        "        if not df.empty:\n",
        "            return df\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Fallback: direct call with start/end\n",
        "    bars = api.get_bars(\n",
        "        symbol,\n",
        "        TimeFrame.Minute,\n",
        "        start=start_utc.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "        end=end_utc.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "    )\n",
        "    if hasattr(bars, \"df\"):\n",
        "        df = bars.df.copy()\n",
        "        if isinstance(df.index, pd.MultiIndex):\n",
        "            df = df.xs(symbol, level=0)\n",
        "        df.index = pd.to_datetime(df.index, utc=True)\n",
        "        df = df.rename(columns={\"open\":\"Open\",\"high\":\"High\",\"low\":\"Low\",\"close\":\"Close\",\"volume\":\"Volume\"})\n",
        "        return df[[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]].sort_index()\n",
        "    else:\n",
        "        rows = []\n",
        "        for b in bars:\n",
        "            rows.append({\n",
        "                \"timestamp\": pd.to_datetime(getattr(b, \"t\", None), utc=True),\n",
        "                \"Open\":  float(getattr(b, \"o\", getattr(b, \"open\", np.nan))),\n",
        "                \"High\":  float(getattr(b, \"h\", getattr(b, \"high\", np.nan))),\n",
        "                \"Low\":   float(getattr(b, \"l\", getattr(b, \"low\", np.nan))),\n",
        "                \"Close\": float(getattr(b, \"c\", getattr(b, \"close\", np.nan))),\n",
        "                \"Volume\":float(getattr(b, \"v\", getattr(b, \"volume\", np.nan))),\n",
        "            })\n",
        "        return pd.DataFrame(rows).set_index(\"timestamp\").sort_index()\n",
        "\n",
        "def plot_live_trades(api, symbol: str, since_utc: datetime, window_before=\"30min\", window_after=\"30min\"):\n",
        "    \"\"\"\n",
        "    Plot minute price around *new* fills only.\n",
        "    If no fills -> prints a note and returns (no plot).\n",
        "    \"\"\"\n",
        "    fills = get_fills_since(api, symbol, since_utc)\n",
        "    if not fills:\n",
        "        print(f\"No new fills for {symbol}; not plotting.\")\n",
        "        return\n",
        "\n",
        "    # Build a single price series covering all fills + padding windows\n",
        "    first_ts = min(f[\"ts\"] for f in fills)\n",
        "    last_ts  = max(f[\"ts\"] for f in fills)\n",
        "    start = first_ts - pd.Timedelta(window_before)\n",
        "    end   = last_ts  + pd.Timedelta(window_after)\n",
        "\n",
        "    df = _bars_between(api, symbol, start, end)\n",
        "    if df.empty:\n",
        "        print(f\"No bars available around fills for {symbol}; not plotting.\")\n",
        "        return\n",
        "\n",
        "    # Plot\n",
        "    tz = \"America/New_York\"\n",
        "    idx_ny = df.index.tz_convert(tz)\n",
        "    close = df[\"Close\"]\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(idx_ny, close.values, lw=2, label=f\"{symbol} Close\")\n",
        "\n",
        "    # annotate each fill\n",
        "    for f in fills:\n",
        "        ts_ny = f[\"ts\"].tz_convert(tz)\n",
        "        # nearest bar price for marker\n",
        "        try:\n",
        "            near_idx = close.index.get_indexer([f[\"ts\"]], method=\"nearest\")[0]\n",
        "            y = close.iloc[near_idx]\n",
        "        except Exception:\n",
        "            y = np.nan\n",
        "\n",
        "        if f[\"side\"] == \"buy\":\n",
        "            plt.scatter([ts_ny], [y], marker=\"^\", s=80, label=\"BUY fill\", zorder=5)\n",
        "        elif f[\"side\"] == \"sell\":\n",
        "            plt.scatter([ts_ny], [y], marker=\"v\", s=80, label=\"SELL fill\", zorder=5)\n",
        "        else:\n",
        "            plt.scatter([ts_ny], [y], s=70, label=\"FILL\", zorder=5)\n",
        "\n",
        "        # optional dashed vline\n",
        "        plt.axvline(ts_ny, linestyle=\"--\", alpha=0.2)\n",
        "\n",
        "    plt.title(f\"{symbol} — Live fills\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DibQP-mRtcJE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Trade & Position Summary (UNH-only) ---\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Report only what's in your env (default UNH)\n",
        "tickers_to_report = [t.strip().upper() for t in os.getenv(\"TICKERS\", \"UNH\").split(\",\") if t.strip()]\n",
        "\n",
        "# Where your trade logs live; default to ARTIFACTS_DIR\n",
        "ART_DIR = Path(os.getenv(\"ARTIFACTS_DIR\", \"/content\"))\n",
        "RESULTS_DIR = Path(os.getenv(\"RESULTS_DIR\", str(ART_DIR)))\n",
        "\n",
        "print(\"\\nTrade Summary:\")\n",
        "for ticker in tickers_to_report:\n",
        "    log_path = RESULTS_DIR / f\"trade_log_{ticker}.csv\"\n",
        "    if not log_path.exists():\n",
        "        print(f\"{ticker}: no trades logged yet.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(log_path, on_bad_lines=\"skip\",\n",
        "                         parse_dates=[\"log_time\",\"bar_time\"], infer_datetime_format=True)\n",
        "\n",
        "        # Count by 'signal' (or 'action' if older logs)\n",
        "        key = \"signal\" if \"signal\" in df.columns else (\"action\" if \"action\" in df.columns else None)\n",
        "        if key:\n",
        "            counts = df[key].value_counts(dropna=False).to_dict()\n",
        "            print(f\"{ticker}: {counts}\")\n",
        "        else:\n",
        "            print(f\"{ticker}: log present but missing 'signal'/'action' columns.\")\n",
        "\n",
        "        # Optional confidence histogram\n",
        "        if \"confidence\" in df.columns and df[\"confidence\"].notna().any():\n",
        "            plt.figure(figsize=(8, 3.5))\n",
        "            df[\"confidence\"].dropna().plot(kind=\"hist\", bins=10, edgecolor=\"black\")\n",
        "            plt.title(f\"{ticker} - Confidence Distribution\")\n",
        "            plt.xlabel(\"confidence\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # Optional: PPO-specific stats\n",
        "        for col in [\"weight\", \"raw_action\"]:\n",
        "            if col in df.columns and df[col].notna().any():\n",
        "                s = df[col].dropna()\n",
        "                print(f\"{ticker} {col}: mean={s.mean():.3f}, std={s.std():.3f}, \"\n",
        "                      f\"min={s.min():.3f}, max={s.max():.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{ticker}: could not summarize trades: {e}\")\n",
        "\n",
        "# --- Position Summary (unchanged) ---\n",
        "try:\n",
        "    if 'api' not in globals():\n",
        "        api = init_alpaca()\n",
        "    positions = api.list_positions()\n",
        "    total_market_value = 0.0\n",
        "    print(\"\\nPosition Summary:\")\n",
        "    for p in positions:\n",
        "        mv = float(p.market_value)\n",
        "        total_market_value += mv\n",
        "        print(f\"  {p.symbol}: {p.qty} shares @ ${float(p.current_price):.2f} | Value: ${mv:,.2f}\")\n",
        "    print(f\"\\nTotal Market Value: ${total_market_value:,.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not summarize positions: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVDk8hn5wJqY",
        "outputId": "f860ad0e-29d4-4a72-dc87-9d76f5dc7aa1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trade Summary:\n",
            "UNH: no trades logged yet.\n",
            "\n",
            "Position Summary:\n",
            "  AAPL: 2 shares @ $234.35 | Value: $468.70\n",
            "  ABT: 1 shares @ $131.95 | Value: $131.95\n",
            "  BRK.B: 1 shares @ $492.81 | Value: $492.81\n",
            "  MDT: 1 shares @ $94.07 | Value: $94.07\n",
            "  PG: 1 shares @ $159.50 | Value: $159.50\n",
            "  PM: 1 shares @ $164.74 | Value: $164.74\n",
            "\n",
            "Total Market Value: $1,511.77\n"
          ]
        }
      ]
    }
  ]
}