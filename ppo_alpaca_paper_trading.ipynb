{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/exploratory_daytrading/blob/main/ppo_alpaca_paper_trading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-DEy5gEqqEi",
        "outputId": "4fda1e79-e97f-42cb-9859-8201a1848e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping stable-baselines3 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping shimmy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: gymnasium 1.2.0\n",
            "Uninstalling gymnasium-1.2.0:\n",
            "  Successfully uninstalled gymnasium-1.2.0\n",
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "\u001b[33mWARNING: Skipping autorom as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping AutoROM.accept-rom-license as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: ale-py 0.11.2\n",
            "Uninstalling ale-py-0.11.2:\n",
            "  Successfully uninstalled ale-py-0.11.2\n",
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting shimmy==1.3.0\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting stable-baselines3==2.3.0\n",
            "  Downloading stable_baselines3-2.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.3.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->stable-baselines3==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13->stable-baselines3==2.3.0) (3.0.2)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'stable-baselines3' candidate (version 2.3.0 at https://files.pythonhosted.org/packages/51/0b/6539076ed58343f1404dea0462167b079b5264508b8e5bbed01cea9f66b8/stable_baselines3-2.3.0-py3-none-any.whl (from https://pypi.org/simple/stable-baselines3/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Loading broken with PyTorch 1.13\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium, shimmy, stable-baselines3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 shimmy-1.3.0 stable-baselines3-2.3.0\n",
            "Collecting alpaca-trade-api\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Collecting gym-anytrading\n",
            "  Downloading gym_anytrading-2.0.0-py3-none-any.whl.metadata (292 bytes)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.32.4)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.8.0)\n",
            "Collecting websockets<11,>=9.0 (from alpaca-trade-api)\n",
            "  Downloading websockets-10.4.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting msgpack==1.0.3 (from alpaca-trade-api)\n",
            "  Downloading msgpack-1.0.3.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (3.12.15)\n",
            "Collecting PyYAML==6.0.1 (from alpaca-trade-api)\n",
            "  Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting deprecation==2.1.0 (from alpaca-trade-api)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (25.0)\n",
            "Requirement already satisfied: gymnasium>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from gym-anytrading) (0.29.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from gym-anytrading) (3.10.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.20.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.1->gym-anytrading) (1.17.0)\n",
            "Downloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (724 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gym_anytrading-2.0.0-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: msgpack, ta, websockets\n",
            "  Building wheel for msgpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-1.0.3-cp312-cp312-linux_x86_64.whl size=15688 sha256=e8c24c1e746c8509229dcf18f6aa84d0860f068c19aaf62ddd2251eac0c581be\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/bd/3f/f043e8f634db9c90ae128d631f43ae9990eef01274a63291f9\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=58a1ad3333c8d10c7aa823db6dd63a4006db677c3004f5480f364221a21d084e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-linux_x86_64.whl size=107327 sha256=5c90504a68d779191866a9b2d11a569c871a3ea97746c711089126f07b59eed8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/cf/6d/5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
            "Successfully built msgpack ta websockets\n",
            "Installing collected packages: msgpack, websockets, urllib3, PyYAML, deprecation, ta, gym-anytrading, alpaca-trade-api\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.1\n",
            "    Uninstalling msgpack-1.1.1:\n",
            "      Successfully uninstalled msgpack-1.1.1\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\n",
            "google-adk 1.14.1 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\n",
            "google-adk 1.14.1 requires websockets<16.0.0,>=15.0.1, but you have websockets 10.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "google-genai 1.38.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.1 alpaca-trade-api-3.2.0 deprecation-2.1.0 gym-anytrading-2.0.0 msgpack-1.0.3 ta-0.11.0 urllib3-1.26.20 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Clean any partials\n",
        "!pip uninstall -y stable-baselines3 shimmy gymnasium gym autorom AutoROM.accept-rom-license ale-py\n",
        "\n",
        "# Install the compatible trio (no [extra] to avoid Atari deps)\n",
        "!pip install \"gymnasium==0.29.1\" \"shimmy==1.3.0\" \"stable-baselines3==2.3.0\"\n",
        "\n",
        "# Your other libs (safe to keep separate)\n",
        "!pip install alpaca-trade-api ta python-dotenv gym-anytrading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyc0Dr0D86cA",
        "outputId": "6b85ca35-f380-4e71-aff6-bb8e67833322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.8.0+cu126\n",
            "gymnasium: 0.29.1\n",
            "shimmy: 1.3.0\n",
            "stable-baselines3: 2.3.0\n",
            "alpaca-trade-api: 3.2.0\n",
            "websockets: 10.4\n",
            "pywavelets: 1.8.0\n"
          ]
        }
      ],
      "source": [
        "import torch, gymnasium, shimmy, stable_baselines3 as sb3\n",
        "import alpaca_trade_api, websockets, pywt\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"gymnasium:\", gymnasium.__version__)\n",
        "print(\"shimmy:\", shimmy.__version__)\n",
        "print(\"stable-baselines3:\", sb3.__version__)\n",
        "print(\"alpaca-trade-api:\", alpaca_trade_api.__version__)\n",
        "print(\"websockets:\", websockets.__version__)\n",
        "print(\"pywavelets:\", pywt.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cM2YQVmcZjEu",
        "outputId": "36defc91-79fd-4570-8e81-f345641f7c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Upload your .env (or Alpaca_keys.env.txt). Cancel if already on Drive.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b4116698-4dea-4891-89c5-62564e84b59d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b4116698-4dea-4891-89c5-62564e84b59d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Alpaca_keys.env.txt to Alpaca_keys.env.txt\n",
            "Saved env → /content/drive/MyDrive/AlpacaPaper/.env\n",
            "Upload your artifacts (ppo_*_model.zip, *_vecnorm.pkl, *_features.json or .txt).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-473d239b-59de-4194-b0db-2e548e8d565f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-473d239b-59de-4194-b0db-2e548e8d565f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving ppo_GE_window1_features.json to ppo_GE_window1_features.json\n",
            "Saving ppo_GE_window1_model_info.json to ppo_GE_window1_model_info.json\n",
            "Saving ppo_GE_window1_model.zip to ppo_GE_window1_model.zip\n",
            "Saving ppo_GE_window1_probability_config.json to ppo_GE_window1_probability_config.json\n",
            "Saving ppo_GE_window1_vecnorm.pkl to ppo_GE_window1_vecnorm.pkl\n",
            "Saving ppo_UNH_window3_features.json to ppo_UNH_window3_features.json\n",
            "Saving ppo_UNH_window3_model_info.json to ppo_UNH_window3_model_info.json\n",
            "Saving ppo_UNH_window3_model.zip to ppo_UNH_window3_model.zip\n",
            "Saving ppo_UNH_window3_probability_config.json to ppo_UNH_window3_probability_config.json\n",
            "Saving ppo_UNH_window3_vecnorm.pkl to ppo_UNH_window3_vecnorm.pkl\n",
            "Artifacts now in: ['ppo_GE_window1_features.json', 'ppo_GE_window1_model.zip', 'ppo_GE_window1_model_info.json', 'ppo_GE_window1_probability_config.json', 'ppo_GE_window1_vecnorm.pkl', 'ppo_UNH_window3_features.json', 'ppo_UNH_window3_model.zip', 'ppo_UNH_window3_model_info.json', 'ppo_UNH_window3_probability_config.json', 'ppo_UNH_window3_vecnorm.pkl']\n",
            "=== CONFIG ===\n",
            "Project root  : /content/drive/MyDrive/AlpacaPaper\n",
            "ARTIFACTS_DIR : /content/drive/MyDrive/AlpacaPaper/artifacts\n",
            "RESULTS_DIR   : /content/drive/MyDrive/AlpacaPaper/results/2025-09-26\n",
            "Tickers       : ['UNH', 'GE']\n",
            "DRY_RUN       : False | BARS_FEED: iex | USE_FRACTIONALS: True\n",
            "Artifacts present: ['ppo_GE_window1_features.json', 'ppo_GE_window1_model.zip', 'ppo_GE_window1_model_info.json', 'ppo_GE_window1_probability_config.json', 'ppo_GE_window1_vecnorm.pkl', 'ppo_UNH_window3_features.json', 'ppo_UNH_window3_model.zip', 'ppo_UNH_window3_model_info.json', 'ppo_UNH_window3_probability_config.json', 'ppo_UNH_window3_vecnorm.pkl']\n",
            "==============\n",
            "SIZING_MODE: linear -> linear\n",
            "CONF_FLOOR:  0.20 -> 0.20\n",
            "TAKE_PROFIT_PCT: 0.0500 -> 0.0500\n",
            "STOP_LOSS_PCT:   0.0300 -> 0.0300\n",
            "BARS_FEED:     iex -> (default)\n",
            "COOLDOWN_MIN:  5 -> 1\n",
            "STALE_MAX_SEC: 120 -> 1800\n",
            "EFFECTIVE (after overrides)\n",
            "DRY_RUN      : False | BARS_FEED:  | COOLDOWN_MIN: 1 | STALE_MAX_SEC: 1800\n",
            "\n",
            "Running strategy for UNH...\n",
            "Model artifacts loaded for UNH\n",
            "Prediction for UNH: 0 (1 = Buy, 0 = Sell)\n",
            "raw=-0.3604 conf=0.346 target_w=0.000 price=$344.42 equity=$99,988.10\n",
            "No action taken for UNH\n",
            "\n",
            "========== SUMMARY ==========\n",
            "Processed:         1\n",
            "Models loaded:     1\n",
            "Predictions made:  1\n",
            "Market closed:     0\n",
            "Orders submitted:  0 (dry_run=False)\n",
            "Existing positions (start -> end): 1 -> 1\n",
            "Open orders        (start -> end): 0 -> 0\n",
            "=============================\n",
            "\n",
            "Running strategy for GE...\n",
            "Model artifacts loaded for GE\n",
            "Prediction for GE: 1 (1 = Buy, 0 = Sell)\n",
            "raw=0.1859 conf=0.184 target_w=0.046 price=$294.03 equity=$99,988.10\n",
            "No action taken for GE\n",
            "\n",
            "========== SUMMARY ==========\n",
            "Processed:         1\n",
            "Models loaded:     1\n",
            "Predictions made:  1\n",
            "Market closed:     0\n",
            "Orders submitted:  0 (dry_run=False)\n",
            "Existing positions (start -> end): 1 -> 1\n",
            "Open orders        (start -> end): 0 -> 0\n",
            "=============================\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=12905s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8225s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=12966s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8286s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13026s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8346s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13087s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8407s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13148s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8468s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13209s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8530s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13271s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8591s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13331s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8651s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13392s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8712s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13453s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8773s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13513s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8834s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13575s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8895s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13636s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=8956s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13697s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9017s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13757s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9077s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13818s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9138s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13879s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9199s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=13940s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9260s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14001s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9321s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14062s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9382s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14122s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9442s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14183s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9503s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14244s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9564s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14305s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9625s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14366s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9686s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14427s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9747s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14487s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9807s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14548s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9868s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14610s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9930s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14670s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=9991s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14732s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10052s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14792s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10112s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14853s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10173s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14914s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10234s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=14976s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10296s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15036s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10356s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15098s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10418s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15159s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10479s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15220s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10540s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15281s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10601s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15341s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10661s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15402s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10722s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15463s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10783s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15525s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10845s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15585s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10906s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15646s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=10966s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15707s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11027s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15768s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11088s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15830s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11150s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15891s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11211s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=15952s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11272s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16013s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11333s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16073s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11393s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16135s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11455s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16196s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11516s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16256s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11576s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16317s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11637s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16378s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11698s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16439s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11759s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16500s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11820s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16562s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11882s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16622s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=11942s | feed='default'\n",
            "[UNH] last bar: 2025-09-26 15:21:00+00:00 | age=16683s | feed='default'\n",
            "[GE] last bar: 2025-09-26 16:39:00+00:00 | age=12003s | feed='default'\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-09-26/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        }
      ],
      "source": [
        "# ======================= PPO Alpaca Paper Trading — End-to-End =======================\n",
        "# Runs an RL-based paper trading loop on Alpaca, logs per-ticker CSVs, and saves plots.\n",
        "# Works in Google Colab (saves to Drive) or locally (saves to ./AlpacaPaper/*).\n",
        "# ====================================================================================\n",
        "\n",
        "# --- Imports & Setup ----------------------------------------------------------------\n",
        "import os, re, json, csv, shutil, logging, pickle, warnings, time, math, gc\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")   # save-to-file only; no inline rendering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import alpaca_trade_api as tradeapi\n",
        "from alpaca_trade_api.rest import TimeFrame, APIError\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "\n",
        "# --- Detect Colab and mount Drive ---------------------------------------------------\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    from google.colab import drive, files  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# --- Project paths (Drive in Colab; cwd locally) -----------------------------------\n",
        "if IN_COLAB:\n",
        "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/AlpacaPaper\")\n",
        "else:\n",
        "    PROJECT_ROOT = Path.cwd() / \"AlpacaPaper\"\n",
        "\n",
        "\n",
        "# --- Optional: Upload helpers (Colab only) ------------------------------------------\n",
        "def upload_env_and_artifacts_in_colab():\n",
        "    \"\"\"\n",
        "    In Colab this will prompt for:\n",
        "      1) .env (or Alpaca_keys.env.txt)  -> moves to PROJECT_ROOT/.env\n",
        "      2) Any model/feature/vecnorm files -> moves to ARTIFACTS_DIR\n",
        "\n",
        "    Comment out this function call if you don't want upload prompts.\n",
        "    \"\"\"\n",
        "    if not IN_COLAB:\n",
        "        return\n",
        "\n",
        "    print(\"Upload your .env (or Alpaca_keys.env.txt). Cancel if already on Drive.\")\n",
        "    up = files.upload()\n",
        "    if up:\n",
        "        # Try common names\n",
        "        if \"Alpaca_keys.env.txt\" in up:\n",
        "            src = Path(\"Alpaca_keys.env.txt\")\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env → {dst}\")\n",
        "        elif \".env\" in up:\n",
        "            src = Path(\".env\")\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env → {dst}\")\n",
        "        else:\n",
        "            # If user picked a different name, move the first file to .env\n",
        "            any_name = next(iter(up.keys()))\n",
        "            src = Path(any_name)\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env (renamed {any_name}) → {dst}\")\n",
        "\n",
        "    print(\"Upload your artifacts (ppo_*_model.zip, *_vecnorm.pkl, *_features.json or .txt).\")\n",
        "    up2 = files.upload()\n",
        "    for name in up2.keys():\n",
        "        shutil.move(name, ARTIFACTS_DIR / name)\n",
        "    print(\"Artifacts now in:\", sorted(p.name for p in ARTIFACTS_DIR.iterdir()))\n",
        "\n",
        "def _maybe_convert_features_txt_to_json():\n",
        "    \"\"\"\n",
        "    Convert any 'features_<TICKER>.txt' into 'ppo_<TICKER>_features.json' (simple list).\n",
        "    \"\"\"\n",
        "    for p in ARTIFACTS_DIR.glob(\"features_*.txt\"):\n",
        "        ticker = re.sub(r\"^features_|\\.txt$\", \"\", p.name, flags=re.IGNORECASE)\n",
        "        try:\n",
        "            raw = p.read_text().strip()\n",
        "            items = [x.strip() for x in raw.replace(\",\", \"\\n\").splitlines() if x.strip()]\n",
        "            out = {\"features\": items}\n",
        "            out_path = ARTIFACTS_DIR / f\"ppo_{ticker}_features.json\"\n",
        "            out_path.write_text(json.dumps(out, indent=2))\n",
        "            print(f\"Converted {p.name} → {out_path.name}  ({len(items)} features)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not convert {p.name}: {e}\")\n",
        "\n",
        "def _maybe_rename_vecnorm_scaler():\n",
        "    \"\"\"\n",
        "    Rename any 'scaler_<TICKER>.pkl' to 'ppo_<TICKER>_vecnorm.pkl'.\n",
        "    \"\"\"\n",
        "    for p in ARTIFACTS_DIR.glob(\"scaler_*.pkl\"):\n",
        "        ticker = re.sub(r\"^scaler_|\\.pkl$\", \"\", p.name, flags=re.IGNORECASE)\n",
        "        dst = ARTIFACTS_DIR / f\"ppo_{ticker}_vecnorm.pkl\"\n",
        "        if not dst.exists():\n",
        "            shutil.move(str(p), str(dst))\n",
        "            print(f\"Renamed {p.name} → {dst.name}\")\n",
        "\n",
        "# --- Env & logging ------------------------------------------------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load env (supports PROJECT_ROOT/.env)\n",
        "env_candidates = [PROJECT_ROOT / \".env\", Path(\".env\")]\n",
        "for env_path in env_candidates:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(dotenv_path=env_path, override=True)\n",
        "        break\n",
        "else:\n",
        "    load_dotenv(override=True)  # fallback to default search\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "# Baseline env defaults (can be overridden by your .env or shell)\n",
        "os.environ.setdefault(\"APCA_API_BASE_URL\", \"https://paper-api.alpaca.markets\")\n",
        "os.environ.setdefault(\"DRY_RUN\", \"0\")                 # 0 => place PAPER orders on PAPER endpoint\n",
        "os.environ.setdefault(\"AUTO_RUN_LIVE\", \"1\")           # loop when market open\n",
        "os.environ.setdefault(\"SIZING_MODE\", \"linear\")        # or \"threshold\"\n",
        "os.environ.setdefault(\"WEIGHT_CAP\", \"0.25\")           # max 25% per symbol\n",
        "os.environ.setdefault(\"CONF_FLOOR\", \"0.20\")\n",
        "os.environ.setdefault(\"ENTER_CONF_MIN\", \"0.08\")\n",
        "os.environ.setdefault(\"ENTER_WEIGHT_MIN\", \"0.015\")\n",
        "os.environ.setdefault(\"EXIT_WEIGHT_MAX\", \"0.003\")\n",
        "os.environ.setdefault(\"COOLDOWN_MIN\", \"5\")\n",
        "os.environ.setdefault(\"TAKE_PROFIT_PCT\", \"0.05\")\n",
        "os.environ.setdefault(\"STOP_LOSS_PCT\", \"0.03\")\n",
        "os.environ.setdefault(\"BARS_FEED\", \"iex\")\n",
        "os.environ.setdefault(\"INF_DETERMINISTIC\", \"1\")\n",
        "os.environ.setdefault(\"STALE_MAX_SEC\", \"120\")\n",
        "os.environ.setdefault(\"SEED_FIRST_SHARE\", \"1\")\n",
        "os.environ.setdefault(\"USE_FRACTIONALS\", \"1\")\n",
        "os.environ.setdefault(\"TICKERS\", \"UNH,GE\")            # <-- change to your tickers (comma-separated)\n",
        "os.environ.setdefault(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\"))\n",
        "os.environ.setdefault(\"RESULTS_ROOT\",  str(PROJECT_ROOT / \"results\"))\n",
        "\n",
        "# --- Resolve paths from env (single source of truth for outputs) --------------------\n",
        "RUN_DATE      = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
        "ARTIFACTS_DIR = Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")))\n",
        "RESULTS_ROOT  = Path(os.getenv(\"RESULTS_ROOT\",  str(PROJECT_ROOT / \"results\")))\n",
        "RESULTS_DIR   = RESULTS_ROOT / RUN_DATE\n",
        "LATEST_DIR    = RESULTS_ROOT / \"latest\"\n",
        "\n",
        "for p in (ARTIFACTS_DIR, RESULTS_DIR, LATEST_DIR):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# File targets (daily + a 'latest' copy for quick viewing)\n",
        "TRADE_LOG_CSV    = RESULTS_DIR / \"trade_log_master.csv\"\n",
        "EQUITY_LOG_CSV   = RESULTS_DIR / \"equity_log.csv\"\n",
        "PLOT_PATH        = RESULTS_DIR / \"equity_curve.png\"\n",
        "PLOT_PATH_LATEST = LATEST_DIR / \"equity_curve.png\"\n",
        "EQUITY_LOG_LATEST = LATEST_DIR / \"equity_log.csv\"   # <— add this\n",
        "TRADE_LOG_LATEST  = LATEST_DIR / \"trade_log_master.csv\"\n",
        "\n",
        "if IN_COLAB:\n",
        "    upload_env_and_artifacts_in_colab()\n",
        "    _maybe_convert_features_txt_to_json()\n",
        "    _maybe_rename_vecnorm_scaler()\n",
        "    # NEW: re-load .env now that it exists\n",
        "    load_dotenv(dotenv_path=PROJECT_ROOT / \".env\", override=True)\n",
        "\n",
        "API_KEY    = os.getenv(\"APCA_API_KEY_ID\")     or os.getenv(\"ALPACA_API_KEY_ID\")     or \"\"\n",
        "API_SECRET = os.getenv(\"APCA_API_SECRET_KEY\") or os.getenv(\"ALPACA_API_SECRET_KEY\") or \"\"\n",
        "BASE_URL   = os.getenv(\"APCA_API_BASE_URL\")   or os.getenv(\"ALPACA_API_BASE_URL\")   or \"https://paper-api.alpaca.markets\"\n",
        "\n",
        "# --- Runtime knobs -----------------------------------------------------------------\n",
        "BEST_WINDOW_ENV   = (os.getenv(\"BEST_WINDOW\", \"\").strip() or None)  # e.g., \"3\"\n",
        "TICKERS = [s.strip().upper() for s in os.getenv(\"TICKERS\", \"UNH,GE\").split(\",\") if s.strip()]\n",
        "BARS_FEED         = os.getenv(\"BARS_FEED\", \"iex\").strip()\n",
        "DRY_RUN           = os.getenv(\"DRY_RUN\", \"1\").lower() in (\"1\",\"true\",\"yes\")\n",
        "INF_DETERMINISTIC = os.getenv(\"INF_DETERMINISTIC\", \"1\").lower() in (\"1\",\"true\",\"yes\")\n",
        "COOLDOWN_MIN      = int(os.getenv(\"COOLDOWN_MIN\", \"5\"))\n",
        "STALE_MAX_SEC     = int(os.getenv(\"STALE_MAX_SEC\", \"120\"))\n",
        "\n",
        "# Entry/exit sensitivity\n",
        "SIZING_MODE       = os.getenv(\"SIZING_MODE\", \"linear\")          # \"linear\" | \"threshold\"\n",
        "WEIGHT_CAP        = float(os.getenv(\"WEIGHT_CAP\", \"0.25\"))\n",
        "ENTER_CONF_MIN    = float(os.getenv(\"ENTER_CONF_MIN\", \"0.08\"))\n",
        "ENTER_WEIGHT_MIN  = float(os.getenv(\"ENTER_WEIGHT_MIN\", \"0.015\"))\n",
        "EXIT_WEIGHT_MAX   = float(os.getenv(\"EXIT_WEIGHT_MAX\", \"0.003\"))\n",
        "SEED_FIRST_SHARE  = os.getenv(\"SEED_FIRST_SHARE\",\"1\").lower() in (\"1\",\"true\",\"yes\")\n",
        "USE_FRACTIONALS   = os.getenv(\"USE_FRACTIONALS\",\"1\").lower() in (\"1\",\"true\",\"yes\")\n",
        "\n",
        "\n",
        "# Sizing / TP-SL\n",
        "CONF_FLOOR      = float(os.getenv(\"CONF_FLOOR\", \"0.20\"))    # Threshold mode only\n",
        "TAKE_PROFIT_PCT = float(os.getenv(\"TAKE_PROFIT_PCT\", \"0.05\"))\n",
        "STOP_LOSS_PCT   = float(os.getenv(\"STOP_LOSS_PCT\", \"0.03\"))\n",
        "\n",
        "# --- Time helpers -------------------------------------------------------------------\n",
        "def now_utc() -> datetime:\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "def utc_ts(dt_like) -> int:\n",
        "    \"\"\"\n",
        "    Return epoch seconds (UTC) for int/float epochs, datetimes, strings, numpy datetime, etc.\n",
        "    \"\"\"\n",
        "    # Fast paths for numbers\n",
        "    if isinstance(dt_like, (int, np.integer)):\n",
        "        return int(dt_like)\n",
        "    if isinstance(dt_like, (float, np.floating)):\n",
        "        return int(dt_like)\n",
        "    ts = pd.Timestamp(dt_like)\n",
        "    if ts.tzinfo is None:\n",
        "        ts = ts.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        ts = ts.tz_convert(\"UTC\")\n",
        "    return int(ts.value // 10**9)\n",
        "\n",
        "def utcnow_iso() -> str:\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "# --- Env override helpers ------------------------------------------------------------\n",
        "def apply_env_overrides(verbose: bool = False):\n",
        "    \"\"\"Re-read SIZING_MODE / CONF_FLOOR from environment and update the globals.\"\"\"\n",
        "    global SIZING_MODE, CONF_FLOOR\n",
        "    old_mode, old_floor = SIZING_MODE, CONF_FLOOR\n",
        "    SIZING_MODE = os.getenv(\"SIZING_MODE\", SIZING_MODE)\n",
        "    CONF_FLOOR  = float(os.getenv(\"CONF_FLOOR\", str(CONF_FLOOR)))\n",
        "    if verbose:\n",
        "        print(f\"SIZING_MODE: {old_mode} -> {SIZING_MODE}\")\n",
        "        print(f\"CONF_FLOOR:  {old_floor:.2f} -> {CONF_FLOOR:.2f}\")\n",
        "\n",
        "def apply_risk_overrides(verbose: bool = True):\n",
        "    \"\"\"Re-read TAKE_PROFIT_PCT / STOP_LOSS_PCT from env and update globals.\"\"\"\n",
        "    global TAKE_PROFIT_PCT, STOP_LOSS_PCT\n",
        "    old_tp, old_sl = TAKE_PROFIT_PCT, STOP_LOSS_PCT\n",
        "    TAKE_PROFIT_PCT = float(os.getenv(\"TAKE_PROFIT_PCT\", str(TAKE_PROFIT_PCT)))\n",
        "    STOP_LOSS_PCT   = float(os.getenv(\"STOP_LOSS_PCT\",   str(STOP_LOSS_PCT)))\n",
        "    if verbose:\n",
        "        print(f\"TAKE_PROFIT_PCT: {old_tp:.4f} -> {TAKE_PROFIT_PCT:.4f}\")\n",
        "        print(f\"STOP_LOSS_PCT:   {old_sl:.4f} -> {STOP_LOSS_PCT:.4f}\")\n",
        "\n",
        "def apply_runtime_overrides(verbose: bool = True):\n",
        "    \"\"\"Refresh BARS_FEED / COOLDOWN_MIN / STALE_MAX_SEC from environment.\"\"\"\n",
        "    global BARS_FEED, COOLDOWN_MIN, STALE_MAX_SEC\n",
        "    old_feed, old_cd, old_stale = BARS_FEED, COOLDOWN_MIN, STALE_MAX_SEC\n",
        "    BARS_FEED     = os.getenv(\"BARS_FEED\", BARS_FEED).strip()\n",
        "    COOLDOWN_MIN  = int(os.getenv(\"COOLDOWN_MIN\", str(COOLDOWN_MIN)))\n",
        "    STALE_MAX_SEC = int(os.getenv(\"STALE_MAX_SEC\", str(STALE_MAX_SEC)))\n",
        "    if verbose:\n",
        "        print(f\"BARS_FEED:     {old_feed or '(default)'} -> {BARS_FEED or '(default)'}\")\n",
        "        print(f\"COOLDOWN_MIN:  {old_cd} -> {COOLDOWN_MIN}\")\n",
        "        print(f\"STALE_MAX_SEC: {old_stale} -> {STALE_MAX_SEC}\")\n",
        "\n",
        "# --- CSV logging (master, optional) -------------------------------------------------\n",
        "def ensure_trade_log_header():\n",
        "    if not TRADE_LOG_CSV.exists():\n",
        "        pd.DataFrame([{\n",
        "            \"datetime_utc\": \"\", \"ticker\": \"\", \"signal\": np.nan, \"action\": \"\",\n",
        "            \"price\": np.nan, \"equity\": np.nan, \"qty\": np.nan, \"comment\": \"\"\n",
        "        }]).iloc[0:0].to_csv(TRADE_LOG_CSV, index=False)\n",
        "\n",
        "def log_trade(ticker:str, signal:float, action:str, price:float, equity:float, qty:float=None, comment:str=\"\"):\n",
        "    \"\"\"Append one row to the (optional) master trade log CSV.\"\"\"\n",
        "    ensure_trade_log_header()\n",
        "    row = {\n",
        "        \"datetime_utc\": utcnow_iso(),\n",
        "        \"ticker\": ticker,\n",
        "        \"signal\": signal,\n",
        "        \"action\": action,    # \"BUY\" | \"SELL\" | \"HOLD\" | \"FLATTEN\"\n",
        "        \"price\": float(price) if price is not None else np.nan,\n",
        "        \"equity\": float(equity) if equity is not None else np.nan,\n",
        "        \"qty\": float(qty) if qty is not None else np.nan,\n",
        "        \"comment\": str(comment) if comment else \"\"\n",
        "    }\n",
        "    df_new = pd.DataFrame([row])\n",
        "    if TRADE_LOG_CSV.exists():\n",
        "        df_old = pd.read_csv(TRADE_LOG_CSV)\n",
        "        pd.concat([df_old, df_new], ignore_index=True).to_csv(TRADE_LOG_CSV, index=False)\n",
        "    else:\n",
        "        df_new.to_csv(TRADE_LOG_CSV, index=False)\n",
        "\n",
        "    # Keep a quick-access copy in results/latest/\n",
        "    try:\n",
        "        shutil.copy2(TRADE_LOG_CSV, TRADE_LOG_LATEST)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "# --- Alpaca API init ----------------------------------------------------------------\n",
        "def init_alpaca() -> \"tradeapi.REST\":\n",
        "    if not API_KEY or not API_SECRET:\n",
        "        raise RuntimeError(\"Missing Alpaca API keys (check your .env).\")\n",
        "    api = tradeapi.REST(API_KEY, API_SECRET, base_url=BASE_URL)\n",
        "    _ = api.get_account()  # sanity call\n",
        "    return api\n",
        "\n",
        "# --- Portfolio equity logging + metrics ----------------------------------------------\n",
        "def fetch_portfolio_history(period=\"1M\", timeframe=\"1Hour\"):\n",
        "    \"\"\"Return equity history as a tidy DataFrame with UTC timestamps.\"\"\"\n",
        "    hist = api.get_portfolio_history(period=period, timeframe=timeframe)\n",
        "    df = pd.DataFrame({\n",
        "        \"timestamp_utc\": pd.to_datetime(hist.timestamp, unit=\"s\", utc=True),\n",
        "        \"equity\": pd.Series(hist.equity, dtype=\"float64\")\n",
        "    }).dropna()\n",
        "    return df\n",
        "\n",
        "def log_equity_snapshot():\n",
        "    \"\"\"Append the latest portfolio equity point (if new) to the EQUITY_LOG_CSV.\"\"\"\n",
        "    snap = fetch_portfolio_history(period=\"1D\", timeframe=\"5Min\")\n",
        "    if snap.empty:\n",
        "        return\n",
        "    latest = snap.iloc[-1:].copy()\n",
        "    latest.rename(columns={\"timestamp_utc\": \"datetime_utc\"}, inplace=True)\n",
        "\n",
        "    if EQUITY_LOG_CSV.exists():\n",
        "        df_old = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "        merged = pd.concat([df_old, latest], ignore_index=True)\n",
        "        merged.drop_duplicates(subset=[\"datetime_utc\"], keep=\"last\").to_csv(EQUITY_LOG_CSV, index=False)\n",
        "    else:\n",
        "        latest.to_csv(EQUITY_LOG_CSV, index=False)\n",
        "    try:\n",
        "        shutil.copy2(EQUITY_LOG_CSV, EQUITY_LOG_LATEST)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def plot_equity_curve(from_equity_csv: bool = True):\n",
        "    # Backend is Agg (set in imports), so this will only write files.\n",
        "    with plt.ioff():\n",
        "        if from_equity_csv and EQUITY_LOG_CSV.exists():\n",
        "            df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"]).sort_values(\"datetime_utc\")\n",
        "        else:\n",
        "            df = fetch_portfolio_history(period=\"3M\", timeframe=\"1Hour\").rename(columns={\"timestamp_utc\":\"datetime_utc\"})\n",
        "        if df.empty:\n",
        "            print(\"No equity data to plot yet.\")\n",
        "            return\n",
        "        fig, ax = plt.subplots(figsize=(10, 4))\n",
        "        ax.plot(df[\"datetime_utc\"], df[\"equity\"])\n",
        "        ax.set_title(\"Portfolio Value Over Time (Paper)\")\n",
        "        ax.set_xlabel(\"Time (UTC)\")\n",
        "        ax.set_ylabel(\"Equity ($)\")\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(PLOT_PATH, bbox_inches=\"tight\")\n",
        "        fig.savefig(PLOT_PATH_LATEST, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved equity curve → {PLOT_PATH}\")\n",
        "        print(f\"Updated latest copy → {PLOT_PATH_LATEST}\")\n",
        "\n",
        "def compute_performance_metrics(df_equity: pd.DataFrame):\n",
        "    \"\"\"Compute cumulative return, Sharpe (hourly), max drawdown from an equity curve.\"\"\"\n",
        "    if df_equity.empty or df_equity[\"equity\"].isna().all():\n",
        "        return {\"cum_return\": np.nan, \"sharpe_hourly\": np.nan, \"max_drawdown\": np.nan}\n",
        "    e = df_equity[\"equity\"].astype(float)\n",
        "    r = e.pct_change().dropna()\n",
        "    ann_factor = 252 * 6.5  # hours/year approx\n",
        "    sharpe = (r.mean() / (r.std() + 1e-12)) * math.sqrt(ann_factor) if len(r) > 1 else np.nan\n",
        "    cum = (1 + r).cumprod()\n",
        "    peak = cum.cummax()\n",
        "    dd = (cum / peak - 1.0).min() if not cum.empty else np.nan\n",
        "    cum_return = e.iloc[-1] / e.iloc[0] - 1.0 if len(e) > 1 else 0.0\n",
        "    return {\"cum_return\": float(cum_return), \"sharpe_hourly\": float(sharpe), \"max_drawdown\": float(dd)}\n",
        "\n",
        "# --- Hook for strategy loops ---------------------------------------------------------\n",
        "def handle_signal_and_trade(ticker:str, signal:float, action:str, price:float, qty:int):\n",
        "    \"\"\"Call this right after you place/cancel an order.\"\"\"\n",
        "    log_equity_snapshot()\n",
        "    eq_df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"]) if EQUITY_LOG_CSV.exists() else pd.DataFrame()\n",
        "    eq_val = float(eq_df.iloc[-1][\"equity\"]) if not eq_df.empty else np.nan\n",
        "    log_trade(ticker=ticker, signal=signal, action=action, price=price, equity=eq_val, qty=qty)\n",
        "\n",
        "# --- Per-ticker CSV logging -----------------------------------------------------------\n",
        "def _append_csv_row(path: Path, row: dict):\n",
        "    write_header = not path.exists()\n",
        "    with path.open(\"a\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=row.keys())\n",
        "        if write_header:\n",
        "            w.writeheader()\n",
        "        w.writerow(row)\n",
        "\n",
        "def log_trade_symbol(symbol: str,\n",
        "                     bar_time,\n",
        "                     signal: int,\n",
        "                     raw_action: float,\n",
        "                     weight: float,\n",
        "                     confidence: float,\n",
        "                     price: float,\n",
        "                     equity: float,\n",
        "                     dry_run: bool,\n",
        "                     note: str = \"\"):\n",
        "    try:\n",
        "        bt_iso = pd.to_datetime(bar_time, utc=True).isoformat()\n",
        "    except Exception:\n",
        "        bt_iso = \"\"\n",
        "    row = {\n",
        "        \"log_time\": now_utc().isoformat(),\n",
        "        \"symbol\": symbol,\n",
        "        \"bar_time\": bt_iso,\n",
        "        \"signal\": \"BUY\" if int(signal) == 1 else \"SELL_OR_HOLD\",\n",
        "        \"raw_action\": float(raw_action) if np.isfinite(raw_action) else \"\",\n",
        "        \"weight\": float(weight) if np.isfinite(weight) else \"\",\n",
        "        \"confidence\": float(confidence) if np.isfinite(confidence) else \"\",\n",
        "        \"price\": float(price) if np.isfinite(price) else \"\",\n",
        "        \"equity\": float(equity) if np.isfinite(equity) else \"\",\n",
        "        \"dry_run\": int(bool(dry_run)),\n",
        "        \"note\": note,\n",
        "    }\n",
        "    _append_csv_row(RESULTS_DIR / f\"trade_log_{symbol}.csv\", row)\n",
        "\n",
        "# --- Artifacts: picker & loaders ------------------------------------------------------\n",
        "def _extract_window_idx(path: Path) -> Optional[int]:\n",
        "    m = re.search(r\"_window(\\d+)_\", path.stem, re.IGNORECASE)\n",
        "    if not m:\n",
        "        return None\n",
        "    try:\n",
        "        return int(m.group(1))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def pick_artifacts_for_ticker(\n",
        "    ticker: str,\n",
        "    artifacts_dir: str,\n",
        "    best_window: Optional[str] = None\n",
        ") -> Dict[str, Optional[Path]]:\n",
        "    p = Path(artifacts_dir)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Artifacts directory not found: {p.resolve()}\")\n",
        "\n",
        "    models = sorted(p.glob(f\"ppo_{ticker}_window*_model.zip\"))\n",
        "    if not models:\n",
        "        models = sorted(p.glob(f\"ppo_{ticker}_model.zip\")) or sorted(p.glob(f\"*{ticker}*model.zip\"))\n",
        "        if not models:\n",
        "            found = \"\\n\".join(f\" - {x.name}\" for x in p.rglob(\"*model.zip\"))\n",
        "            raise FileNotFoundError(f\"No PPO model for {ticker} in {p.resolve()}.\\nSeen:\\n{found or ' (none)'}\")\n",
        "\n",
        "    chosen: Optional[Path] = None\n",
        "    if best_window:\n",
        "        chosen = next((m for m in models if f\"_window{best_window}_\" in m.stem), None)\n",
        "        if chosen is None:\n",
        "            logging.warning(\"BEST_WINDOW=%s not found; falling back to best available.\", best_window)\n",
        "\n",
        "    if chosen is None:\n",
        "        with_idx = [(m, _extract_window_idx(m)) for m in models]\n",
        "        with_idx = [(m, w) for (m, w) in with_idx if w is not None]\n",
        "        chosen = max(with_idx, key=lambda t: t[1])[0] if with_idx else models[-1]\n",
        "\n",
        "    base    = chosen.stem.replace(\"_model\", \"\")\n",
        "    vecnorm = next(iter(sorted(p.glob(base + \"_vecnorm.pkl\"))), None)\n",
        "    feats   = next(iter(sorted(p.glob(base + \"_features.json\"))), None)\n",
        "\n",
        "    logging.info(f\"[{ticker}] model={chosen.name} | vecnorm={bool(vecnorm)} | features={bool(feats)}\")\n",
        "    return {\"model\": chosen, \"vecnorm\": vecnorm, \"features\": feats}\n",
        "\n",
        "def load_vecnormalize(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def load_features(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_ppo_model(model_path: Path):\n",
        "    return PPO.load(str(model_path))\n",
        "\n",
        "# --- Market data + account helpers --------------------------------------------------\n",
        "def get_recent_bars(api, symbol: str, limit: int = 200, timeframe=TimeFrame.Minute) -> pd.DataFrame:\n",
        "    def _as_df(bars):\n",
        "        if hasattr(bars, \"df\"):\n",
        "            df = bars.df.copy()\n",
        "            if not df.empty:\n",
        "                if isinstance(df.index, pd.MultiIndex):\n",
        "                    try:\n",
        "                        df = df.xs(symbol, level=0)\n",
        "                    except KeyError:\n",
        "                        # Symbol not present; fall back to dropping the outer level\n",
        "                        df = df.reset_index(level=0, drop=True)\n",
        "                df.index = pd.to_datetime(df.index, utc=True)\n",
        "                df = df.rename(columns={\"open\": \"Open\", \"high\": \"High\", \"low\": \"Low\",\n",
        "                                        \"close\": \"Close\", \"volume\": \"Volume\"})\n",
        "                cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in df.columns]\n",
        "                return df[cols].sort_index()\n",
        "            return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "\n",
        "        rows = []\n",
        "        for b in bars:\n",
        "            ts = getattr(b, \"t\", None)\n",
        "            ts = pd.to_datetime(ts, utc=True) if ts is not None else pd.NaT\n",
        "            rows.append({\n",
        "                \"timestamp\": ts,\n",
        "                \"Open\":   float(getattr(b, \"o\", getattr(b, \"open\",  np.nan))),\n",
        "                \"High\":   float(getattr(b, \"h\", getattr(b, \"high\",  np.nan))),\n",
        "                \"Low\":    float(getattr(b, \"l\", getattr(b, \"low\",   np.nan))),\n",
        "                \"Close\":  float(getattr(b, \"c\", getattr(b, \"close\", np.nan))),\n",
        "                \"Volume\": float(getattr(b, \"v\", getattr(b, \"volume\",np.nan))),\n",
        "            })\n",
        "        df = pd.DataFrame(rows)\n",
        "        if df.empty:\n",
        "            return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "        return df.set_index(pd.to_datetime(df[\"timestamp\"], utc=True)).drop(columns=[\"timestamp\"]).sort_index()\n",
        "\n",
        "    feed = os.getenv(\"BARS_FEED\", \"\").strip()\n",
        "    try:\n",
        "        logging.info(f\"[{symbol}] fetching {limit} {timeframe} bars (feed='{feed or 'default'}')\")\n",
        "        bars = api.get_bars(symbol, timeframe, limit=limit, feed=feed) if feed else api.get_bars(symbol, timeframe, limit=limit)\n",
        "        df = _as_df(bars)\n",
        "        if not df.empty:\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars(limit) failed: {e}\")\n",
        "\n",
        "    # Fallback: explicit 5-day window (UTC-aware)\n",
        "    try:\n",
        "        end_dt = datetime.now(timezone.utc).replace(microsecond=0)\n",
        "        start_dt = end_dt - timedelta(days=5)\n",
        "        end = end_dt.isoformat().replace(\"+00:00\", \"Z\")\n",
        "        start = start_dt.isoformat().replace(\"+00:00\", \"Z\")\n",
        "        logging.info(f\"[{symbol}] retry window start={start} end={end} (feed='{feed or 'default'}')\")\n",
        "        bars = api.get_bars(symbol, timeframe, start=start, end=end, feed=feed) if feed else api.get_bars(symbol, timeframe, start=start, end=end)\n",
        "        return _as_df(bars)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars(start/end) failed: {e}\")\n",
        "        return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "\n",
        "\n",
        "def get_account_equity(api) -> float:\n",
        "    return float(api.get_account().equity)\n",
        "\n",
        "def get_position(api, symbol: str):\n",
        "    try:\n",
        "        return api.get_position(symbol)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_position_qty(api, symbol: str):\n",
        "    pos = get_position(api, symbol)\n",
        "    if not pos:\n",
        "        return 0.0 if USE_FRACTIONALS else 0\n",
        "    try:\n",
        "        q = float(pos.qty)\n",
        "        return q if USE_FRACTIONALS else int(round(q))\n",
        "    except Exception:\n",
        "        return 0.0 if USE_FRACTIONALS else 0\n",
        "\n",
        "def get_last_price(api, symbol: str) -> float:\n",
        "    try:\n",
        "        bars = api.get_bars(symbol, TimeFrame.Minute, limit=1, feed=os.getenv(\"BARS_FEED\",\"\") or None)\n",
        "        if hasattr(bars, \"df\"):\n",
        "            df = bars.df.copy()\n",
        "            if isinstance(df.index, pd.MultiIndex):\n",
        "                df = df.xs(symbol, level=0)\n",
        "            if not df.empty:\n",
        "                if \"close\" in df.columns: return float(df[\"close\"].iloc[-1])\n",
        "                if \"Close\" in df.columns: return float(df[\"Close\"].iloc[-1])\n",
        "        else:\n",
        "            if len(bars) > 0:\n",
        "                b = bars[0]\n",
        "                close = getattr(b, \"c\", getattr(b, \"close\", None))\n",
        "                if close is not None: return float(close)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_last_price via bars failed: {e}\")\n",
        "    try:\n",
        "        pos = api.get_position(symbol)\n",
        "        return float(pos.avg_entry_price)\n",
        "    except Exception:\n",
        "        return float(\"nan\")\n",
        "\n",
        "def cancel_open_symbol_orders(api, symbol: str):\n",
        "    try:\n",
        "        for o in api.list_orders(status=\"open\"):\n",
        "            if o.symbol == symbol:\n",
        "                api.cancel_order(o.id)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] cancel orders failed: {e}\")\n",
        "\n",
        "def market_order(api, symbol: str, side: str, qty: Union[int, str] = None, notional: float = None):\n",
        "    if DRY_RUN:\n",
        "        logging.info(f\"[DRY_RUN] Would submit {side} {('notional=$'+str(notional)) if notional else ('qty='+str(qty))} {symbol} (market, day)\")\n",
        "        return None\n",
        "    try:\n",
        "        o = api.submit_order(\n",
        "            symbol=symbol,\n",
        "            side=side,\n",
        "            type=\"market\",\n",
        "            time_in_force=\"day\",\n",
        "            qty=(qty if notional is None else None),\n",
        "            notional=(notional if notional is not None else None),\n",
        "        )\n",
        "        logging.info(f\"[{symbol}] Submitted {side} {(f'notional=${notional:.2f}' if notional else f'qty={qty}')}\")\n",
        "        return o\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[{symbol}] submit_order failed: {e}\")\n",
        "        return None\n",
        "def market_order_to_qty(api, symbol: str, side: str, qty: Union[int, float, str]):\n",
        "    q = (str(float(qty))) if USE_FRACTIONALS else int(qty)\n",
        "    return market_order(api, symbol, side=side, qty=q)\n",
        "\n",
        "\n",
        "# --- Sizing / risk + (un)flatten + rebalancer ---------------------------------------\n",
        "def action_to_weight(action) -> Tuple[float, float, float]:\n",
        "    a = float(np.array(action).squeeze())\n",
        "    conf = float(abs(np.tanh(a)))\n",
        "    if a <= 0:\n",
        "        return 0.0, conf, a\n",
        "    if SIZING_MODE == \"linear\":\n",
        "        w = WEIGHT_CAP * conf\n",
        "    else:\n",
        "        w = 0.0 if conf < CONF_FLOOR else WEIGHT_CAP * (conf - CONF_FLOOR) / (1.0 - CONF_FLOOR)\n",
        "    w = max(0.0, min(WEIGHT_CAP, float(w)))\n",
        "    return w, conf, a\n",
        "\n",
        "MIN_POS_QTY = 1\n",
        "\n",
        "def compute_target_qty_by_cash(equity: float, price: float, target_weight: float, api=None) -> int:\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        return 0\n",
        "    cash_cap = float(api.get_account().cash) if api else equity\n",
        "    notional = min(cash_cap, equity * target_weight)\n",
        "    qty = int(notional // price)\n",
        "    if target_weight > 0 and qty == 0 and notional >= price:\n",
        "        return 1\n",
        "    return max(0, qty)\n",
        "\n",
        "def flatten_symbol(api, symbol: str):\n",
        "    qty = get_position_qty(api, symbol)\n",
        "    if (USE_FRACTIONALS and abs(qty) < 1e-8) or (not USE_FRACTIONALS and int(qty) == 0):\n",
        "        return\n",
        "    cancel_open_symbol_orders(api, symbol)\n",
        "    if DRY_RUN:\n",
        "        logging.info(f\"[DRY_RUN] Would close position {symbol}\")\n",
        "        return\n",
        "    try:\n",
        "        # simplest + accurate for whole or fractional\n",
        "        api.close_position(symbol)\n",
        "        logging.info(f\"[{symbol}] close_position submitted\")\n",
        "    except Exception:\n",
        "        side = \"sell\" if qty > 0 else \"buy\"\n",
        "        market_order_to_qty(api, symbol, side, abs(qty))\n",
        "\n",
        "\n",
        "def rebalance_to_weight(api, symbol: str, equity: float, target_weight: float):\n",
        "    if target_weight <= 0:\n",
        "        flatten_symbol(api, symbol)\n",
        "        return\n",
        "    price = get_last_price(api, symbol)\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        logging.warning(f\"[{symbol}] Price unavailable; skipping rebalance this cycle.\")\n",
        "        return\n",
        "\n",
        "    if USE_FRACTIONALS:\n",
        "        want_notional = equity * target_weight\n",
        "        have_qty = get_position_qty(api, symbol)\n",
        "        have_notional = have_qty * price\n",
        "        delta_notional = want_notional - have_notional\n",
        "        if abs(delta_notional) < max(1.00, 0.001 * equity):  # tiny noise guard\n",
        "            return\n",
        "        side = \"buy\" if delta_notional > 0 else \"sell\"\n",
        "        market_order(api, symbol, side=side, notional=abs(delta_notional))\n",
        "    else:\n",
        "        have_qty = get_position_qty(api, symbol)\n",
        "        want_qty = compute_target_qty_by_cash(equity, price, target_weight, api)\n",
        "        delta = want_qty - have_qty\n",
        "        if delta == 0:\n",
        "            return\n",
        "        side = \"buy\" if delta > 0 else \"sell\"\n",
        "        market_order(api, symbol, side=side, qty=abs(delta))\n",
        "\n",
        "def check_tp_sl_and_maybe_flatten(api, symbol: str) -> bool:\n",
        "    if TAKE_PROFIT_PCT <= 0 and STOP_LOSS_PCT <= 0:\n",
        "        return False\n",
        "    pos = get_position(api, symbol)\n",
        "    if not pos:\n",
        "        return False\n",
        "    try:\n",
        "        plpc = float(pos.unrealized_plpc)  # +0.031 = +3.1%\n",
        "    except Exception:\n",
        "        return False\n",
        "    if TAKE_PROFIT_PCT > 0 and plpc >= TAKE_PROFIT_PCT:\n",
        "        logging.info(f\"[{symbol}] TP hit ({plpc:.4f} >= {TAKE_PROFIT_PCT:.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    if STOP_LOSS_PCT > 0 and plpc <= -abs(STOP_LOSS_PCT):\n",
        "        logging.info(f\"[{symbol}] SL hit ({plpc:.4f} <= {-abs(STOP_LOSS_PCT):.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# --- Inference helpers ------------------------------------------------------------------\n",
        "def expected_obs_shape(model, vecnorm) -> Optional[tuple]:\n",
        "    for src in (vecnorm, model):\n",
        "        try:\n",
        "            shp = tuple(src.observation_space.shape)\n",
        "            if shp:\n",
        "                return shp\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def compute_art_feat_order(features_hint: Any, df: pd.DataFrame) -> List[str]:\n",
        "    if features_hint is None:\n",
        "        return [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    feats = features_hint.get(\"features\", features_hint) if isinstance(features_hint, dict) else list(features_hint)\n",
        "    drop = {\"datetime\", \"symbol\", \"target\", \"return\"}\n",
        "    return [c for c in feats if c not in drop and (c in df.columns) and pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "def build_obs_from_row(row: pd.Series, order: List[str]) -> np.ndarray:\n",
        "    vals = []\n",
        "    for c in order:\n",
        "        v = row.get(c, np.nan)\n",
        "        vals.append(0.0 if (pd.isna(v) or v is None or v is False) else float(v))\n",
        "    return np.array(vals, dtype=np.float32)\n",
        "\n",
        "def _pick_columns_for_channels(features_hint: Any, df: pd.DataFrame, channels: int) -> List[str]:\n",
        "    numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    cols: List[str] = []\n",
        "    if isinstance(features_hint, dict) and \"features\" in features_hint:\n",
        "        cand = [c for c in features_hint[\"features\"] if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
        "        if len(cand) >= channels:\n",
        "            cols = cand[:channels]\n",
        "    if not cols:\n",
        "        pref = [\"Close\", \"Volume\", \"Adj Close\", \"Open\", \"High\", \"Low\"]\n",
        "        cols = [c for c in pref if c in numeric]\n",
        "        cols += [c for c in numeric if c not in cols]\n",
        "        cols = cols[:channels]\n",
        "    if len(cols) < channels and cols:\n",
        "        while len(cols) < channels:\n",
        "            cols.append(cols[-1])\n",
        "    return cols[:channels]\n",
        "\n",
        "def add_regime(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"Vol20\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Ret20\"] = df[\"Close\"].pct_change(20)\n",
        "    vol_hi   = (df[\"Vol20\"] > df[\"Vol20\"].median()).astype(int)\n",
        "    trend_hi = (df[\"Ret20\"].abs() > df[\"Ret20\"].abs().median()).astype(int)\n",
        "    df[\"Regime4\"] = vol_hi * 2 + trend_hi\n",
        "    return df\n",
        "\n",
        "def denoise_wavelet(series: pd.Series, wavelet: str = \"db1\", level: int = 2) -> pd.Series:\n",
        "    try:\n",
        "        import pywt\n",
        "    except Exception:\n",
        "        return pd.Series(series).astype(float).ewm(span=5, adjust=False).mean()\n",
        "    s = pd.Series(series).astype(float).ffill().bfill()\n",
        "    arr = s.to_numpy()\n",
        "    try:\n",
        "        w = pywt.Wavelet(wavelet)\n",
        "        maxlvl = pywt.dwt_max_level(len(arr), w.dec_len)\n",
        "        lvl = int(max(0, min(level, maxlvl)))\n",
        "        if lvl < 1:\n",
        "            return s\n",
        "        coeffs = pywt.wavedec(arr, w, mode=\"symmetric\", level=lvl)\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = np.zeros_like(coeffs[i])\n",
        "        rec = pywt.waverec(coeffs, w, mode=\"symmetric\")\n",
        "        return pd.Series(rec[:len(arr)], index=s.index)\n",
        "    except Exception:\n",
        "        return s.ewm(span=5, adjust=False).mean()\n",
        "\n",
        "def add_features_live(\n",
        "    df: pd.DataFrame,\n",
        "    use_sentiment: bool = False,\n",
        "    rsi_wilder: bool = True,\n",
        "    atr_wilder: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    df = df.copy().sort_index()\n",
        "    # Normalize column names\n",
        "    cols_ci = {c.lower(): c for c in df.columns}\n",
        "    rename = {}\n",
        "    for final, alts in {\n",
        "        \"Open\": [\"open\"], \"High\": [\"high\"], \"Low\": [\"low\"],\n",
        "        \"Close\": [\"close\",\"close*\",\"last\"], \"Adj Close\":[\"adj close\",\"adj_close\",\"adjclose\",\"adjusted close\"],\n",
        "        \"Volume\":[\"volume\",\"vol\"]\n",
        "    }.items():\n",
        "        for a in [final.lower()] + alts:\n",
        "            if a in cols_ci:\n",
        "                rename[cols_ci[a]] = final\n",
        "                break\n",
        "    df = df.rename(columns=rename)\n",
        "    if \"Adj Close\" not in df.columns and \"Close\" in df.columns:\n",
        "        df[\"Adj Close\"] = df[\"Close\"]\n",
        "\n",
        "    # Bollinger\n",
        "    df[\"SMA_20\"] = df[\"Close\"].rolling(20).mean()\n",
        "    df[\"STD_20\"] = df[\"Close\"].rolling(20).std()\n",
        "    df[\"Upper_Band\"] = df[\"SMA_20\"] + 2 * df[\"STD_20\"]\n",
        "    df[\"Lower_Band\"] = df[\"SMA_20\"] - 2 * df[\"STD_20\"]\n",
        "\n",
        "    # Stochastic\n",
        "    df[\"Lowest_Low\"]   = df[\"Low\"].rolling(14).min()\n",
        "    df[\"Highest_High\"] = df[\"High\"].rolling(14).max()\n",
        "    denom = (df[\"Highest_High\"] - df[\"Lowest_Low\"]).replace(0, np.nan)\n",
        "    df[\"Stoch\"] = ((df[\"Close\"] - df[\"Lowest_Low\"]) / denom) * 100\n",
        "\n",
        "    # Momentum / volume / CCI\n",
        "    df[\"ROC\"] = df[\"Close\"].pct_change(10)\n",
        "    sign = np.sign(df[\"Close\"].diff().fillna(0))\n",
        "    df[\"OBV\"] = (sign * df[\"Volume\"].fillna(0)).cumsum()\n",
        "\n",
        "    tp = (df[\"High\"] + df[\"Low\"] + df[\"Close\"]) / 3.0\n",
        "    sma_tp = tp.rolling(20).mean()\n",
        "    md = (tp - sma_tp).abs().rolling(20).mean().replace(0, np.nan)\n",
        "    df[\"CCI\"] = (tp - sma_tp) / (0.015 * md)\n",
        "\n",
        "    # EMAs + MACD\n",
        "    df[\"EMA_10\"] = df[\"Close\"].ewm(span=10, adjust=False).mean()\n",
        "    df[\"EMA_50\"] = df[\"Close\"].ewm(span=50, adjust=False).mean()\n",
        "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "    df[\"MACD_Line\"]   = ema12 - ema26\n",
        "    df[\"MACD_Signal\"] = df[\"MACD_Line\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    # RSI (Wilder)\n",
        "    d = df[\"Close\"].diff()\n",
        "    gain = d.clip(lower=0)\n",
        "    loss = (-d.clip(upper=0))\n",
        "    if rsi_wilder:\n",
        "        avg_gain = gain.ewm(alpha=1/14, adjust=False).mean()\n",
        "        avg_loss = loss.ewm(alpha=1/14, adjust=False).mean()\n",
        "    else:\n",
        "        avg_gain = gain.rolling(14).mean()\n",
        "        avg_loss = loss.rolling(14).mean()\n",
        "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
        "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # ATR (Wilder)\n",
        "    tr = pd.concat([\n",
        "        (df[\"High\"] - df[\"Low\"]),\n",
        "        (df[\"High\"] - df[\"Close\"].shift()).abs(),\n",
        "        (df[\"Low\"]  - df[\"Close\"].shift()).abs(),\n",
        "    ], axis=1).max(axis=1)\n",
        "    df[\"ATR\"] = tr.ewm(alpha=1/14, adjust=False).mean() if atr_wilder else tr.rolling(14).mean()\n",
        "\n",
        "    # Other\n",
        "    df[\"Volatility\"]     = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Denoised_Close\"] = denoise_wavelet(df[\"Close\"])\n",
        "\n",
        "    df = add_regime(df)\n",
        "    df[\"SentimentScore\"] = (df.get(\"SentimentScore\", 0.0) if use_sentiment else 0.0)\n",
        "    df[\"Delta\"] = df[\"Close\"].pct_change(1).fillna(0.0)\n",
        "    df[\"Gamma\"] = df[\"Delta\"].diff().fillna(0.0)\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "def prepare_observation_from_bars(\n",
        "    bars_df: pd.DataFrame,\n",
        "    features_hint: Any = None,\n",
        "    min_required_rows: int = 60,\n",
        "    expected_shape: Optional[tuple] = None,\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    feats_df = add_features_live(bars_df).replace([np.inf, -np.inf], np.nan)\n",
        "    ts = pd.Timestamp.utcnow()\n",
        "    try:\n",
        "        idx_ts = pd.Timestamp(feats_df.index[-1])\n",
        "        ts = idx_ts.tz_convert(\"UTC\") if idx_ts.tzinfo else idx_ts.tz_localize(\"UTC\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if expected_shape is not None and len(expected_shape) == 2:\n",
        "        lookback, channels = int(expected_shape[0]), int(expected_shape[1])\n",
        "        cols = _pick_columns_for_channels(features_hint, feats_df, channels)\n",
        "        window_df = feats_df[cols].tail(lookback)\n",
        "        arr = window_df.to_numpy(dtype=np.float32)\n",
        "        if arr.shape[0] < lookback:\n",
        "            pad_rows = lookback - arr.shape[0]\n",
        "            arr = np.vstack([np.zeros((pad_rows, channels), dtype=np.float32), arr])\n",
        "        arr = arr[-lookback:, :channels]\n",
        "        return arr.reshape(lookback, channels), int(ts.timestamp())\n",
        "\n",
        "    order = compute_art_feat_order(features_hint, feats_df)\n",
        "    if not order:\n",
        "        raise ValueError(\"No usable features after resolving artifact order.\")\n",
        "    feats_df = feats_df.dropna(subset=order)\n",
        "    if len(feats_df) < max(20, min_required_rows):\n",
        "        raise ValueError(f\"Not enough bars to compute features robustly (have {len(feats_df)}).\")\n",
        "    last = feats_df.iloc[-1]\n",
        "    obs = build_obs_from_row(last, order)\n",
        "    return obs.astype(np.float32), int(ts.timestamp())\n",
        "\n",
        "# --- Live step & loop -------------------------------------------------------------------\n",
        "def ensure_market_open(api) -> bool:\n",
        "    try:\n",
        "        return bool(api.get_clock().is_open)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def infer_target_weight(model: PPO, vecnorm: Optional[VecNormalize], obs: np.ndarray) -> Tuple[float, float, float]:\n",
        "    x = obs\n",
        "    if vecnorm is not None and hasattr(vecnorm, \"normalize_obs\"):\n",
        "        try:\n",
        "            x = vecnorm.normalize_obs(x)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"VecNormalize.normalize_obs failed; using raw obs. Err: {e}\")\n",
        "    action, _ = model.predict(x, deterministic=INF_DETERMINISTIC)\n",
        "    return action_to_weight(action)\n",
        "\n",
        "def run_live_once_for_symbol(api, symbol: str, model: PPO, vecnorm: Optional[VecNormalize], features_hint: Optional[dict] = None):\n",
        "    shape = expected_obs_shape(model, vecnorm)\n",
        "    bars_df = get_recent_bars(api, symbol, limit=200, timeframe=TimeFrame.Minute)\n",
        "    if bars_df.empty:\n",
        "        logging.warning(f\"[{symbol}] No recent bars; skipping.\")\n",
        "        return\n",
        "\n",
        "    # --- recency debug (shows how fresh the bars are) ---\n",
        "    last_ts = pd.Timestamp(bars_df.index[-1])\n",
        "    if last_ts.tzinfo is None:\n",
        "        last_ts = last_ts.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        last_ts = last_ts.tz_convert(\"UTC\")\n",
        "    age = int((now_utc() - last_ts).total_seconds())\n",
        "    print(f\"[{symbol}] last bar: {last_ts} | age={age}s | feed='{BARS_FEED or 'default'}'\")\n",
        "\n",
        "    obs, obs_ts = prepare_observation_from_bars(\n",
        "        bars_df,\n",
        "        features_hint=features_hint,\n",
        "        min_required_rows=60,\n",
        "        expected_shape=shape,\n",
        "    )\n",
        "    if utc_ts(now_utc()) - obs_ts > STALE_MAX_SEC:\n",
        "        logging.info(f\"[{symbol}] Stale obs ({utc_ts(now_utc()) - obs_ts}s > {STALE_MAX_SEC}); skip.\")\n",
        "        return\n",
        "\n",
        "    # Respect TP/SL\n",
        "    if check_tp_sl_and_maybe_flatten(api, symbol):\n",
        "        return\n",
        "\n",
        "    # --- model -> weight/conf ---\n",
        "    target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "    eq   = get_account_equity(api)\n",
        "    px   = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api, symbol)\n",
        "    have = get_position_qty(api, symbol)\n",
        "    logging.info(f\"[{symbol}] raw={raw:.4f} conf={conf:.3f} → target_w={target_w:.4f} px=${px:.2f} eq=${eq:,.2f} have={have}\")\n",
        "\n",
        "    # --- basic entry/exit gates ---\n",
        "    # 1) below gates -> log no_action\n",
        "    if conf < ENTER_CONF_MIN and target_w <= EXIT_WEIGHT_MAX:\n",
        "        logging.info(f\"[{symbol}] Below conf/weight gates; no action.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"no_action\")\n",
        "        return\n",
        "\n",
        "    # 2) near-flat and holding -> flatten\n",
        "    if target_w <= EXIT_WEIGHT_MAX and have > 0:\n",
        "        logging.info(f\"[{symbol}] Model near-flat (≤{EXIT_WEIGHT_MAX:.3f}); flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"flatten\")\n",
        "        return\n",
        "\n",
        "    # 3) seed first share if tiny weight but confident (cash-limited)\n",
        "    notional = eq * target_w\n",
        "    need_one_share = (notional < px)\n",
        "    if target_w >= ENTER_WEIGHT_MIN and conf >= ENTER_CONF_MIN:\n",
        "        if need_one_share and have == 0 and SEED_FIRST_SHARE and not DRY_RUN:\n",
        "            logging.info(f\"[{symbol}] Seeding first share (target_w={target_w:.4f}, conf={conf:.3f}).\")\n",
        "            if USE_FRACTIONALS:\n",
        "                market_order(api, symbol, side=\"buy\", notional=px)  # ~1 share notional\n",
        "            else:\n",
        "                market_order_to_qty(api, symbol, side=\"buy\", qty=1)\n",
        "\n",
        "            log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"seed_buy\")\n",
        "            return\n",
        "\n",
        "    # 4) normal rebalance path\n",
        "    if target_w >= ENTER_WEIGHT_MIN and conf >= ENTER_CONF_MIN:\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"live\")\n",
        "        rebalance_to_weight(api, symbol, eq, target_w)\n",
        "    else:\n",
        "        logging.info(f\"[{symbol}] target_w ({target_w:.4f}) or conf ({conf:.3f}) below entry gates; hold.\")\n",
        "\n",
        "def run_live(tickers: List[str]):\n",
        "    api_local = init_alpaca()\n",
        "\n",
        "    # Load per-ticker artifacts once\n",
        "    per_ticker: Dict[str, Tuple[PPO, Optional[VecNormalize], Optional[dict]]] = {}\n",
        "    for t in tickers:\n",
        "        picks      = pick_artifacts_for_ticker(t, ARTIFACTS_DIR, best_window=BEST_WINDOW_ENV)\n",
        "        model      = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm    = load_vecnormalize(picks[\"vecnorm\"]) if picks[\"vecnorm\"] else None\n",
        "        feat_order = load_features(picks[\"features\"])\n",
        "        per_ticker[t] = (model, vecnorm, feat_order)\n",
        "\n",
        "    logging.info(f\"Starting live execution for: {tickers}\")\n",
        "    last_exec_at = now_utc() - timedelta(minutes=COOLDOWN_MIN)\n",
        "    cycle = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            if not ensure_market_open(api_local):\n",
        "                logging.info(\"Market closed. Sleeping 30s.\")\n",
        "                time.sleep(30)\n",
        "                continue\n",
        "\n",
        "            # Throttle by cooldown\n",
        "            if (now_utc() - last_exec_at) < timedelta(minutes=COOLDOWN_MIN):\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "\n",
        "            t_cycle_start = time.perf_counter()\n",
        "            # Run once per ticker\n",
        "            for t in tickers:\n",
        "                t_sym_start = time.perf_counter()\n",
        "                model, vecnorm, feat_hint = per_ticker[t]\n",
        "                run_live_once_for_symbol(api_local, t, model, vecnorm, features_hint=feat_hint)\n",
        "                logging.info(\"[TIMER] %s symbol work: %.3fs\", t, time.perf_counter() - t_sym_start)\n",
        "\n",
        "            # After each cycle, record equity and occasionally plot\n",
        "            log_equity_snapshot()\n",
        "            cycle += 1\n",
        "            if (cycle % 6) == 0:\n",
        "                try:\n",
        "                    plot_equity_curve(from_equity_csv=True)\n",
        "                except Exception as e:\n",
        "                    logging.warning(\"Plotting failed: %s\", e)\n",
        "\n",
        "            last_exec_at = now_utc()\n",
        "            logging.info(\"[TIMER] full-cycle active time: %.3fs (cooldown=%d min)\",\n",
        "                         time.perf_counter() - t_cycle_start, COOLDOWN_MIN)\n",
        "\n",
        "            if (cycle % 12) == 0:\n",
        "                gc.collect()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logging.info(\"KeyboardInterrupt: stopping live loop.\")\n",
        "        try:\n",
        "            log_equity_snapshot()\n",
        "            plot_equity_curve(from_equity_csv=True)\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Finalization failed: %s\", e)\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Live loop exception: %s\", e)\n",
        "        try:\n",
        "            log_equity_snapshot()\n",
        "        except Exception:\n",
        "            pass\n",
        "        time.sleep(5)\n",
        "\n",
        "# --- Generic per-ticker diagnostic ---------------------------------------------------\n",
        "def ticker_diagnostic(ticker: str,\n",
        "                      dry_run: bool = DRY_RUN,\n",
        "                      timeframe: TimeFrame = TimeFrame.Minute,\n",
        "                      limit: int = 300):\n",
        "    print(f\"\\nRunning strategy for {ticker}...\")\n",
        "\n",
        "    # Snapshot start\n",
        "    try:\n",
        "        api_local = init_alpaca()\n",
        "        positions_start = len(api_local.list_positions())\n",
        "        orders_start    = len(api_local.list_orders(status=\"open\"))\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Alpaca: {e}\")\n",
        "        return\n",
        "\n",
        "    # Load artifacts\n",
        "    try:\n",
        "        best   = (os.getenv(\"BEST_WINDOW\", \"\").strip() or None)\n",
        "        picks  = pick_artifacts_for_ticker(\n",
        "            ticker,\n",
        "            os.getenv(\"ARTIFACTS_DIR\", str(ARTIFACTS_DIR)),\n",
        "            best_window=best\n",
        "        )\n",
        "        model   = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm = load_vecnormalize(picks[\"vecnorm\"]) if picks[\"vecnorm\"] else None\n",
        "        feats   = load_features(picks[\"features\"])\n",
        "        print(f\"Model artifacts loaded for {ticker}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load model for {ticker}: {e}\")\n",
        "        return\n",
        "\n",
        "    # Bars / shape-aware requirements\n",
        "    try:\n",
        "        shape     = expected_obs_shape(model, vecnorm)\n",
        "        lookback  = int(shape[0]) if (shape is not None and len(shape) == 2) else None\n",
        "        bars_need = max(200, (lookback or 0) * 3)\n",
        "        bars_df   = get_recent_bars(api_local, ticker, limit=max(limit, bars_need), timeframe=timeframe)\n",
        "\n",
        "        min_rows_needed = lookback if lookback is not None else 60\n",
        "        if len(bars_df) < min_rows_needed:\n",
        "            print(f\"Not enough data for {ticker}: {len(bars_df)} rows (need ≥ {min_rows_needed})\")\n",
        "            bars_df = pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching bars for {ticker}: {e}\")\n",
        "        bars_df = pd.DataFrame()\n",
        "\n",
        "    # Build observation\n",
        "    obs, obs_ts = None, None\n",
        "    if not bars_df.empty:\n",
        "        try:\n",
        "            obs, obs_ts = prepare_observation_from_bars(\n",
        "                bars_df,\n",
        "                features_hint=feats,\n",
        "                min_required_rows=min_rows_needed,\n",
        "                expected_shape=shape,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing observation for {ticker}: {e}\")\n",
        "\n",
        "    # Predict\n",
        "    signal = None\n",
        "    target_w = conf = raw = float(\"nan\")\n",
        "    predictions_made = 0\n",
        "    bar_time = pd.NaT\n",
        "    price = float(\"nan\")\n",
        "    equity = float(\"nan\")\n",
        "\n",
        "    orders_submitted = 0\n",
        "    market_closed = 0\n",
        "\n",
        "    if obs is not None:\n",
        "        try:\n",
        "            target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "            signal = int(target_w > 0.0)  # 1 = Buy, 0 = Sell/Hold\n",
        "            predictions_made = 1\n",
        "            print(f\"Prediction for {ticker}: {signal} (1 = Buy, 0 = Sell)\")\n",
        "\n",
        "            bar_time = bars_df.index[-1] if not bars_df.empty else pd.NaT\n",
        "            price    = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api_local, ticker)\n",
        "            equity   = get_account_equity(api_local)\n",
        "            print(f\"raw={raw:.4f} conf={conf:.3f} target_w={target_w:.3f} price=${price:.2f} equity=${equity:,.2f}\")\n",
        "\n",
        "            # Log to per-ticker CSV\n",
        "            log_trade_symbol(ticker, bar_time, signal, raw, target_w, conf, price, equity,\n",
        "                             dry_run=dry_run, note=\"diagnostic\")\n",
        "\n",
        "            # Optional order submission (paper)\n",
        "            try:\n",
        "                clock = api_local.get_clock()\n",
        "                if not clock.is_open:\n",
        "                    print(\"Market is closed.\")\n",
        "                    market_closed = 1\n",
        "                else:\n",
        "                    if signal is not None and not dry_run:\n",
        "                        FORCE_FIRST_BUY = os.getenv(\"FORCE_FIRST_BUY\",\"0\").lower() in (\"1\",\"true\",\"yes\")\n",
        "                        # Do we already hold ticker?\n",
        "                        has_position = False\n",
        "                        try:\n",
        "                            pos = api_local.get_position(ticker)\n",
        "                            has_position = float(pos.qty) > 0\n",
        "                        except APIError:\n",
        "                            has_position = False\n",
        "                        have = get_position_qty(api_local, ticker)\n",
        "                        want_notional = (equity if np.isfinite(equity) else 0.0) * (target_w if np.isfinite(target_w) else 0.0)\n",
        "                        need_one_share = (np.isfinite(price) and price > 0 and want_notional < price)\n",
        "\n",
        "                        if SEED_FIRST_SHARE and signal == 1 and have == 0 and need_one_share:\n",
        "                            if USE_FRACTIONALS:\n",
        "                                market_order(api_local, ticker, side=\"buy\", notional=price)  # ~1 share notional\n",
        "                            else:\n",
        "                                market_order(api_local, ticker, side=\"buy\", qty=1)\n",
        "                            # Keep this if you want an extra log row for the seed action:\n",
        "                            log_trade_symbol(ticker, bar_time, int(target_w > 0), raw, target_w, conf, price, equity, dry_run, note=\"seed_buy\")\n",
        "                            orders_submitted += 1\n",
        "                        elif (FORCE_FIRST_BUY and not has_position) or (signal == 1 and not has_position):\n",
        "                            market_order(\n",
        "                                api_local,\n",
        "                                symbol=ticker,\n",
        "                                side=\"buy\",\n",
        "                                qty=(1 if not USE_FRACTIONALS else None),\n",
        "                                notional=(price if USE_FRACTIONALS else None),\n",
        "                            )\n",
        "                            print(f\"BUY order submitted for {ticker}\")\n",
        "                            orders_submitted += 1\n",
        "                        elif signal == 0 and has_position:\n",
        "                            market_order(\n",
        "                                api_local,\n",
        "                                symbol=ticker,\n",
        "                                side=\"sell\",\n",
        "                                qty=(1 if not USE_FRACTIONALS else None),\n",
        "                                notional=(price if USE_FRACTIONALS else None),\n",
        "                            )\n",
        "                            print(f\"SELL order submitted for {ticker}\")\n",
        "                            orders_submitted += 1\n",
        "                        else:\n",
        "                            print(f\"No action taken for {ticker}\")\n",
        "                    else:\n",
        "                        print(f\"(dry-run) No order submitted for {ticker} — signal={signal}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Trade/clock error for {ticker}: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction error for {ticker}: {e}\")\n",
        "\n",
        "    # Snapshot end + summary\n",
        "    try:\n",
        "        positions_end = len(api_local.list_positions())\n",
        "        orders_end    = len(api_local.list_orders(status=\"open\"))\n",
        "        print(\"\\n========== SUMMARY ==========\")\n",
        "        print(f\"Processed:         1\")\n",
        "        print(f\"Models loaded:     1\")\n",
        "        print(f\"Predictions made:  {predictions_made}\")\n",
        "        print(f\"Market closed:     {market_closed}\")\n",
        "        print(f\"Orders submitted:  {orders_submitted} (dry_run={dry_run})\")\n",
        "        print(f\"Existing positions (start -> end): {positions_start} -> {positions_end}\")\n",
        "        print(f\"Open orders        (start -> end): {orders_start} -> {orders_end}\")\n",
        "        print(\"=============================\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return {\n",
        "        \"signal\": signal,\n",
        "        \"target_w\": target_w,\n",
        "        \"conf\": conf,\n",
        "        \"raw\": raw,\n",
        "        \"bar_time\": bar_time,\n",
        "        \"price\": price,\n",
        "        \"equity\": equity,\n",
        "        \"dry_run\": dry_run,\n",
        "    }\n",
        "\n",
        "# ============================== MAIN =====================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Force paper endpoint + enable live loop + allow PAPER orders (env-driven)\n",
        "    os.environ[\"APCA_API_BASE_URL\"] = \"https://paper-api.alpaca.markets\"\n",
        "    os.environ[\"AUTO_RUN_LIVE\"] = \"1\"   # run the live loop\n",
        "    os.environ[\"DRY_RUN\"]       = os.getenv(\"DRY_RUN\", \"0\")\n",
        "\n",
        "    # Refresh globals that depend on env\n",
        "    BASE_URL = os.getenv(\"APCA_API_BASE_URL\", BASE_URL)\n",
        "    DRY_RUN  = os.getenv(\"DRY_RUN\", \"1\").lower() in (\"1\",\"true\",\"yes\")\n",
        "\n",
        "    # Informative banner\n",
        "    print(\"=== CONFIG ===\")\n",
        "    print(\"Project root  :\", PROJECT_ROOT)\n",
        "    print(\"ARTIFACTS_DIR :\", ARTIFACTS_DIR)\n",
        "    print(\"RESULTS_DIR   :\", RESULTS_DIR)\n",
        "    print(\"Tickers       :\", TICKERS)\n",
        "    print(\"DRY_RUN       :\", DRY_RUN, \"| BARS_FEED:\", BARS_FEED, \"| USE_FRACTIONALS:\", USE_FRACTIONALS)\n",
        "\n",
        "    if ARTIFACTS_DIR.exists():\n",
        "        print(\"Artifacts present:\", sorted(p.name for p in ARTIFACTS_DIR.iterdir()))\n",
        "    print(\"==============\")\n",
        "\n",
        "    # Initialize Alpaca (will error if keys missing)\n",
        "    api = init_alpaca()\n",
        "    acct = api.get_account()\n",
        "    logging.info(\"Account status: %s | equity=%s | cash=%s\", acct.status, acct.equity, acct.cash)\n",
        "\n",
        "    # -------- Visibility & delayed-feed friendly settings --------\n",
        "    # (Helps you see activity even with delayed minute bars)\n",
        "    os.environ[\"BARS_FEED\"]     = \"\"       # let Alpaca choose (avoids IEX empty responses)\n",
        "    os.environ[\"COOLDOWN_MIN\"]  = \"1\"      # faster cycles while testing\n",
        "    os.environ[\"STALE_MAX_SEC\"] = \"1800\"\n",
        "\n",
        "    # Re-apply all overrides so globals actually change\n",
        "    apply_env_overrides(verbose=True)      # SIZING_MODE / CONF_FLOOR\n",
        "    apply_risk_overrides(verbose=True)     # TP/SL\n",
        "    apply_runtime_overrides(verbose=True)  # BARS_FEED / COOLDOWN_MIN / STALE_MAX_SEC\n",
        "    print(\"EFFECTIVE (after overrides)\")\n",
        "    print(\"DRY_RUN      :\", DRY_RUN, \"| BARS_FEED:\", BARS_FEED, \"| COOLDOWN_MIN:\", COOLDOWN_MIN, \"| STALE_MAX_SEC:\", STALE_MAX_SEC)\n",
        "    # --- One-off diagnostics so you can see raw/conf/target_w immediately ---\n",
        "    for _sym in [\"UNH\", \"GE\"]:\n",
        "        try:\n",
        "            ticker_diagnostic(_sym, dry_run=DRY_RUN)\n",
        "        except Exception as e:\n",
        "            print(f\"[DIAG] {_sym} failed: {e}\")\n",
        "\n",
        "    # ----------------- Start the normal live loop -----------------\n",
        "    run_live(TICKERS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MuKmDDidRdKs",
        "outputId": "efeda7f6-7eaf-467c-eaf9-aa4d561e0527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Equity summary — last: $99,997.47 | n=64 pts | Sharpe(h): 2.77\n"
          ]
        }
      ],
      "source": [
        "eq_path = RESULTS_DIR / \"equity_log.csv\"\n",
        "if eq_path.exists():\n",
        "    try:\n",
        "        eq = pd.read_csv(eq_path, parse_dates=[\"datetime_utc\"]).sort_values(\"datetime_utc\")\n",
        "        if not eq.empty:\n",
        "            r = eq[\"equity\"].pct_change().dropna()\n",
        "            sharpe_h = (r.mean() / (r.std() + 1e-12)) * np.sqrt(252 * 6.5) if len(r) else float(\"nan\")\n",
        "            print(f\"\\nEquity summary — last: ${eq['equity'].iloc[-1]:,.2f} | \"\n",
        "                  f\"n={len(eq)} pts | Sharpe(h): {sharpe_h:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not summarize equity: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_b4wIA0gjEg0",
        "outputId": "391c1bdb-3432-4b26-d573-b52e46a4e59e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Trade Summary:\n",
            "UNH: no trades logged yet.\n",
            "GE: no trades logged yet.\n",
            "\n",
            "Position Summary:\n",
            "  GE: 15 shares @ $294.98 | Value: $4,424.70\n",
            "\n",
            "Total Market Value: $4,424.70\n"
          ]
        }
      ],
      "source": [
        "# Report only what's in your env (default UNH)\n",
        "tickers_to_report = [t.strip().upper() for t in os.getenv(\"TICKERS\", \"UNH\").split(\",\") if t.strip()]\n",
        "\n",
        "# Where your trade logs live; default to ARTIFACTS_DIR\n",
        "ART_DIR = Path(os.getenv(\"ARTIFACTS_DIR\", \"/content\"))\n",
        "RESULTS_DIR = Path(os.getenv(\"RESULTS_DIR\", str(ART_DIR)))\n",
        "\n",
        "print(\"\\nTrade Summary:\")\n",
        "for ticker in tickers_to_report:\n",
        "    log_path = RESULTS_DIR / f\"trade_log_{ticker}.csv\"\n",
        "    if not log_path.exists():\n",
        "        print(f\"{ticker}: no trades logged yet.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(log_path, on_bad_lines=\"skip\",\n",
        "                         parse_dates=[\"log_time\",\"bar_time\"], infer_datetime_format=True)\n",
        "\n",
        "        # Count by 'signal' (or 'action' if older logs)\n",
        "        key = \"signal\" if \"signal\" in df.columns else (\"action\" if \"action\" in df.columns else None)\n",
        "        if key:\n",
        "            counts = df[key].value_counts(dropna=False).to_dict()\n",
        "            print(f\"{ticker}: {counts}\")\n",
        "        else:\n",
        "            print(f\"{ticker}: log present but missing 'signal'/'action' columns.\")\n",
        "\n",
        "        # Optional confidence histogram\n",
        "        if \"confidence\" in df.columns and df[\"confidence\"].notna().any():\n",
        "            plt.figure(figsize=(8, 3.5))\n",
        "            df[\"confidence\"].dropna().plot(kind=\"hist\", bins=10, edgecolor=\"black\")\n",
        "            plt.title(f\"{ticker} - Confidence Distribution\")\n",
        "            plt.xlabel(\"confidence\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # Optional: PPO-specific stats\n",
        "        for col in [\"weight\", \"raw_action\"]:\n",
        "            if col in df.columns and df[col].notna().any():\n",
        "                s = df[col].dropna()\n",
        "                print(f\"{ticker} {col}: mean={s.mean():.3f}, std={s.std():.3f}, \"\n",
        "                      f\"min={s.min():.3f}, max={s.max():.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{ticker}: could not summarize trades: {e}\")\n",
        "\n",
        "# --- Position Summary (unchanged) ---\n",
        "try:\n",
        "    if 'api' not in globals():\n",
        "        api = init_alpaca()\n",
        "    positions = api.list_positions()\n",
        "    total_market_value = 0.0\n",
        "    print(\"\\nPosition Summary:\")\n",
        "    for p in positions:\n",
        "        mv = float(p.market_value)\n",
        "        total_market_value += mv\n",
        "        print(f\"  {p.symbol}: {p.qty} shares @ ${float(p.current_price):.2f} | Value: ${mv:,.2f}\")\n",
        "    print(f\"\\nTotal Market Value: ${total_market_value:,.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not summarize positions: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M8JhoBJZ2r4m",
        "outputId": "eada0e81-9642-412b-eac8-29ce9cd374ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNH: 0 filled trades in last 14 days\n",
            "GE: 6 filled trades in last 14 days\n"
          ]
        }
      ],
      "source": [
        "#Goal 20+ trades\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "def count_filled_orders_since(api, symbol: str, days: int = 14) -> int:\n",
        "    after = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
        "    orders = api.list_orders(status=\"all\", after=after, nested=True)  # nested grabs any child legs too\n",
        "    return sum(1 for o in orders if o.symbol == symbol and o.status in (\"filled\",\"partially_filled\"))\n",
        "\n",
        "api_chk = init_alpaca()\n",
        "for sym in TICKERS:  # e.g., [\"UNH\",\"GE\"]\n",
        "    n = count_filled_orders_since(api_chk, sym, days=14)\n",
        "    print(f\"{sym}: {n} filled trades in last 14 days\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}