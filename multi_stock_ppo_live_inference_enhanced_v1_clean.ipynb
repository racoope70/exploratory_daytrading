{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/racoope70/daytrading-with-ml/blob/main/multi_stock_ppo_live_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!apt-get remove --purge -y cuda* libcuda* nvidia* || echo \"No conflicting CUDA packages\"\n",
    "!apt-get autoremove -y\n",
    "!apt-get clean"
   ],
   "metadata": {
    "id": "0xMJutMCD_uu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "!apt-get update -qq && apt-get install -y \\\n",
    "    libcusolver11 libcusparse11 libcurand10 libcufft10 libnppig10 libnppc10 libnppial10 \\\n",
    "    cuda-toolkit-12-4\n"
   ],
   "metadata": {
    "id": "eANWApmOE_ke"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip uninstall -y protobuf\n",
    "!pip install protobuf==3.20.3\n"
   ],
   "metadata": {
    "id": "DyLxSfGAFFcT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --extra-index-url=https://pypi.nvidia.com \\\n",
    "    cuml-cu12==25.2.0 cudf-cu12==25.2.0 cupy-cuda12x \\\n",
    "    dask-cuda==25.2.0 dask-cudf-cu12==25.2.0\n"
   ],
   "metadata": {
    "id": "IxNjaKmaFI0s"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install numba==0.60.0\n"
   ],
   "metadata": {
    "id": "NnmelMY0FMO3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install stable-baselines3[extra] gymnasium gym-anytrading yfinance xgboost joblib\n",
    "!pip install matplotlib scikit-learn pandas\n"
   ],
   "metadata": {
    "id": "YZaPFzTYFQ3i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tensorflow==2.18.0\n"
   ],
   "metadata": {
    "id": "v8lDHud8FS72"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n"
   ],
   "metadata": {
    "id": "K-iHY6_pFVEO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"TensorFlow GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"TensorFlow GPU memory config failed: {e}\")\n"
   ],
   "metadata": {
    "id": "1KUGG5IaFWl7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda-12.4'\n",
    "os.environ['PATH'] += ':/usr/local/cuda-12.4/bin'\n",
    "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/cuda-12.4/lib64'\n"
   ],
   "metadata": {
    "id": "RdkCCYU1FApi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gz6JMzAHdEJZ"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxTo_GOSemtU"
   },
   "outputs": [],
   "source": [
    "#Step 7: authenticate with hugging face hub (optional)\n",
    "#This allows for better access and avoids rate limits when downloading public models/datasets\n",
    "\n",
    "# Authenticate with Hugging Face Hub\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9AoPa7K4FEK"
   },
   "outputs": [],
   "source": [
    "# === Import Libraries ===\n",
    "import os, gc, time, json, pywt, logging\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from transformers import pipeline\n",
    "from google.colab import drive\n",
    "\n",
    "# === Logging Setup ===\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# === Mount Google Drive ===\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# === Configuration ===\n",
    "ticker_list = ['AAPL', 'TSLA', 'MSFT', 'GOOGL', 'NVDA']\n",
    "strategy_name = \"ppo_walkforward_v1\"\n",
    "CONFIG = {\n",
    "    'symbols': ticker_list,\n",
    "    'period': '720d',\n",
    "    'interval': '1h',\n",
    "    'target': 'Target',\n",
    "    'sharpe_threshold': 1.5,\n",
    "    'return_threshold': 1.25,\n",
    "    'strategy_name': strategy_name\n",
    "}\n",
    "test_mode = True\n",
    "symbols = ['AAPL', 'NVDA', 'MSFT'] if test_mode else CONFIG['symbols']\n",
    "end_date = datetime.today()\n",
    "start_date = end_date - timedelta(days=729)\n",
    "\n",
    "# === Sentiment Pipeline ===\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "# === Download Stock Data ===\n",
    "def download_stock_data(ticker, start_date=None, end_date=None, interval=\"1h\", max_retries=5, window_days=730):\n",
    "    if start_date is None or end_date is None:\n",
    "        end_date = datetime.today()\n",
    "        start_date = end_date - timedelta(days=window_days)\n",
    "    start_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            logging.info(f\"ðŸ“¥ Attempt {attempt}: Downloading {ticker} from {start_str} to {end_str}...\")\n",
    "            df = yf.download(\n",
    "                ticker,\n",
    "                start=start_str,\n",
    "                end=end_str,\n",
    "                interval=interval,\n",
    "                progress=False,\n",
    "                auto_adjust=False\n",
    "            )\n",
    "            if not df.empty:\n",
    "                df.reset_index(inplace=True)\n",
    "                df['Symbol'] = ticker\n",
    "                df['Datetime'] = pd.to_datetime(df['Datetime'] if 'Datetime' in df.columns else df['Date'])\n",
    "                return df\n",
    "            raise ValueError(\"Empty data\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"âš ï¸ Error downloading {ticker}: {e}. Retrying in {attempt * 5} sec...\")\n",
    "            time.sleep(attempt * 5)\n",
    "    logging.error(f\"Failed to download {ticker}\")\n",
    "    return None\n",
    "\n",
    "# === Wavelet Denoising ===\n",
    "def denoise_wavelet(series, wavelet='db1', level=2):\n",
    "    coeffs = pywt.wavedec(series, wavelet, mode='smooth')\n",
    "    coeffs[1:] = [np.zeros_like(c) for c in coeffs[1:]]\n",
    "    return pywt.waverec(coeffs, wavelet, mode='smooth')[:len(series)]\n",
    "\n",
    "# === Sentiment Scoring ===\n",
    "def score_sentiment(texts):\n",
    "    try:\n",
    "        outputs = sentiment_pipeline(texts, truncation=True, max_length=256)\n",
    "        results = []\n",
    "        for r in outputs:\n",
    "            label = r['label'].lower()\n",
    "            score = r['score'] if label == 'positive' else (-r['score'] if label == 'negative' else 0)\n",
    "            results.append(score)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Sentiment scoring error: {e}\")\n",
    "        return [0] * len(texts)\n",
    "\n",
    "# === Feature Engineering ===\n",
    "def compute_enhanced_features(df):\n",
    "    df = df.copy()\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "    df['STD_20'] = df['Close'].rolling(20).std()\n",
    "    df['Upper_Band'] = df['SMA_20'] + 2 * df['STD_20']\n",
    "    df['Lower_Band'] = df['SMA_20'] - 2 * df['STD_20']\n",
    "    df['Lowest_Low'] = df['Low'].rolling(14).min()\n",
    "    df['Highest_High'] = df['High'].rolling(14).max()\n",
    "    denom = (df['Highest_High'] - df['Lowest_Low']).replace(0, np.nan)\n",
    "    df['Stoch'] = ((df['Close'] - df['Lowest_Low']) / denom) * 100\n",
    "    df['ROC'] = df['Close'].pct_change(10)\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    df['CCI'] = ((df['High'] + df['Low'] + df['Close']) / 3 - df['Close'].rolling(20).mean()) / (0.015 * df['Close'].rolling(20).std())\n",
    "    df['EMA_10'] = df['Close'].ewm(span=10).mean()\n",
    "    df['EMA_50'] = df['Close'].ewm(span=50).mean()\n",
    "    df['MACD_Line'] = df['Close'].ewm(span=12).mean() - df['Close'].ewm(span=26).mean()\n",
    "    df['MACD_Signal'] = df['MACD_Line'].ewm(span=9).mean()\n",
    "    df['RSI'] = 100 - (100 / (1 + df['Close'].diff().apply(lambda x: x if x > 0 else 0).rolling(14).mean() /\n",
    "                              -df['Close'].diff().apply(lambda x: x if x < 0 else 0).rolling(14).mean()))\n",
    "    tr = pd.concat([\n",
    "        df['High'] - df['Low'],\n",
    "        abs(df['High'] - df['Close'].shift()),\n",
    "        abs(df['Low'] - df['Close'].shift())\n",
    "    ], axis=1).max(axis=1)\n",
    "    df['ATR'] = tr.rolling(14).mean()\n",
    "    df['Volatility'] = df['Close'].pct_change().rolling(20).std()\n",
    "    df['Return'] = (df['Close'].shift(-10) - df['Close']) / df['Close']\n",
    "    df['Target'] = np.select([df['Return'] > 0.02, df['Return'] < -0.02], [1, -1], default=0)\n",
    "\n",
    "    # === Denoise ===\n",
    "    try:\n",
    "        df['Denoised_Close'] = denoise_wavelet(df['Close'].ffill())\n",
    "        logging.info(df[['Close', 'Denoised_Close']].head())\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Wavelet denoising failed: {e}\")\n",
    "        df['Denoised_Close'] = df['Close']\n",
    "\n",
    "    # === Sentiment ===\n",
    "    df['Mock_Headline'] = f\"{df['Symbol'].iloc[0]} is expected to perform well in the market.\"\n",
    "    try:\n",
    "        df['SentimentScore'] = score_sentiment(df['Mock_Headline'].tolist())\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Sentiment scoring failed for {df['Symbol'].iloc[0]}: {e}\")\n",
    "        df['SentimentScore'] = 0\n",
    "\n",
    "    # === Greeks ===\n",
    "    df['Delta'] = df['Close'].pct_change(1).fillna(0)\n",
    "    df['Gamma'] = df['Delta'].diff().fillna(0)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # === Reorder Columns ===\n",
    "    cols = [col for col in df.columns if col not in ['Target', 'Return', 'Symbol']] + ['Target', 'Return', 'Symbol']\n",
    "    return df[cols]\n",
    "\n",
    "# === Process All Symbols ===\n",
    "all_dfs = []\n",
    "for i, ticker in enumerate(symbols, 1):\n",
    "    logging.info(f\"[{i}/{len(symbols)}] Processing {ticker}\")\n",
    "    df_single = download_stock_data(ticker, start_date, end_date, interval=\"1h\")\n",
    "    if df_single is not None and not df_single.empty:\n",
    "        try:\n",
    "            df_features = compute_enhanced_features(df_single)\n",
    "            all_dfs.append(df_features)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Feature engineering failed for {ticker}: {e}\")\n",
    "        finally:\n",
    "            del df_single\n",
    "            try: del df_features\n",
    "            except NameError: pass\n",
    "            gc.collect()\n",
    "    else:\n",
    "        logging.warning(f\"No data for {ticker}\")\n",
    "\n",
    "# === Save Final Dataset ===\n",
    "if all_dfs:\n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    logging.info(f\"Combined dataset created with shape: {df.shape}\")\n",
    "    df.to_csv(\"multi_stock_feature_engineered_dataset.csv\", index=False)\n",
    "    drive_path = \"/content/drive/MyDrive/trading_data/\"\n",
    "    os.makedirs(drive_path, exist_ok=True)\n",
    "    df.to_csv(os.path.join(drive_path, \"multi_stock_feature_engineered_dataset.csv\"), index=False)\n",
    "    logging.info(f\"Dataset saved to Google Drive at: {drive_path}\")\n",
    "    del all_dfs, df\n",
    "    gc.collect()\n",
    "else:\n",
    "    logging.warning(\"No usable data found for any ticker.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47KOlGdVbGMl"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"multi_stock_feature_engineered_dataset.csv\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9YSsE644rcX"
   },
   "outputs": [],
   "source": [
    "# === PPO Walkforward with Runtime Timing, Full Model Saving, and Logging ===\n",
    "import os, gc, time, json, torch, logging, joblib\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from gym_anytrading.envs import StocksEnv\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "# === Configuration ===\n",
    "RESULTS_DIR = \"/content/drive/MyDrive/Results_May_2025/ppo_walkforward_results\"\n",
    "FINAL_MODEL_DIR = os.path.join(RESULTS_DIR, \"models\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# === Logging Setup ===\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# === Flags ===\n",
    "ENABLE_SENTIMENT = True\n",
    "ENABLE_SLO = True\n",
    "ENABLE_WAVELET = True\n",
    "test_mode = True\n",
    "ENABLE_PLOTS = True\n",
    "\n",
    "# === Load Dataset ===\n",
    "if not os.path.exists(\"multi_stock_feature_engineered_dataset.csv\"):\n",
    "    raise FileNotFoundError(\"Required feature-engineered dataset not found!\")\n",
    "df = pd.read_csv(\"multi_stock_feature_engineered_dataset.csv\")\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "# === PPO-Compatible Environment ===\n",
    "class ContinuousTradingEnv(StocksEnv):\n",
    "    def __init__(self, df, frame_bound, window_size):\n",
    "        super().__init__(df=df.reset_index(drop=True), frame_bound=frame_bound, window_size=window_size)\n",
    "        self.action_space = Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        try:\n",
    "            if action < -0.3:\n",
    "                discrete_action = 0\n",
    "            elif action > 0.1:\n",
    "                discrete_action = 1\n",
    "            else:\n",
    "                discrete_action = 2\n",
    "\n",
    "            step_result = super().step(discrete_action)\n",
    "            if len(step_result) == 5:\n",
    "                obs, reward, terminated, truncated, info = step_result\n",
    "            else:\n",
    "                obs, reward, terminated, info = step_result\n",
    "                truncated = False\n",
    "\n",
    "            current_price = self.df.loc[self._current_tick, 'Close']\n",
    "            slippage_pct = 0.002\n",
    "            fill_price = current_price * (1 + slippage_pct if discrete_action == 1 else (1 - slippage_pct if discrete_action == 0 else 0))\n",
    "            price_diff = fill_price - self.df.loc[self._current_tick - 1, 'Close']\n",
    "            reward += price_diff * discrete_action\n",
    "\n",
    "            action_scalar = float(np.array(action).squeeze())\n",
    "            reward += abs(action_scalar) * 0.01\n",
    "\n",
    "            if ENABLE_SENTIMENT and 'SentimentScore' in self.df.columns:\n",
    "                sentiment = self.df.loc[self._current_tick, 'SentimentScore']\n",
    "                reward += sentiment * 0.01\n",
    "\n",
    "            return obs, np.clip(reward, -1.0, 1.0), terminated, truncated, info\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Step error: {e}\")\n",
    "            return self.reset(), 0, True, True, {}\n",
    "\n",
    "# === Utilities ===\n",
    "def plot_performance(ticker, portfolio, hold_value):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(portfolio, label=\"PPO Portfolio\")\n",
    "    plt.axhline(y=hold_value, color=\"r\", linestyle=\"--\", label=\"Buy & Hold\")\n",
    "    plt.title(f\"{ticker} - PPO vs Buy & Hold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f\"{ticker}_performance.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def get_walk_forward_windows(df, window_size=3000, step_size=500, min_len=1000):\n",
    "    return [\n",
    "        (start, start + window_size)\n",
    "        for start in range(0, len(df) - min_len, step_size)\n",
    "        if start + window_size < len(df)\n",
    "    ]\n",
    "\n",
    "# === Save QuantConnect-Compatible Artifacts ===\n",
    "def save_quantconnect_model(artifact, prefix, save_dir):\n",
    "    artifact['model'].save(os.path.join(save_dir, f\"{prefix}_model.zip\"))\n",
    "    if artifact['vecnorm']:\n",
    "        artifact['vecnorm'].save(os.path.join(save_dir, f\"{prefix}_vecnorm.pkl\"))\n",
    "    json.dump({\"features\": artifact['features']}, open(os.path.join(save_dir, f\"{prefix}_features.json\"), \"w\"))\n",
    "    json.dump({\"threshold\": 0.05, \"use_confidence\": True, \"inference_mode\": \"deterministic\"}, open(os.path.join(save_dir, f\"{prefix}_probability_config.json\"), \"w\"))\n",
    "    json.dump({\n",
    "        \"model\": \"PPO\",\n",
    "        \"ticker\": artifact['result']['Ticker'],\n",
    "        \"window\": artifact['result']['Window'],\n",
    "        \"date_trained\": datetime.today().strftime(\"%Y-%m-%d\"),\n",
    "        \"framework\": \"stable-baselines3\",\n",
    "        \"input_features\": artifact['features'],\n",
    "        \"final_portfolio\": artifact['result']['PPO_Portfolio'],\n",
    "        \"buy_hold\": artifact['result']['BuyHold'],\n",
    "        \"sharpe\": artifact['result']['Sharpe']\n",
    "    }, open(os.path.join(save_dir, f\"{prefix}_model_info.json\"), \"w\"))\n",
    "    logging.info(f\"Saved QuantConnect-compatible model for {artifact['result']['Ticker']} | {artifact['result']['Window']}\")\n",
    "\n",
    "# === PPO Walkforward Function ===\n",
    "def walkforward_ppo(df, ticker, window_size=3000, step_size=500, timesteps=1_500_000, learning_rate=2e-5):\n",
    "    if len(df) < window_size:\n",
    "        logging.warning(f\"Skipping {ticker}: only {len(df)} rows (min required: {window_size})\")\n",
    "        return []\n",
    "    artifacts, results = [], []\n",
    "    windows = get_walk_forward_windows(df, window_size, step_size)\n",
    "    device = \"cpu\"\n",
    "\n",
    "    for w_idx, (start, end) in enumerate(windows):\n",
    "        last_trade_step = -10\n",
    "        trade_occurred = False\n",
    "        whipsaw_penalty = 0.0\n",
    "        confidence_buckets = {'Low': 0, 'Mid': 0, 'High': 0}\n",
    "        cooldown_skips = []\n",
    "        risk_flags = []\n",
    "        TRADE_COOLDOWN_STEPS = int(0.3 * len(df) / 100)  #Cooldown logic\n",
    "        window_start_time = time.time()\n",
    "        gc.collect()\n",
    "        df_window = df.iloc[start:end].reset_index(drop=True)\n",
    "        if len(df_window) <= 52 or len(df_window) % 2 != 0:\n",
    "            df_window = df_window.iloc[:-1]\n",
    "\n",
    "        frame_bound = (50, len(df_window) - 3)\n",
    "        env = DummyVecEnv([lambda: ContinuousTradingEnv(df=df_window, frame_bound=frame_bound, window_size=10)])\n",
    "        env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, device=device,\n",
    "                    learning_rate=learning_rate, n_steps=2048,\n",
    "                    batch_size=64, n_epochs=10, gamma=0.99,\n",
    "                    gae_lambda=0.95, clip_range=0.2, ent_coef=0.01,\n",
    "                    policy_kwargs=dict(net_arch=[64, 64]))\n",
    "\n",
    "        logging.info(f\"Training {ticker} Window {w_idx+1}/{len(windows)}\")\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        obs = env.reset()\n",
    "        position, balance, portfolio = 0, 100000, []\n",
    "        trade_log, trade_returns, risk_flags = [], [], []\n",
    "        action_vals, cooldown_skips = [], []\n",
    "        entry_price, last_trade_step, total_reward = None, -10, 0\n",
    "        confidence_buckets = {'Low': 0, 'Mid': 0, 'High': 0}  #define only once\n",
    "\n",
    "        for i in range(len(df_window)):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            #VecEnv returns array for done; extract scalar\n",
    "            if isinstance(done, (np.ndarray, list)):\n",
    "                done = done[0]\n",
    "\n",
    "            action_val = float(np.array(action).squeeze())\n",
    "\n",
    "            #Confidence bucket logic\n",
    "            abs_action_val = abs(action_val)\n",
    "            if abs_action_val < 0.2:\n",
    "                confidence_buckets['Low'] += 1\n",
    "            elif abs_action_val < 0.5:\n",
    "                confidence_buckets['Mid'] += 1\n",
    "            else:\n",
    "                confidence_buckets['High'] += 1\n",
    "\n",
    "            action_vals.append(action_val)\n",
    "            price = df_window['Close'].iloc[i]\n",
    "\n",
    "            # Risk management\n",
    "            STOP_LOSS_PCT = 0.04\n",
    "            TAKE_PROFIT_PCT = 0.12\n",
    "            if position > 0 and entry_price:\n",
    "                change = (price - entry_price) / entry_price\n",
    "                if change < -STOP_LOSS_PCT:\n",
    "                    balance = position * price\n",
    "                    position = 0\n",
    "                    trade_log.append(\"STOP-LOSS\")\n",
    "                    reward -= 0.2\n",
    "                    entry_price = None\n",
    "                elif change > TAKE_PROFIT_PCT:\n",
    "                    balance = position * price\n",
    "                    position = 0\n",
    "                    trade_log.append(\"TAKE-PROFIT\")\n",
    "                    reward += 0.2\n",
    "                    entry_price = None\n",
    "            prior_position = position\n",
    "\n",
    "            if abs(action_val) > 0.02 and (i - last_trade_step) > int(0.3 * len(df_window) / 100):\n",
    "                if action_val > 0 and position == 0:\n",
    "                    position = balance / price\n",
    "                    balance = 0\n",
    "                    entry_price = price\n",
    "                    trade_log.append(\"BUY\")\n",
    "                    last_trade_step = i\n",
    "                elif action_val < 0 and position > 0:\n",
    "                    balance = position * price\n",
    "                    position = 0\n",
    "                    trade_log.append(\"SELL\")\n",
    "                    last_trade_step = i\n",
    "                else:\n",
    "                    trade_log.append(\"HOLD\")\n",
    "            else:\n",
    "                trade_log.append(\"HOLD\")\n",
    "\n",
    "            if prior_position != position:\n",
    "                trade_occurred = True  #only if a trade occurred\n",
    "\n",
    "            portfolio.append(balance if balance > 0 else position * price)\n",
    "\n",
    "        if not portfolio:\n",
    "            logging.warning(f\"Empty portfolio for {ticker} | Window {start}-{end}\")\n",
    "            continue\n",
    "        # Whipsaw Penalty\n",
    "        current_step = i\n",
    "        if trade_occurred:\n",
    "            steps_since_last_trade = current_step - last_trade_step\n",
    "            if steps_since_last_trade < 30:\n",
    "                whipsaw_penalty = -0.4\n",
    "                logging.info(f\"Whipsaw penalty at step {i} | Î”={steps_since_last_trade}\")\n",
    "            else:\n",
    "                whipsaw_penalty = 0.0\n",
    "            last_trade_step = current_step\n",
    "\n",
    "        reward += whipsaw_penalty\n",
    "        reward = np.clip(reward, -1.0, 1.0)\n",
    "        if (i - last_trade_step) <= TRADE_COOLDOWN_STEPS:\n",
    "            cooldown_skips.append(i)\n",
    "            logging.info(f\"â³ Cooldown: Skipped at step {i} | Î”={i - last_trade_step}\")\n",
    "\n",
    "        final_value = portfolio[-1]\n",
    "        hold_value = (100000 / df_window['Close'].iloc[0]) * df_window['Close'].iloc[-1]\n",
    "        returns = pd.Series(portfolio).pct_change().fillna(0)\n",
    "        sharpe = (returns.mean() / (returns.std() + 1e-6)) * np.sqrt(252)\n",
    "        drawdown = ((pd.Series(portfolio).cummax() - pd.Series(portfolio)) / pd.Series(portfolio).cummax()).max() * 100\n",
    "        prefix = f\"ppo_{ticker}_window{w_idx+1}\"\n",
    "\n",
    "        artifacts.append({\n",
    "            \"model\": model,\n",
    "            \"vecnorm\": env,\n",
    "            \"features\": df_window.columns.tolist(),\n",
    "            \"result\": {\n",
    "                \"Ticker\": ticker,\n",
    "                \"Window\": f\"{start}-{end}\",\n",
    "                \"PPO_Portfolio\": round(final_value, 2),\n",
    "                \"BuyHold\": round(hold_value, 2),\n",
    "                \"Sharpe\": round(sharpe, 3),\n",
    "                \"Drawdown_%\": round(drawdown, 2),\n",
    "                \"Winner\": \"PPO\" if final_value > hold_value else \"Buy & Hold\"\n",
    "            },\n",
    "            \"prefix\": prefix,\n",
    "            \"portfolio\": portfolio.copy()\n",
    "        })\n",
    "\n",
    "        results.append(artifacts[-1][\"result\"])\n",
    "        if ENABLE_PLOTS:\n",
    "            plot_performance(ticker, portfolio, hold_value)\n",
    "\n",
    "        # Hit Ratio\n",
    "        logging.info(f\"Confidence Buckets for {ticker} | Window {start}-{end}: {confidence_buckets}\")\n",
    "        successful_trades = sum([1 for t in trade_log if t == 'TAKE-PROFIT'])\n",
    "        total_trades = sum([1 for t in trade_log if t in ['SELL', 'STOP-LOSS', 'TAKE-PROFIT']])\n",
    "        hit_ratio = successful_trades / (total_trades + 1e-6)\n",
    "        logging.info(f\"Hit Ratio: {hit_ratio:.2%}\")\n",
    "\n",
    "        # Trade Summary\n",
    "        logging.info(f\"{ticker} | Trade Summary (Window {start}-{end})\")\n",
    "        logging.info(f\"{pd.Series(trade_log).value_counts().to_dict()}\")\n",
    "\n",
    "\n",
    "        # Cooldown skip diagnostics\n",
    "        if cooldown_skips:\n",
    "            logging.info(f\"Cooldown skips: {len(cooldown_skips)} | Sample: {cooldown_skips[:5]}\")\n",
    "\n",
    "        # âœ… Now clean up\n",
    "        logging.info(f\"{ticker} | Window {w_idx+1} runtime: {round(time.time() - window_start_time, 2)} sec\")\n",
    "        del env, model, df_window, portfolio, trade_log\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Save top-N windows by Sharpe\n",
    "    TOP_N_WINDOWS = 3\n",
    "    top_artifacts = sorted(artifacts, key=lambda x: x['result']['Sharpe'], reverse=True)[:TOP_N_WINDOWS]\n",
    "    for artifact in top_artifacts:\n",
    "        save_quantconnect_model(artifact, artifact['prefix'], FINAL_MODEL_DIR)\n",
    "    for artifact in top_artifacts:\n",
    "        save_quantconnect_model(artifact, artifact['prefix'], FINAL_MODEL_DIR)\n",
    "    artifacts.clear()\n",
    "    return results\n",
    "\n",
    "# === Ticker Runner ===\n",
    "def process_ticker(ticker):\n",
    "    logging.info(f\"Starting PPO Walkforward for {ticker}\")\n",
    "    df_ticker = df[df['Symbol'] == ticker].copy()\n",
    "    if df_ticker.empty:\n",
    "        logging.warning(f\"No data found for {ticker}\")\n",
    "        return []\n",
    "    return walkforward_ppo(df_ticker, ticker)\n",
    "\n",
    "# === Parallel Execution ===\n",
    "def run_parallel_tickers(tickers, out_path=os.path.join(RESULTS_DIR, \"summary.csv\")):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for result in executor.map(process_ticker, tickers):\n",
    "            if result:\n",
    "                results.extend(result)\n",
    "                pd.DataFrame(results).to_csv(out_path, index=False)\n",
    "    logging.info(\"All tickers processed in parallel.\")\n",
    "    return results\n",
    "\n",
    "# === Execution Block ===\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {'symbols': df['Symbol'].unique().tolist()}\n",
    "    all_results = []\n",
    "\n",
    "if test_mode:\n",
    "    test_stocks = ['AAPL', 'NVDA', 'MSFT']\n",
    "    for stock in test_stocks:\n",
    "        df_stock = df[df['Symbol'] == stock].copy()\n",
    "        results = walkforward_ppo(df_stock, stock)\n",
    "        all_results.extend(results)\n",
    "    pd.DataFrame(all_results).to_csv(os.path.join(RESULTS_DIR, \"summary_test_mode.csv\"), index=False)\n",
    "else:\n",
    "    summary_df = run_parallel_tickers(CONFIG['symbols'])\n",
    "    if not summary_df:\n",
    "        logging.warning(\"No results generated.\")\n",
    "    else:\n",
    "        path = os.path.join(RESULTS_DIR, \"summary.csv\")\n",
    "        pd.DataFrame(summary_df).to_csv(path, index=False)\n",
    "        logging.info(f\"Summary saved to {path}\")\n",
    "\n",
    "logging.info(\"Script finished execution.\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# === Enable Logging Format ===\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# === Paths ===\n",
    "RESULTS_DIR = \"/content/drive/MyDrive/Results_May_2025/ppo_walkforward_results\"\n",
    "MODEL_DIR = os.path.join(RESULTS_DIR, \"models\")\n",
    "TICKER = \"AAPL\"\n",
    "WINDOW = \"window1\"\n",
    "\n",
    "# === Filenames ===\n",
    "model_path = os.path.join(MODEL_DIR, f\"ppo_{TICKER}_{WINDOW}_model.zip\")\n",
    "vecnorm_path = os.path.join(MODEL_DIR, f\"ppo_{TICKER}_{WINDOW}_vecnorm.pkl\")\n",
    "summary_path = os.path.join(RESULTS_DIR, \"summary_results.csv\")\n",
    "\n",
    "# === Load PPO Model ===\n",
    "if os.path.exists(model_path):\n",
    "    model = PPO.load(model_path)\n",
    "    logging.info(f\"Loaded PPO model from: {model_path}\")\n",
    "else:\n",
    "    logging.warning(f\"Model file not found at: {model_path}\")\n",
    "    model = None\n",
    "\n",
    "# === Load VecNormalize ===\n",
    "if os.path.exists(vecnorm_path):\n",
    "    vec_env = joblib.load(vecnorm_path)\n",
    "    logging.info(f\"Loaded VecNormalize from: {vecnorm_path}\")\n",
    "else:\n",
    "    logging.warning(f\"VecNormalize file not found at: {vecnorm_path}\")\n",
    "    vec_env = None\n",
    "\n",
    "# === PPO Model Architecture ===\n",
    "if model:\n",
    "    logging.info(\"PPO Model Architecture:\")\n",
    "    logging.info(str(model.policy))\n",
    "\n",
    "# === VecNormalize Diagnostics ===\n",
    "if vec_env:\n",
    "    try:\n",
    "        logging.info(\"VecNormalize Statistics:\")\n",
    "        logging.info(f\"Mean: {vec_env.obs_rms.mean}\")\n",
    "        logging.info(f\"Var: {vec_env.obs_rms.var}\")\n",
    "        logging.info(f\"Clip Range: {vec_env.clip_obs}\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not extract VecNormalize stats: {e}\")\n",
    "\n",
    "# === Summary Results (Top 3 by Sharpe) ===\n",
    "if os.path.exists(summary_path):\n",
    "    try:\n",
    "        summary_df = pd.read_csv(summary_path)\n",
    "        top3 = summary_df.sort_values(by=\"Sharpe\", ascending=False).head(3)\n",
    "        logging.info(\"Top 3 Results by Sharpe Ratio:\")\n",
    "        for i, row in top3.iterrows():\n",
    "            logging.info(f\"{row['Ticker']} | Window: {row['Window']} | Sharpe: {row['Sharpe']:.3f} | Portfolio: {row['PPO_Portfolio']:.2f} | Drawdown: {row['Drawdown_%']:.2f}%\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error reading summary file: {e}\")\n",
    "else:\n",
    "    logging.warning(\"No summary_results.csv found. Skipping top-3 summary.\")\n"
   ],
   "metadata": {
    "id": "WQzMmdUL4WHN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Old file (replace with your current notebook path)\n",
    "old_file = \"multi_stock_ppo_live_inference_v5_1.ipynb\"\n",
    "# New cleaned file\n",
    "new_file = \"multi_stock_ppo_live_inference_enhanced_v1.ipynb\"\n",
    "\n",
    "# Load notebook JSON\n",
    "with open(old_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Remove problematic widget metadata\n",
    "if \"widgets\" in nb.get(\"metadata\", {}):\n",
    "    del nb[\"metadata\"][\"widgets\"]\n",
    "\n",
    "# Save cleaned notebook\n",
    "with open(new_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(nb, f, indent=1, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Cleaned notebook saved as {new_file}\")\n"
   ],
   "metadata": {
    "id": "LR3vv-cX5gMR"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}